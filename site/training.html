<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Anatomy of LLM Training</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:wght@400;700&family=DM+Mono:wght@300;400;500&family=Instrument+Serif:ital@0;1&display=swap" rel="stylesheet">
<style>
/* ─── RESET & BASE ─── */
*, *::before, *::after { margin:0; padding:0; box-sizing:border-box; }

:root {
  --bg: #0a0a0c;
  --bg-card: #111115;
  --bg-card-hover: #18181e;
  --border: #1e1e28;
  --border-active: #3a3a50;
  --text: #e8e6e3;
  --text-dim: #9a98a0;
  --text-muted: #4a4850;
  --accent: #E69F00;
  --accent-dim: #b87e00;
  --accent-glow: rgba(230,159,0,0.08);
  --compute: #D55E00;
  --memory: #0090D6;
  --io: #E69F00;
  --logic: #CC79A7;
  --network: #56B4E9;
  --font-display: 'Instrument Serif', serif;
  --font-body: 'Atkinson Hyperlegible', sans-serif;
  --font-mono: 'DM Mono', monospace;
}

html { scroll-behavior: smooth; }
body {
  background: var(--bg);
  color: var(--text);
  font-family: var(--font-body);
  font-weight: 400;
  line-height: 1.8;
  overflow-x: hidden;
  -webkit-font-smoothing: antialiased;
  padding-top: 48px;
}

::selection { background: var(--accent); color: var(--bg); }

/* ─── HERO ─── */
.hero {
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  text-align: center;
  padding: 2rem;
  position: relative;
  overflow: hidden;
}

.hero::before {
  content: '';
  position: absolute;
  inset: 0;
  background:
    radial-gradient(ellipse 80% 50% at 50% 0%, rgba(230,159,0,0.05) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 20% 80%, rgba(213,94,0,0.03) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 80% 80%, rgba(204,121,167,0.03) 0%, transparent 50%);
  pointer-events: none;
}

.hero-label {
  font-family: var(--font-mono);
  font-size: 0.7rem;
  letter-spacing: 0.3em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 2rem;
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.2s;
}

.hero h1 {
  font-family: var(--font-display);
  font-size: clamp(3rem, 8vw, 7rem);
  font-weight: 400;
  line-height: 1.05;
  letter-spacing: -0.02em;
  max-width: 900px;
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.4s;
}

.hero h1 em { font-style: italic; color: var(--accent); }

.hero-sub {
  font-size: clamp(1rem, 2vw, 1.2rem);
  color: var(--text-dim);
  max-width: 600px;
  margin-top: 1.5rem;
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.6s;
}

.hero-cta {
  margin-top: 3rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  font-family: var(--font-mono);
  font-size: 0.8rem;
  color: var(--text-muted);
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.8s;
}

.hero-cta .arrow { display: inline-block; animation: bounce 2s ease infinite; }

@keyframes fadeUp {
  from { opacity: 0; transform: translateY(20px); }
  to { opacity: 1; transform: translateY(0); }
}
@keyframes bounce {
  0%, 100% { transform: translateY(0); }
  50% { transform: translateY(6px); }
}

/* ─── PHASE OVERVIEW ─── */
.phase-overview {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 1.5rem;
  max-width: 1200px;
  margin: 0 auto;
  padding: 0 2rem 4rem;
}

.phase-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 1.5rem;
  cursor: pointer;
  transition: border-color 0.18s ease, background-color 0.18s ease, transform 0.18s ease, box-shadow 0.18s ease;
  position: relative;
}

.phase-card:hover {
  border-color: var(--border-active);
  background: var(--bg-card-hover);
}

.phase-letter {
  font-family: var(--font-display);
  font-size: 2.5rem;
  color: var(--accent);
  line-height: 1;
  margin-bottom: 0.5rem;
}

.phase-name { font-family: var(--font-display); font-size: 1.3rem; margin-bottom: 0.25rem; }
.phase-subtitle { font-size: 0.85rem; color: var(--text-dim); margin-bottom: 1rem; }
.phase-steps { display: flex; flex-direction: column; gap: 0.25rem; }
.phase-steps span { font-family: var(--font-mono); font-size: 0.72rem; color: var(--text-muted); letter-spacing: 0.05em; }

.phase-card:not(:last-child)::after {
  content: '\2192';
  position: absolute;
  right: -1rem;
  top: 50%;
  transform: translate(50%, -50%);
  color: var(--text-muted);
  font-size: 1.2rem;
  z-index: 3;
}

/* ─── PIPELINE SECTION ─── */
.pipeline-section {
  padding: 4rem 2rem 6rem;
  max-width: 1200px;
  margin: 0 auto;
}

.section-label {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  letter-spacing: 0.3em;
  text-transform: uppercase;
  color: var(--text-muted);
  margin-bottom: 0.5rem;
}

.section-title {
  font-family: var(--font-display);
  font-size: clamp(2rem, 4vw, 3.5rem);
  font-weight: 400;
  margin-bottom: 1rem;
}

.section-desc { color: var(--text-dim); max-width: 650px; margin-bottom: 3rem; font-size: 1rem; }

/* ─── PHASE DIVIDERS ─── */
.phase-divider {
  display: grid;
  grid-template-columns: 48px 1fr;
  padding: 2rem 0 0.5rem;
  position: relative;
}
.phase-divider:first-child { padding-top: 0; }

.phase-divider-marker {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border-radius: 8px;
  background: var(--accent);
  color: var(--bg);
  font-family: var(--font-mono);
  font-weight: 700;
  font-size: 0.75rem;
  margin-left: 8px;
  z-index: 2;
}

.phase-divider-content { padding: 0.25rem 0 0 0.75rem; }
.phase-divider-name { font-family: var(--font-display); font-size: 1.15rem; color: var(--accent); }
.phase-divider-desc { font-size: 0.8rem; color: var(--text-muted); }

/* ─── PIPELINE FLOW ─── */
.pipeline-flow {
  display: flex;
  flex-direction: column;
  gap: 0;
  position: relative;
}

.pipeline-flow::before {
  content: '';
  position: absolute;
  left: 24px;
  top: 0;
  bottom: 0;
  width: 1px;
  background: linear-gradient(to bottom, transparent, var(--border) 5%, var(--border) 95%, transparent);
}

.pipeline-step {
  display: grid;
  grid-template-columns: 48px 1fr;
  gap: 0;
  cursor: pointer;
  position: relative;
}

.step-marker { display: flex; flex-direction: column; align-items: center; position: relative; z-index: 2; }
.step-dot {
  width: 10px;
  height: 10px;
  border-radius: 50%;
  border: 2px solid var(--border);
  background: var(--bg);
  margin-top: 1.5rem;
  transition: border-color 0.18s ease, background-color 0.18s ease, box-shadow 0.18s ease;
  flex-shrink: 0;
}

.pipeline-step:hover .step-dot,
.pipeline-step.active .step-dot {
  border-color: var(--accent);
  background: var(--accent);
  box-shadow: 0 0 12px rgba(230,159,0,0.3);
}

.step-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 1.25rem 1.5rem;
  margin: 0.5rem 0;
  transition: border-color 0.18s ease, background-color 0.18s ease;
  position: relative;
  overflow: hidden;
  cursor: pointer;
}

.step-card::before {
  content: '';
  position: absolute;
  inset: 0;
  background: linear-gradient(135deg, var(--accent-glow) 0%, transparent 50%);
  opacity: 0;
  transition: opacity 0.18s ease;
}

.pipeline-step:hover .step-card,
.pipeline-step.active .step-card {
  border-color: var(--border-active);
  background: var(--bg-card-hover);
}

.pipeline-step:hover .step-card::before,
.pipeline-step.active .step-card::before { opacity: 1; }

.step-header {
  display: flex;
  align-items: center;
  gap: 0.75rem;
  position: relative;
  z-index: 1;
  cursor: pointer;
}

.step-number { font-family: var(--font-mono); font-size: 0.6rem; color: var(--accent-dim); letter-spacing: 0.1em; min-width: 24px; }
.step-name { font-family: var(--font-display); font-size: 1.4rem; flex: 1; }

.step-badge {
  font-family: var(--font-mono);
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  padding: 0.2rem 0.5rem;
  border-radius: 4px;
  border: 1px solid;
}

.step-badge::before { margin-right: 0.3em; }
.badge-compute { color: var(--compute); border-color: rgba(213,94,0,0.3); background: rgba(213,94,0,0.05); }
.badge-compute::before { content: '\26A1'; }
.badge-memory { color: var(--memory); border-color: rgba(0,144,214,0.3); background: rgba(0,144,214,0.05); }
.badge-memory::before { content: '\2630'; }
.badge-io { color: var(--io); border-color: rgba(230,159,0,0.3); background: rgba(230,159,0,0.05); }
.badge-io::before { content: '\21C4'; }
.badge-logic { color: var(--logic); border-color: rgba(204,121,167,0.3); background: rgba(204,121,167,0.05); }
.badge-logic::before { content: '\2699'; }
.badge-network { color: var(--network); border-color: rgba(86,180,233,0.3); background: rgba(86,180,233,0.05); }
.badge-network::before { content: '\25CE'; }

.step-summary { color: var(--text-dim); font-size: 0.9rem; margin-top: 0.5rem; position: relative; z-index: 1; }

.step-expand-icon {
  color: var(--text-muted);
  font-size: 1.2rem;
  transition: transform 0.18s ease;
  font-family: var(--font-mono);
}

.pipeline-step.active .step-expand-icon { transform: rotate(45deg); color: var(--accent); }

/* ─── EXPANDED DETAIL ─── */
.step-detail {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.18s ease-out;
  position: relative;
  z-index: 1;
  cursor: default;
}

.pipeline-step.active .step-detail { max-height: 8000px; }

.detail-inner {
  padding: 1.5rem 0 0.5rem;
  border-top: 1px solid var(--border);
  margin-top: 1rem;
}

.detail-section { margin-bottom: 1.5rem; }
.detail-section:last-child { margin-bottom: 0; }

.detail-label {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--accent-dim);
  margin-bottom: 0.5rem;
}

.detail-text { font-size: 0.92rem; color: var(--text-dim); line-height: 1.8; }
.detail-text strong { color: var(--text); font-weight: 400; }
.detail-text code {
  font-family: var(--font-mono);
  font-size: 0.78rem;
  background: rgba(230,159,0,0.06);
  color: var(--accent);
  padding: 0.15rem 0.4rem;
  border-radius: 4px;
}

/* ─── CALLOUTS ─── */
.callout-box {
  border-radius: 8px;
  padding: 0.85rem 1rem;
  margin-bottom: 1.25rem;
  font-size: 0.9rem;
  line-height: 1.7;
}

.callout-takeaway {
  background: rgba(230,159,0,0.06);
  border-left: 3px solid var(--accent);
}

.callout-takeaway .callout-label {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.3rem;
}

.callout-takeaway p { color: var(--text); font-weight: 700; }

.callout-analogy {
  background: rgba(204,121,167,0.05);
  border-left: 3px solid var(--logic);
}

.callout-analogy .callout-label {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--logic);
  margin-bottom: 0.3rem;
}

.callout-analogy p { color: var(--text-dim); font-style: italic; }

/* ─── SUB-TOPICS ─── */
.sub-topics {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
  gap: 0.75rem;
  margin-top: 0.75rem;
}

.sub-topic {
  background: rgba(255,255,255,0.02);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 1rem;
  cursor: pointer;
  transition: border-color 0.18s ease, background-color 0.18s ease, transform 0.18s ease, box-shadow 0.18s ease;
}

.sub-topic:hover { border-color: var(--border-active); background: rgba(255,255,255,0.04); }
.sub-topic.expanded { grid-column: 1 / -1; border-color: var(--accent-dim); }

.sub-topic-header { display: flex; align-items: center; justify-content: space-between; }
.sub-topic-name { font-family: var(--font-display); font-size: 1.1rem; }
.sub-topic-icon { color: var(--text-muted); font-family: var(--font-mono); font-size: 0.9rem; transition: transform 0.3s ease; }
.sub-topic.expanded .sub-topic-icon { transform: rotate(45deg); color: var(--accent); }

.sub-topic-preview { color: var(--text-muted); font-size: 0.8rem; margin-top: 0.35rem; }
.sub-topic-detail { display: none; margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border); }
.sub-topic.expanded .sub-topic-detail { display: block; }
.sub-topic-detail p { font-size: 0.9rem; color: var(--text-dim); margin-bottom: 0.75rem; line-height: 1.8; }
.sub-topic-detail p:last-child { margin-bottom: 0; }

/* ─── DATA TABLES ─── */
.data-table { width: 100%; border-collapse: collapse; margin: 0.75rem 0; font-size: 0.8rem; }
.data-table th {
  font-family: var(--font-mono); font-size: 0.6rem; letter-spacing: 0.15em;
  text-transform: uppercase; color: var(--text-muted); text-align: left;
  padding: 0.5rem 0.75rem; border-bottom: 1px solid var(--border);
}
.data-table td { padding: 0.5rem 0.75rem; color: var(--text-dim); border-bottom: 1px solid rgba(30,30,40,0.5); }
.data-table td:first-child { color: var(--text); }
.data-table tr:last-child td { border-bottom: none; }

/* ─── METRICS ─── */
.metrics-row {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
  gap: 0.75rem;
  margin: 0.75rem 0;
}

.metric {
  background: rgba(255,255,255,0.02);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 0.75rem 1rem;
  text-align: center;
}

.metric-value { font-family: var(--font-display); font-size: 1.6rem; color: var(--accent); }
.metric-label {
  font-family: var(--font-mono); font-size: 0.55rem; letter-spacing: 0.15em;
  text-transform: uppercase; color: var(--text-muted); margin-top: 0.25rem;
}

/* ─── TERMS ─── */
.term {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-style: dotted;
  text-decoration-color: rgba(230,159,0,0.4);
  text-underline-offset: 3px;
  cursor: help;
}

.term-tooltip {
  position: fixed;
  max-width: 340px;
  background: #1a1a22;
  border: 1px solid var(--border-active);
  border-radius: 10px;
  padding: 0.85rem 1rem;
  z-index: 1000;
  box-shadow: 0 8px 32px rgba(0,0,0,0.5);
  animation: tooltipIn 0.2s ease;
}

.term-tooltip-title {
  font-family: var(--font-mono); font-size: 0.65rem; letter-spacing: 0.15em;
  text-transform: uppercase; color: var(--accent); margin-bottom: 0.35rem;
}

.term-tooltip-body { font-size: 0.85rem; color: var(--text-dim); line-height: 1.6; }

.term-tooltip-close {
  position: absolute; top: 0.5rem; right: 0.5rem;
  width: 1.25rem; height: 1.25rem;
  display: flex; align-items: center; justify-content: center;
  background: transparent; border: 1px solid var(--border); border-radius: 4px;
  color: var(--text-muted); font-size: 0.65rem; cursor: pointer;
  padding: 0; line-height: 1;
  transition: color 0.15s, border-color 0.15s;
}
.term-tooltip-close:hover { color: var(--text-dim); border-color: var(--border-active); }

.term-tooltip-footer {
  display: flex; align-items: center; gap: 0.75rem;
  margin-top: 0.5rem; padding-top: 0.4rem; border-top: 1px solid var(--border);
}

.term-tooltip-source {
  display: inline-block;
  margin-top: 0.4rem;
  font-family: var(--font-mono);
  font-size: 0.6rem;
  color: var(--accent-dim);
  text-decoration: none;
  letter-spacing: 0.05em;
}
.term-tooltip-source:hover { color: var(--accent); }

@keyframes tooltipIn {
  from { opacity: 0; transform: translateY(4px); }
  to { opacity: 1; transform: translateY(0); }
}

/* ─── CROSS-LINK ─── */
.cross-link {
  display: inline-block;
  margin-top: 0.5rem;
  color: var(--accent-dim);
  font-family: var(--font-mono);
  font-size: 0.75rem;
  text-decoration: none;
  letter-spacing: 0.05em;
  transition: color 0.2s;
}
.cross-link:hover { color: var(--accent); }

/* ─── FOOTER ─── */
.footer {
  padding: 4rem 2rem;
  text-align: center;
  border-top: 1px solid var(--border);
}
.footer p { font-family: var(--font-mono); font-size: 0.65rem; color: var(--text-muted); letter-spacing: 0.1em; }

/* ─── SCROLL ANIMATIONS ─── */
.reveal { opacity: 0; transform: translateY(20px); transition: all 0.6s ease; }
.reveal.visible { opacity: 1; transform: translateY(0); }

/* ─── MINIMAP ─── */
.minimap {
  position: fixed;
  right: max(1.5rem, calc((100vw - 1200px) / 2 - 10rem));
  top: 50%;
  transform: translateY(-50%);
  z-index: 400;
  opacity: 0;
  pointer-events: none;
  transition: opacity 0.35s ease;
  display: flex;
  flex-direction: column;
  gap: 0;
}
.minimap.visible { opacity: 1; pointer-events: auto; }

.minimap-phase { padding: 0.4rem 0; position: relative; }
.minimap-phase + .minimap-phase { border-top: 1px solid var(--border); margin-top: 0.15rem; padding-top: 0.55rem; }

.minimap-phase-label {
  font-family: var(--font-mono); font-size: 0.5rem; letter-spacing: 0.18em;
  text-transform: uppercase; color: var(--accent-dim); margin-bottom: 0.35rem; padding-left: 2px;
}

.minimap-item {
  display: flex; align-items: center; gap: 0.5rem;
  padding: 0.2rem 0; text-decoration: none; cursor: pointer; outline: none;
}

.minimap-dot {
  width: 8px; height: 8px; border-radius: 50%;
  border: 1.5px solid var(--border-active); background: var(--bg);
  flex-shrink: 0; box-shadow: 0 0 0 3px var(--bg);
  transition: border-color 0.2s ease, background 0.2s ease, box-shadow 0.2s ease;
}

.minimap-label {
  font-family: var(--font-mono); font-size: 0.55rem; letter-spacing: 0.06em;
  color: var(--text-muted); white-space: nowrap; transition: color 0.2s ease;
}

.minimap-item:hover .minimap-dot { border-color: var(--border-active); }
.minimap-item:hover .minimap-label { color: var(--text-dim); }
.minimap-item:focus-visible .minimap-dot { outline: 2px solid var(--accent); outline-offset: 2px; }
.minimap-item.active .minimap-dot {
  background: var(--accent); border-color: var(--accent);
  box-shadow: 0 0 6px var(--accent-glow), 0 0 12px var(--accent-glow);
}
.minimap-item.active .minimap-label { color: var(--text); }

[data-theme="light"] .minimap-dot { background: var(--bg-card); }
[data-theme="light"] .minimap-item.active .minimap-dot { box-shadow: 0 0 8px rgba(184,126,0,0.2); }

.minimap-label { opacity: 0; width: 0; overflow: hidden; transition: opacity 0.2s ease, width 0.2s ease; }
.minimap-phase-label { opacity: 0; width: 0; overflow: hidden; transition: opacity 0.2s ease, width 0.2s ease; }
.minimap:hover { background: var(--bg); box-shadow: -4px 0 16px rgba(0,0,0,0.3); border: 1px solid var(--border); padding: 0.75rem 1rem 0.75rem 0.5rem; border-radius: 8px; }
.minimap:hover .minimap-label { opacity: 1; width: auto; }
.minimap:hover .minimap-phase-label { opacity: 1; width: auto; }
[data-theme="light"] .minimap:hover { box-shadow: -4px 0 16px rgba(0,0,0,0.08); }
[data-theme="light"] .step-visual { background: rgba(0,0,0,0.02); }
[data-theme="light"] .funnel-bar, [data-theme="light"] .opt-bar { color: rgba(255,255,255,0.95); }
[data-theme="light"] .par-cell { color: rgba(255,255,255,0.95); }
@media (max-width: 768px) {
  .minimap { display: none; }
}

/* ─── SITE NAV ─── */
.site-nav {
  position: fixed;
  top: 0; left: 0; right: 0;
  z-index: 600;
  height: 48px;
  background: rgba(10,10,12,0.85);
  backdrop-filter: blur(12px);
  -webkit-backdrop-filter: blur(12px);
  border-bottom: 1px solid var(--border);
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 0 2rem;
}

.nav-logo { font-family: var(--font-display); font-size: 1rem; color: var(--text); text-decoration: none; white-space: nowrap; }
.nav-links { display: flex; align-items: center; gap: 1.5rem; }
.nav-link {
  font-family: var(--font-mono); font-size: 0.65rem; letter-spacing: 0.15em;
  text-transform: uppercase; color: var(--text-muted); text-decoration: none;
  padding: 0.25rem 0; transition: color 0.2s ease;
}
.nav-link:hover { color: var(--text-dim); }
.nav-link.active { color: var(--accent); border-bottom: 1.5px solid var(--accent); }
.nav-divider { width: 1px; height: 18px; background: var(--border); flex-shrink: 0; }

[data-theme="light"] .site-nav { background: rgba(245,243,239,0.85); }

.theme-toggle {
  width: 32px; height: 32px; border-radius: 50%;
  border: 1px solid var(--border); background: transparent;
  cursor: pointer; display: flex; align-items: center; justify-content: center;
  color: var(--text-dim); font-size: 1rem;
  transition: border-color 0.2s ease, color 0.2s ease;
}
.theme-toggle:hover { border-color: var(--border-active); color: var(--text); }
.theme-toggle::after { content: '\263C'; }
[data-theme="light"] .theme-toggle::after { content: '\263E'; }

/* ─── INTERACTIVE VISUAL ─── */
.visual-container {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 1.5rem;
  margin: 1rem 0;
  position: relative;
  min-height: 200px;
}

.visual-label {
  font-family: var(--font-mono); font-size: 0.6rem; letter-spacing: 0.2em;
  text-transform: uppercase; color: var(--accent-dim); margin-bottom: 1rem;
}

.visual-bar-track {
  background: rgba(255,255,255,0.03);
  border-radius: 6px;
  height: 32px;
  position: relative;
  overflow: hidden;
  margin-bottom: 0.5rem;
}

.visual-bar-fill {
  height: 100%;
  border-radius: 6px;
  transition: width 0.6s ease;
  display: flex;
  align-items: center;
  padding: 0 0.5rem;
  font-family: var(--font-mono);
  font-size: 0.6rem;
  color: var(--bg);
  white-space: nowrap;
}

.visual-bar-label {
  display: flex;
  justify-content: space-between;
  font-family: var(--font-mono);
  font-size: 0.6rem;
  color: var(--text-muted);
  margin-bottom: 0.75rem;
}

.visual-legend {
  display: flex;
  flex-wrap: wrap;
  gap: 1rem;
  margin-top: 0.75rem;
  font-family: var(--font-mono);
  font-size: 0.6rem;
  color: var(--text-muted);
}

.visual-legend-item {
  display: flex;
  align-items: center;
  gap: 0.35rem;
}

.visual-legend-dot {
  width: 8px;
  height: 8px;
  border-radius: 2px;
  flex-shrink: 0;
}

.step-visual { position: relative; min-height: 200px; background: rgba(255,255,255,0.015); border: 1px solid var(--border); border-radius: 10px; overflow: hidden; margin: 0.75rem 0; font-family: var(--font-mono); font-size: 0.7rem; padding: 1.5rem; }

/* Data Funnel */
.funnel-stage { display: flex; align-items: center; gap: 0.75rem; margin-bottom: 0.5rem; }
.funnel-label { min-width: 110px; font-size: 0.6rem; color: var(--text-dim); text-align: right; }
.funnel-bar { height: 28px; border-radius: 4px; transition: width 1s ease; display: flex; align-items: center; padding: 0 0.5rem; font-size: 0.55rem; color: rgba(255,255,255,0.9); white-space: nowrap; }
.funnel-arrow { text-align: center; color: var(--text-muted); font-size: 0.7rem; margin: 0.25rem 0 0.25rem 115px; }
.funnel-value { min-width: 60px; font-size: 0.65rem; color: var(--text); }

/* Optimizer Comparison */
.opt-chart { display: flex; flex-direction: column; gap: 0.4rem; }
.opt-row { display: flex; align-items: center; gap: 0.75rem; }
.opt-label { min-width: 90px; font-size: 0.6rem; color: var(--text-dim); text-align: right; }
.opt-bar-wrap { flex: 1; height: 24px; background: rgba(255,255,255,0.03); border-radius: 4px; overflow: hidden; }
.opt-bar { height: 100%; border-radius: 4px; display: flex; align-items: center; padding: 0 0.5rem; font-size: 0.55rem; color: rgba(255,255,255,0.9); transition: width 0.8s ease; }
.opt-val { min-width: 60px; font-size: 0.6rem; color: var(--text-dim); }

/* Parallelism Grid */
.par-grid { display: grid; grid-template-columns: repeat(4, 1fr); gap: 3px; max-width: 320px; margin: 0.5rem auto; }
.par-cell { width: 100%; aspect-ratio: 1; border-radius: 3px; display: flex; align-items: center; justify-content: center; font-size: 0.45rem; color: rgba(255,255,255,0.8); transition: all 0.4s ease; }
.par-controls { display: flex; gap: 0.5rem; justify-content: center; margin-bottom: 1rem; flex-wrap: wrap; }
.par-btn { background: rgba(255,255,255,0.03); border: 1px solid var(--border); border-radius: 6px; padding: 0.3rem 0.75rem; font-family: var(--font-mono); font-size: 0.6rem; color: var(--text-dim); cursor: pointer; transition: all 0.2s; }
.par-btn.active { border-color: var(--accent); color: var(--accent); background: var(--accent-glow); }
.par-btn:hover { border-color: var(--border-active); }

/* ─── RESPONSIVE ─── */
@media (max-width: 768px) {
  .site-nav { padding: 0 0.75rem; }
  .nav-logo { font-size: 0.85rem; }
  .nav-link { font-size: 0.5rem; letter-spacing: 0.08em; }
  .nav-links { gap: 0.5rem; }
  .nav-divider { height: 14px; }
  .pipeline-flow::before { left: 18px; }
  .pipeline-step { grid-template-columns: 36px 1fr; }
  .step-card { padding: 1rem; }
  .step-name { font-size: 1.15rem; }
  .sub-topics { grid-template-columns: 1fr; }
  .step-badge { display: none; }
  .metrics-row { grid-template-columns: repeat(2, 1fr); }
  .phase-overview { grid-template-columns: 1fr; }
  .phase-card:not(:last-child)::after {
    content: '\2193';
    right: 50%;
    top: auto;
    bottom: -1rem;
    transform: translate(50%, 50%);
  }
}

/* ─── LIGHT THEME ─── */
[data-theme="light"] {
  --bg: #f5f3ef;
  --bg-card: #ffffff;
  --bg-card-hover: #f0eee9;
  --border: #d8d5ce;
  --border-active: #b0abb8;
  --text: #1a1a1f;
  --text-dim: #4a4750;
  --text-muted: #8a8690;
  --accent: #9d6e00;
  --accent-dim: #7a5600;
  --accent-glow: rgba(157,110,0,0.07);
  --compute: #b34a00;
  --memory: #006fa8;
  --io: #9d6e00;
  --logic: #a85d87;
  --network: #1a7ab5;
}

[data-theme="light"] .hero::before {
  background:
    radial-gradient(ellipse 80% 50% at 50% 0%, rgba(157,110,0,0.06) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 20% 80%, rgba(179,74,0,0.04) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 80% 80%, rgba(168,93,135,0.04) 0%, transparent 50%);
}

[data-theme="light"] .step-dot { background: var(--bg-card); }
[data-theme="light"] .pipeline-step:hover .step-dot,
[data-theme="light"] .pipeline-step.active .step-dot { box-shadow: 0 0 12px rgba(157,110,0,0.25); }

[data-theme="light"] .badge-compute { border-color: rgba(179,74,0,0.3); background: rgba(179,74,0,0.06); }
[data-theme="light"] .badge-memory { border-color: rgba(0,111,168,0.3); background: rgba(0,111,168,0.06); }
[data-theme="light"] .badge-io { border-color: rgba(157,110,0,0.3); background: rgba(157,110,0,0.06); }
[data-theme="light"] .badge-logic { border-color: rgba(168,93,135,0.3); background: rgba(168,93,135,0.06); }
[data-theme="light"] .badge-network { border-color: rgba(26,122,181,0.3); background: rgba(26,122,181,0.06); }

[data-theme="light"] .sub-topic { background: rgba(0,0,0,0.02); }
[data-theme="light"] .sub-topic:hover { background: rgba(0,0,0,0.04); }
[data-theme="light"] .metric { background: rgba(0,0,0,0.02); }

[data-theme="light"] .term-tooltip { background: #ffffff; box-shadow: 0 8px 32px rgba(0,0,0,0.12); }
[data-theme="light"] .term-tooltip-close { border-color: rgba(0,0,0,0.15); color: rgba(0,0,0,0.3); }
[data-theme="light"] .term-tooltip-close:hover { border-color: rgba(0,0,0,0.3); color: rgba(0,0,0,0.5); }
[data-theme="light"] .term-tooltip-footer { border-top-color: rgba(0,0,0,0.08); }

[data-theme="light"] .callout-takeaway { background: rgba(157,110,0,0.06); }
[data-theme="light"] .callout-analogy { background: rgba(168,93,135,0.06); }
[data-theme="light"] .detail-text code { background: rgba(157,110,0,0.08); }
[data-theme="light"] .data-table td { border-bottom-color: rgba(0,0,0,0.06); }
[data-theme="light"] .term { text-decoration-color: rgba(157,110,0,0.4); }
[data-theme="light"] ::selection { background: var(--accent); color: #ffffff; }

body, .callout-box, .metric, .site-nav, .minimap-phase { transition: background-color 0.3s ease, color 0.3s ease, border-color 0.3s ease; }


/* ─── SEARCH ─── */
.search-overlay { position: fixed; inset: 0; background: rgba(0,0,0,0.55); z-index: 2000; display: flex; align-items: flex-start; justify-content: center; padding-top: 12vh; opacity: 0; pointer-events: none; transition: opacity 0.15s ease; }
.search-overlay.open { opacity: 1; pointer-events: auto; }
.search-modal { background: var(--bg-card); border: 1px solid var(--border-active); border-radius: 12px; width: min(560px, calc(100vw - 2rem)); box-shadow: 0 24px 64px rgba(0,0,0,0.5); transform: translateY(-8px); transition: transform 0.15s ease; }
.search-overlay.open .search-modal { transform: translateY(0); }
.search-input-wrap { display: flex; align-items: center; gap: 0.75rem; padding: 0.875rem 1rem; border-bottom: 1px solid var(--border); }
.search-icon-svg { width: 16px; height: 16px; color: var(--text-muted); flex-shrink: 0; fill: none; }
.search-input { flex: 1; background: none; border: none; outline: none; font-family: var(--font-body); font-size: 0.95rem; color: var(--text); }
.search-input::placeholder { color: var(--text-muted); }
.search-esc-hint { font-family: var(--font-mono); font-size: 0.6rem; color: var(--text-muted); background: var(--bg); border: 1px solid var(--border); border-radius: 4px; padding: 0.15rem 0.4rem; flex-shrink: 0; }
.search-results { max-height: 380px; overflow-y: auto; padding: 0.4rem 0; contain: content; }
.search-group-header { font-family: var(--font-mono); font-size: 0.5rem; letter-spacing: 0.18em; text-transform: uppercase; color: var(--text-muted); padding: 0.6rem 1rem 0.2rem; }
.search-result { display: flex; align-items: center; gap: 0.75rem; padding: 0.55rem 1rem; cursor: pointer; transition: background 0.1s ease; }
.search-result:hover, .search-result.selected { background: var(--bg-card-hover); }
.search-result-dot { width: 6px; height: 6px; border-radius: 50%; border: 1.5px solid var(--border-active); background: transparent; flex-shrink: 0; transition: background 0.1s ease, border-color 0.1s ease; }
.search-result.selected .search-result-dot { background: var(--accent); border-color: var(--accent); }
.search-result-body { flex: 1; min-width: 0; }
.search-result-title { font-size: 0.85rem; color: var(--text-dim); }
.search-result.selected .search-result-title { color: var(--text); }
.search-result-desc { font-size: 0.7rem; color: var(--text-muted); margin-top: 0.05rem; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
.search-result-badge { font-family: var(--font-mono); font-size: 0.45rem; letter-spacing: 0.1em; text-transform: uppercase; color: var(--accent-dim); border: 1px solid rgba(86,180,233,0.25); border-radius: 3px; padding: 0.1rem 0.3rem; flex-shrink: 0; }
.search-no-results { padding: 2.5rem 1rem; text-align: center; color: var(--text-muted); font-size: 0.85rem; }
.search-footer { display: flex; gap: 1.5rem; padding: 0.5rem 1rem; border-top: 1px solid var(--border); justify-content: flex-end; align-items: center; }
.search-footer-hint { font-family: var(--font-mono); font-size: 0.5rem; color: var(--text-muted); display: flex; align-items: center; gap: 0.3rem; }
.search-footer-key { background: var(--bg); border: 1px solid var(--border); border-radius: 3px; padding: 0.1rem 0.3rem; }
.search-trigger { background: none; border: 1px solid var(--border); border-radius: 6px; display: flex; align-items: center; gap: 0.4rem; padding: 0.25rem 0.6rem; cursor: pointer; color: var(--text-muted); font-family: var(--font-mono); font-size: 0.55rem; letter-spacing: 0.05em; transition: border-color 0.2s ease, color 0.2s ease; }
.search-trigger:hover { border-color: var(--border-active); color: var(--text-dim); }
.search-trigger svg { width: 12px; height: 12px; fill: none; }
.search-trigger .search-trigger-label { white-space: nowrap; }
.search-trigger kbd { font-family: var(--font-mono); font-size: 0.5rem; color: var(--text-muted); background: var(--bg); border: 1px solid var(--border); border-radius: 3px; padding: 0.05rem 0.3rem; }
.knowledge-trigger[disabled] { opacity: 0.65; cursor: wait; }
[data-theme="light"] .search-overlay { background: rgba(0,0,0,0.25); }
[data-theme="light"] .search-modal { box-shadow: 0 24px 64px rgba(0,0,0,0.15); }
@media (max-width: 768px) {
  .search-trigger .search-trigger-label, .search-trigger kbd { display: none; }
  .search-trigger { padding: 0.35rem; }
}
</style>
<script>
(function(){
  var s = localStorage.getItem('theme');
  if (s) { document.documentElement.setAttribute('data-theme', s); }
  else if (window.matchMedia('(prefers-color-scheme: light)').matches) {
    document.documentElement.setAttribute('data-theme', 'light');
  } else { document.documentElement.setAttribute('data-theme', 'dark'); }
})();
</script>
</head>
<body>

<!-- ─── NAV ─── -->
<nav class="site-nav">
  <a href="index.html" class="nav-logo">LLM Anatomy</a>
  <div class="nav-links">
    <a href="training.html" class="nav-link active">Training</a>
    <a href="training-economics.html" class="nav-link">Training Economics</a>
    <span class="nav-divider"></span>
    <a href="index.html" class="nav-link">Inference</a>
    <a href="economics.html" class="nav-link">Inference Economics</a>
    <button class="search-trigger" id="search-trigger" aria-label="Search all pages">
      <svg viewBox="0 0 16 16" stroke="currentColor" stroke-width="1.5"><circle cx="6.5" cy="6.5" r="4.5"></circle><path stroke-linecap="round" d="m10 10 3.5 3.5"></path></svg>
      <span class="search-trigger-label">Search</span>
      <kbd id="search-key-hint">&#8984;K</kbd>
    </button>
    <button class="search-trigger knowledge-trigger" id="download-knowledge" aria-label="Download website knowledge as Markdown">
      <svg viewBox="0 0 16 16" stroke="currentColor" stroke-width="1.5" fill="none"><path stroke-linecap="round" stroke-linejoin="round" d="M8 2.5v7"></path><path stroke-linecap="round" stroke-linejoin="round" d="m5.5 7.5 2.5 2.5 2.5-2.5"></path><path stroke-linecap="round" stroke-linejoin="round" d="M3 12.5h10"></path></svg>
      <span class="search-trigger-label">Knowledge</span>
      <kbd>.md</kbd>
    </button>
    <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme"></button>
  </div>
</nav>

<!-- ─── MINIMAP ─── -->
<div class="minimap" id="minimap">
  <div class="minimap-phase">
    <div class="minimap-phase-label">G &middot; Data &amp; Arch</div>
    <div class="minimap-item" data-target="data-pipeline"><div class="minimap-dot"></div><span class="minimap-label">Data Pipeline</span></div>
    <div class="minimap-item" data-target="tokenizer"><div class="minimap-dot"></div><span class="minimap-label">Tokenizer</span></div>
    <div class="minimap-item" data-target="architecture"><div class="minimap-dot"></div><span class="minimap-label">Architecture</span></div>
  </div>
  <div class="minimap-phase">
    <div class="minimap-phase-label">H &middot; Training</div>
    <div class="minimap-item" data-target="optimization"><div class="minimap-dot"></div><span class="minimap-label">Optimization</span></div>
    <div class="minimap-item" data-target="distributed"><div class="minimap-dot"></div><span class="minimap-label">Distributed</span></div>
    <div class="minimap-item" data-target="monitoring"><div class="minimap-dot"></div><span class="minimap-label">Monitoring</span></div>
  </div>
  <div class="minimap-phase">
    <div class="minimap-phase-label">I &middot; Post-Training</div>
    <div class="minimap-item" data-target="sft"><div class="minimap-dot"></div><span class="minimap-label">Fine-Tuning</span></div>
    <div class="minimap-item" data-target="alignment"><div class="minimap-dot"></div><span class="minimap-label">Alignment</span></div>
    <div class="minimap-item" data-target="rft"><div class="minimap-dot"></div><span class="minimap-label">Reinforcement FT</span></div>
    <div class="minimap-item" data-target="evaluation"><div class="minimap-dot"></div><span class="minimap-label">Evaluation</span></div>
  </div>
</div>

<!-- ─── HERO ─── -->
<section class="hero">
  <div class="hero-label">How Models Learn</div>
  <h1>Anatomy of <em>LLM Training</em></h1>
  <p class="hero-sub">From raw data to aligned model &mdash; every stage of building a large language model.</p>
  <div class="hero-cta"><span class="arrow">&darr;</span> Nine stages, three phases</div>
</section>

<!-- ─── PHASE OVERVIEW ─── -->
<div class="phase-overview">
  <div class="phase-card" onclick="document.getElementById('phase-g').scrollIntoView({behavior:'smooth',block:'start'})">
    <div class="phase-letter">G</div>
    <div class="phase-name">Data &amp; Architecture</div>
    <div class="phase-subtitle">Raw data to model blueprint</div>
    <div class="phase-steps">
      <span>G1 &middot; Data Pipeline</span>
      <span>G2 &middot; Tokenizer Training</span>
      <span>G3 &middot; Model Architecture</span>
    </div>
  </div>
  <div class="phase-card" onclick="document.getElementById('phase-h').scrollIntoView({behavior:'smooth',block:'start'})">
    <div class="phase-letter">H</div>
    <div class="phase-name">Training Process</div>
    <div class="phase-subtitle">Optimizers, parallelism, recovery</div>
    <div class="phase-steps">
      <span>H1 &middot; Optimization</span>
      <span>H2 &middot; Distributed Training</span>
      <span>H3 &middot; Monitoring &amp; Recovery</span>
    </div>
  </div>
  <div class="phase-card" onclick="document.getElementById('phase-i').scrollIntoView({behavior:'smooth',block:'start'})">
    <div class="phase-letter">I</div>
    <div class="phase-name">Post-Training</div>
    <div class="phase-subtitle">SFT, alignment, evaluation</div>
    <div class="phase-steps">
      <span>I1 &middot; Supervised Fine-Tuning</span>
      <span>I2 &middot; Alignment</span>
      <span>I3 &middot; Evaluation</span>
    </div>
  </div>
</div>

<!-- ─── PIPELINE SECTION ─── -->
<section class="pipeline-section" id="pipeline">
  <div class="section-label">The Training Pipeline</div>
  <div class="section-title">Nine Stages of Model Creation</div>
  <div class="section-desc">From terabytes of raw text to an aligned model that follows instructions &mdash; each stage shapes what the model knows, how it reasons, and how it responds.</div>

  <div class="pipeline-flow">

    <!-- ═══ PHASE G ═══ -->
    <div class="phase-divider" id="phase-g">
      <div class="phase-divider-marker">G</div>
      <div class="phase-divider-content">
        <div class="phase-divider-name">Data &amp; Architecture</div>
        <div class="phase-divider-desc">Curating training data and designing the model</div>
      </div>
    </div>

    <!-- G1: Data Pipeline -->
    <div class="pipeline-step reveal" data-step="data-pipeline" id="data-pipeline">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">G1</span>
          <span class="step-name">Data Pipeline</span>
          <span class="step-badge badge-io">I/O</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Collect, filter, and deduplicate terabytes of text &mdash; <span class="term" data-term="data-quality">data quality</span> determines the ceiling of model capability.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>DCLM showed keeping only the top 10% of data by quality beats training on everything. Data curation is the single highest-leverage investment in model quality.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like&hellip;</div>
              <p>A chef sourcing ingredients &mdash; the finest technique can&rsquo;t save a dish made from spoiled food. Training data is the raw ingredient of intelligence.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Data Collection</div>
              <div class="detail-text">
                <p>Modern training corpora start from web crawls. <strong>CommonCrawl</strong> provides petabytes of raw HTML. Curated derivatives include <span class="term" data-term="fineweb">FineWeb</span> (15T tokens), <span class="term" data-term="dclm">DCLM</span> (240T raw &rarr; 3.8T curated tokens), and RedPajama. Additional sources: books, scientific papers (~5%), code from GitHub/StackOverflow (~15%), and Wikipedia.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Quality Filtering</div>
              <div class="detail-text">
                <p><strong>Heuristic filters</strong>: remove documents by length (too short = low content), perplexity (too high = garbled text), PII patterns, and URL blocklists. <strong>Classifier-based filtering</strong>: train a fastText model on high-quality vs. low-quality examples, then score and threshold every document. DCLM used a 1.4B-parameter classifier trained on OpenHermes 2.5 to keep only top-quality text.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Deduplication</div>
              <div class="detail-text">
                <p>Duplicate content wastes compute and can cause memorization. <span class="term" data-term="minhash">MinHash LSH</span> (Locality-Sensitive Hashing) efficiently finds near-duplicate documents by approximating Jaccard similarity. <strong>SimHash</strong> provides a faster alternative for exact-duplicate detection. Exact substring dedup (suffix arrays) catches boilerplate repeated across many pages.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Data Mixing</div>
              <div class="detail-text">
                <p>The final training mixture balances domains: typically <strong>~70% web text, 15% code, 5% scientific, 5% books, 5% other</strong> (conversation, math, encyclopedic). Optimal mixing ratios are determined empirically by training small proxy models on different mixes and measuring downstream task performance.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Scale</div>
              <div class="metrics-row">
                <div class="metric"><div class="metric-value">15T</div><div class="metric-label">FineWeb Tokens</div></div>
                <div class="metric"><div class="metric-value">3.8T</div><div class="metric-label">DCLM Curated</div></div>
                <div class="metric"><div class="metric-value">15T</div><div class="metric-label">Llama 3 Training</div></div>
                <div class="metric"><div class="metric-value">10%</div><div class="metric-label">DCLM Keep Rate</div></div>
              </div>
            </div>

            <div class="step-visual" id="visual-data-funnel"></div>
          </div>
        </div>
      </div>
    </div>

    <!-- G2: Tokenizer Training -->
    <div class="pipeline-step reveal" data-step="tokenizer" id="tokenizer">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">G2</span>
          <span class="step-name">Tokenizer Training</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Build the vocabulary that converts text into the integer sequences the model actually processes.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Vocabulary size is a fundamental trade-off: larger vocabularies compress text better (fewer tokens per sentence) but increase embedding table size and can fragment rare words.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like&hellip;</div>
              <p>Building a dictionary before learning a language. The tokenizer decides which &ldquo;words&rdquo; the model will think in &mdash; too few and it stutters, too many and it wastes memory on rare terms.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">BPE Algorithm</div>
              <div class="detail-text">
                <p><span class="term" data-term="bpe">Byte-Pair Encoding</span> starts with individual bytes (or characters) and iteratively merges the most frequent adjacent pair into a new token. After thousands of merges, common words become single tokens while rare words decompose into subword pieces. GPT-4 uses <strong>tiktoken</strong> (100K vocabulary); Llama 3 uses <strong>SentencePiece</strong> (128K vocabulary, up from 32K in Llama 2).</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Unigram Model</div>
              <div class="detail-text">
                <p>The <span class="term" data-term="unigram-model">Unigram</span> approach works in reverse: start with a very large vocabulary, then iteratively prune tokens that contribute least to the training corpus likelihood. SentencePiece supports both BPE and Unigram. Unigram tends to produce more linguistically meaningful subwords.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Vocabulary Sizes</div>
              <div class="detail-text">
                <table class="data-table">
                  <thead><tr><th>Model</th><th>Vocab Size</th><th>Method</th></tr></thead>
                  <tbody>
                    <tr><td>GPT-4</td><td>100,256</td><td>BPE (tiktoken)</td></tr>
                    <tr><td>Llama 3</td><td>128,256</td><td>BPE (SentencePiece)</td></tr>
                    <tr><td>Llama 2</td><td>32,000</td><td>BPE (SentencePiece)</td></tr>
                    <tr><td>Claude 3</td><td>~100,000</td><td>BPE</td></tr>
                    <tr><td>Gemini</td><td>256,000</td><td>SentencePiece</td></tr>
                  </tbody>
                </table>
              </div>
            </div>

            <a href="index.html#phase-a" class="cross-link">&rarr; How tokenization works during inference</a>
          </div>
        </div>
      </div>
    </div>

    <!-- G3: Model Architecture -->
    <div class="pipeline-step reveal" data-step="architecture" id="architecture">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">G3</span>
          <span class="step-name">Model Architecture</span>
          <span class="step-badge badge-compute">Compute</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">The 2024 consensus: decoder-only transformer with <span class="term" data-term="rmsnorm">RMSNorm</span>, <span class="term" data-term="rope">RoPE</span>, <span class="term" data-term="swiglu">SwiGLU</span>, and <span class="term" data-term="gqa">GQA</span>.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Modern LLM architecture has converged on a small set of proven components. Innovation now happens at the edges: MoE for efficiency, MLA for KV cache compression, and deeper attention alternatives.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Attention Variants</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">MHA &rarr; MQA &rarr; GQA</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">From full key-value heads to grouped sharing</div>
                  <div class="sub-topic-detail">
                    <p><strong>Multi-Head Attention (MHA)</strong>: each head has its own Q, K, V projections. Full expressivity but large KV cache (proportional to number of heads).</p>
                    <p><strong>Multi-Query Attention (MQA)</strong>: all heads share a single K and V. Dramatically reduces KV cache but can hurt quality.</p>
                    <p><strong><span class="term" data-term="gqa">Grouped-Query Attention (GQA)</span></strong>: compromise &mdash; heads are grouped, each group shares K/V. Llama 3 uses 8 KV heads for 64 query heads (8:1 ratio). Near-MHA quality with near-MQA efficiency.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Multi-head Latent Attention</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">DeepSeek&rsquo;s 28x KV cache reduction</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="mla">MLA</span> (DeepSeek V2/V3) compresses keys and values into a low-rank latent space before caching. Instead of storing full K/V tensors, it stores a compressed representation and reconstructs K/V on the fly during attention.</p>
                    <p>Result: <strong>28x reduction in KV cache size</strong> compared to standard MHA, with minimal quality loss. This enables much larger batch sizes during inference.</p>
                  </div>
                </div>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Feed-Forward Network</div>
              <div class="detail-text">
                <p><span class="term" data-term="swiglu">SwiGLU</span> replaces the standard 2-matrix FFN (up-project &rarr; ReLU &rarr; down-project) with a 3-matrix gated design: two parallel up-projections, one gated by SiLU activation, then multiplied element-wise before the down-projection. Intermediate dimension is typically <code>(8/3) &times; d_model</code> to keep parameter count comparable.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Stability Innovations</div>
              <div class="detail-text">
                <p><strong>QK-Norm</strong>: normalize query and key vectors before the dot product to prevent attention logit explosion at scale. <strong>Logit capping</strong>: clip pre-softmax logits to a maximum value (e.g., 30.0 in Gemma 2). <strong>Z-loss</strong>: auxiliary loss that penalizes large logits, reducing training instability.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Mixture of Experts</div>
              <div class="detail-text">
                <p><span class="term" data-term="moe">MoE</span> replaces each dense FFN layer with N expert FFN layers plus a router. Only a subset of experts are activated per token. <strong>DeepSeek V3</strong>: 671B total parameters, 37B active per token (256 experts, top-8 routing). Auxiliary-loss-free load balancing via bias terms prevents expert collapse.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Positional Encodings</div>
              <div class="detail-text">
                <p><span class="term" data-term="rope">RoPE</span> (Rotary Position Embeddings) encodes position by rotating query/key vectors in 2D subspaces. Relative by construction &mdash; attention depends on distance between tokens, not absolute position. Supports extrapolation to longer sequences than seen during training (with techniques like NTK-aware scaling, YaRN).</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ═══ PHASE H ═══ -->
    <div class="phase-divider" id="phase-h">
      <div class="phase-divider-marker">H</div>
      <div class="phase-divider-content">
        <div class="phase-divider-name">Training Process</div>
        <div class="phase-divider-desc">Optimization, distribution, and resilience at scale</div>
      </div>
    </div>

    <!-- H1: Optimization -->
    <div class="pipeline-step reveal" data-step="optimization" id="optimization">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">H1</span>
          <span class="step-name">Optimization</span>
          <span class="step-badge badge-compute">Compute</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Optimizers, learning rate schedules, and precision formats that turn gradient descent into efficient learning.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Muon (2025) achieves 2x the efficiency of AdamW and is now in PyTorch core. Meanwhile, FP8 training pioneered by DeepSeek V3 delivers 20-50% throughput gains with minimal quality loss.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like&hellip;</div>
              <p>A hiker descending a foggy mountain. The optimizer decides step direction and size; the learning rate schedule decides when to take big strides vs. careful steps; precision format determines how detailed their map is.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Optimizers</div>
              <div class="detail-text">
                <table class="data-table">
                  <thead><tr><th>Optimizer</th><th>Memory/Param</th><th>Key Advantage</th></tr></thead>
                  <tbody>
                    <tr><td><span class="term" data-term="adamw">AdamW</span></td><td>12 bytes</td><td>Incumbent, well-understood, reliable</td></tr>
                    <tr><td><span class="term" data-term="muon">Muon</span></td><td>8 bytes</td><td>2x efficiency, now in PyTorch core (2025)</td></tr>
                    <tr><td>SOAP</td><td>~12 bytes</td><td>&gt;40% fewer iterations to converge</td></tr>
                    <tr><td>Lion</td><td>4 bytes</td><td>50% memory savings, sign-based updates</td></tr>
                  </tbody>
                </table>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Learning Rate Schedules</div>
              <div class="detail-text">
                <p><strong>Cosine decay</strong>: the traditional choice &mdash; warm up linearly, then decay following a cosine curve to near-zero. Problem: schedule length must be set before training begins.</p>
                <p><span class="term" data-term="wsd">WSD (Warmup-Stable-Decay)</span>: decouple the schedule from total training steps. Warm up, hold a stable rate for most of training, then decay sharply at the end. Enables checkpoints at any point during the stable phase to be continued or branched.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Mixed Precision</div>
              <div class="detail-text">
                <p><strong>BF16</strong> (bfloat16): the standard for modern training. Same dynamic range as FP32 with half the bits. Supported natively on A100+.</p>
                <p><strong>FP8</strong>: pioneered at scale by DeepSeek V3. <strong>20-50% throughput gain</strong> over BF16 with careful scaling. Uses per-tensor or per-channel quantization with high-precision master weights.</p>
                <p><strong>FP4</strong>: emerging on NVIDIA Blackwell (B200). Still experimental for training; primarily inference-focused.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Gradient Accumulation</div>
              <div class="detail-text">
                <p>Simulates larger batch sizes by accumulating gradients over multiple forward/backward passes before updating weights. Llama 3 used an effective batch size of <strong>~16M tokens</strong>, achieved through gradient accumulation across thousands of GPUs.</p>
              </div>
            </div>

            <div class="step-visual" id="visual-optimizer-compare"></div>
          </div>
        </div>
      </div>
    </div>

    <!-- H2: Distributed Training -->
    <div class="pipeline-step reveal" data-step="distributed" id="distributed">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">H2</span>
          <span class="step-name">Distributed Training</span>
          <span class="step-badge badge-network">Network</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Splitting model and data across thousands of GPUs &mdash; the engineering challenge that defines frontier training.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Llama 3 405B trained on 16,384 H100 GPUs using 4D parallelism (TP=8, CP=16, PP=16, DP), achieving 400 TFLOPS per GPU (~40% MFU). Every parallelism strategy trades off communication overhead for memory savings.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like&hellip;</div>
              <p>Building a skyscraper with 16,000 workers. Data parallelism gives everyone the same blueprint but different bricks. Tensor parallelism splits each floor across teams. Pipeline parallelism assigns different floors to different crews.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Parallelism Strategies</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Data Parallelism &amp; ZeRO</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Replicate model, split data across GPUs</div>
                  <div class="sub-topic-detail">
                    <p><strong>DDP (Distributed Data Parallel)</strong>: each GPU holds a full model copy, processes different data, synchronizes gradients via all-reduce. Simple but memory-heavy.</p>
                    <p><span class="term" data-term="zero">ZeRO</span> (Zero Redundancy Optimizer) partitions optimizer states (Stage 1: 4x savings), gradients (Stage 2: 8x), and parameters (Stage 3: linear scaling) across GPUs. Only gathers what&rsquo;s needed for each operation.</p>
                    <p><span class="term" data-term="fsdp">FSDP2</span> (Fully Sharded Data Parallelism): PyTorch-native ZeRO Stage 3. The default for most training runs that don&rsquo;t need tensor parallelism.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Tensor Parallelism</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Split weight matrices within a single node</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="tensor-parallelism">Tensor parallelism</span> splits individual weight matrices across GPUs within the same node (connected by NVLink at 900 GB/s on H100). Typically TP=8 (one per GPU in a node). Each GPU computes a slice of every layer, then exchanges results via all-reduce.</p>
                    <p>Communication cost is proportional to hidden dimension and number of layers. Only practical within a node &mdash; inter-node bandwidth (InfiniBand at 400 Gb/s) is too slow for per-layer synchronization.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Pipeline Parallelism</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Distribute layers across nodes</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="pipeline-parallelism">Pipeline parallelism</span> assigns different transformer layers to different nodes. Micro-batches flow through the pipeline like an assembly line. <strong>1F1B scheduling</strong> (one forward, one backward) minimizes the &ldquo;bubble&rdquo; where stages idle.</p>
                    <p>Llama 3 used PP=16, meaning 16 pipeline stages across 16 nodes. Each stage holds ~5 layers of the 126-layer model.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">4D Parallelism</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Combining all strategies at frontier scale</div>
                  <div class="sub-topic-detail">
                    <p><strong>Llama 3 405B on 16,384 H100s</strong>: TP=8 (within node), CP=16 (context/sequence parallelism for 128K context), PP=16 (pipeline stages), DP (data parallelism across remaining dimension). Total: 8 &times; 16 &times; 16 = 2,048 model replicas running in data-parallel.</p>
                    <p><strong>400 TFLOPS per GPU</strong> (~40% MFU). Expert parallelism for MoE adds a 5th dimension in models like DeepSeek V3, where AllToAll routing sends tokens to the correct expert across nodes.</p>
                  </div>
                </div>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Training Frameworks</div>
              <div class="detail-text">
                <table class="data-table">
                  <thead><tr><th>Framework</th><th>Org</th><th>Strength</th></tr></thead>
                  <tbody>
                    <tr><td>Megatron-LM</td><td>NVIDIA</td><td>TP + PP + expert parallelism, highly optimized CUDA kernels</td></tr>
                    <tr><td>DeepSpeed</td><td>Microsoft</td><td>ZeRO optimizer, flexible parallelism composition</td></tr>
                    <tr><td>TorchTitan</td><td>Meta</td><td>Native PyTorch 4D parallelism, used for Llama 3</td></tr>
                    <tr><td>JAX/XLA</td><td>Google</td><td>Functional transforms (pmap/pjit), TPU-native</td></tr>
                  </tbody>
                </table>
              </div>
            </div>

            <div class="step-visual" id="visual-parallelism"></div>

            <a href="index.html#cross-cutting" class="cross-link">&rarr; How parallelism works during inference</a>
          </div>
        </div>
      </div>
    </div>

    <!-- H3: Monitoring & Recovery -->
    <div class="pipeline-step reveal" data-step="monitoring" id="monitoring">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">H3</span>
          <span class="step-name">Monitoring &amp; Recovery</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Frontier training runs fail constantly &mdash; Llama 3 had 419 failures in 54 days. Resilience engineering is non-negotiable.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>At scale, failures are inevitable (~1 every 3 hours for Llama 3). The key differentiator is recovery speed: async checkpointing (ByteCheckpoint: 529x speedup) and auto-repair systems (MegaScale: &gt;90% automated recovery).</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Key Metrics</div>
              <div class="detail-text">
                <p><strong>Training loss</strong>: the primary signal. Should decrease smoothly; spikes indicate instability. <strong>Gradient norms</strong>: sudden increases warn of divergence. <span class="term" data-term="mfu">MFU</span> (Model FLOPS Utilization): what fraction of theoretical GPU compute is actually used. Llama 3 achieved ~40%; &gt;50% is exceptional.</p>
                <p><strong>Validation loss</strong>: evaluated periodically on held-out data to detect overfitting or data quality issues.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Loss Spike Causes</div>
              <div class="detail-text">
                <p><strong>Residual amplification</strong>: deep residual connections can amplify small perturbations. <strong>Gradient intensification</strong>: sudden gradient magnitude jumps, often triggered by unusual data batches. <strong>Attention logit explosion</strong>: unbounded dot products in attention &mdash; mitigated by QK-norm and logit capping.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Failure Statistics</div>
              <div class="metrics-row">
                <div class="metric"><div class="metric-value">419</div><div class="metric-label">Llama 3 Failures</div></div>
                <div class="metric"><div class="metric-value">54d</div><div class="metric-label">Training Duration</div></div>
                <div class="metric"><div class="metric-value">30%</div><div class="metric-label">GPU Hardware Faults</div></div>
                <div class="metric"><div class="metric-value">~3h</div><div class="metric-label">Mean Time Between</div></div>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Checkpoint Strategies</div>
              <div class="detail-text">
                <p><strong>Synchronous checkpointing</strong>: pause training, write full model state to storage. Simple but adds 12-43% overhead at scale. <strong>Async checkpointing</strong> (ByteCheckpoint): snapshot to CPU/NVMe while training continues &mdash; <strong>529x speedup</strong> over synchronous. <span class="term" data-term="fsdp">FSDP2</span> sharded checkpoints: each GPU writes only its shard, parallel I/O.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Automated Recovery</div>
              <div class="detail-text">
                <p><strong>MegaScale</strong> (ByteDance): automated diagnosis and repair for &gt;90% of failures. Detects faulty GPUs, remaps workloads, and resumes from checkpoint without human intervention. At 16K+ GPU scale, manual recovery is impractical.</p>
              </div>
            </div>

            <a href="training-economics.html#failure-costs" class="cross-link">&rarr; The cost of training failures</a>
          </div>
        </div>
      </div>
    </div>

    <!-- ═══ PHASE I ═══ -->
    <div class="phase-divider" id="phase-i">
      <div class="phase-divider-marker">I</div>
      <div class="phase-divider-content">
        <div class="phase-divider-name">Post-Training</div>
        <div class="phase-divider-desc">Turning a base model into an aligned assistant</div>
      </div>
    </div>

    <!-- I1: Supervised Fine-Tuning -->
    <div class="pipeline-step reveal" data-step="sft" id="sft">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">I1</span>
          <span class="step-name">Supervised Fine-Tuning</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Train on curated (instruction, response) pairs to establish format, tone, and instruction-following ability.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>SFT bridges the gap between a base model (which just predicts next tokens) and an assistant (which follows instructions). Quality matters more than quantity &mdash; 10K excellent examples can outperform 1M mediocre ones.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like&hellip;</div>
              <p>An apprenticeship after general education. The model already &ldquo;knows&rdquo; the language; SFT teaches it the manners and format of a helpful assistant.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Process</div>
              <div class="detail-text">
                <p>Train on <strong>10K-100K curated examples</strong> of (instruction, ideal response) pairs. The loss function only computes on the response tokens (masking the instruction). Typically 1-3 epochs to avoid overfitting.</p>
                <p>Examples come from human annotators, distillation from stronger models, or synthetic generation with human filtering.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Production SFT Modes (Fireworks)</div>
              <div class="detail-text">
                <table class="data-table">
                  <thead><tr><th>Mode</th><th>What You Train</th><th>Key Dataset Shape</th></tr></thead>
                  <tbody>
                    <tr><td>Supervised text</td><td>Single-turn text completions</td><td><code>text</code> (and optional <code>loss_mask</code>) fields</td></tr>
                    <tr><td>Supervised chat</td><td>Multi-message assistants</td><td><code>messages[]</code> with role/content</td></tr>
                    <tr><td>Supervised vision</td><td>Multimodal chat + tools</td><td>Chat messages with text/image content arrays</td></tr>
                  </tbody>
                </table>
                <p>Fireworks recommends starting with LoRA adapters for most enterprise fine-tuning, then escalating to heavier strategies only if quality targets are not met.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Dataset Controls</div>
              <div class="detail-text">
                <p><span class="term" data-term="loss-mask">Loss mask</span> lets you train only on selected spans in supervised text examples. In chat mode, <span class="term" data-term="loss-role">loss role</span> training keeps optimization focused on assistant turns instead of user/tool turns.</p>
                <p>For vision and tool-using fine-tunes, Fireworks supports preserving tool-call structure so fine-tuned models remain function-calling compatible in production.</p>
                <p>Enterprise deployments increasingly require secure fine-tuning boundaries (customer-controlled buckets and explicit retention controls) before regulated data can enter training pipelines.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Full vs Parameter-Efficient</div>
              <div class="detail-text">
                <p><strong>Full fine-tuning</strong>: update all parameters. Best quality but expensive (~$50K per run for a 7B model on H100s).</p>
                <p><strong>LoRA/QLoRA</strong>: freeze base weights, train small low-rank adapter matrices. <span class="term" data-term="lora">LoRA</span> adds &lt;1% parameters, achieves 90-95% of full fine-tuning quality at <strong>$300-$3,000 per run</strong>. QLoRA further quantizes the base model to 4-bit during training.</p>
                <p>Fireworks defaults for LoRA SFT are deliberately conservative (rank 32, alpha 64, context 16K, ~3 epochs), then tuned from there. A major deployment advantage is multi-adapter serving: up to <strong>100 LoRA adapters</strong> can share one base model deployment, dramatically improving utilization for long-tail custom models.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- I2: Alignment -->
    <div class="pipeline-step reveal" data-step="alignment" id="alignment">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">I2</span>
          <span class="step-name">Alignment</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Aligning model behavior with human preferences &mdash; from RLHF to <span class="term" data-term="grpo">GRPO</span>, the methods that make models helpful, harmless, and honest.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>GRPO (DeepSeek R1) eliminated the need for reward models entirely by using verifiable rewards, jumping AIME 2024 scores from 15.6% to 71.0%. The field is rapidly moving away from the complexity of traditional RLHF.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Alignment Methods</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">RLHF Pipeline</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">The original 3-stage approach</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="rlhf">RLHF</span> (Reinforcement Learning from Human Feedback) has 3 stages: (1) SFT the base model, (2) train a reward model on human preference comparisons, (3) optimize the policy with PPO against the reward model.</p>
                    <p>Requires <strong>4 models in memory simultaneously</strong> (policy, reference, reward, value). Complex, expensive, and sensitive to hyperparameters. Still used by OpenAI and Anthropic but increasingly supplemented by simpler methods.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">DPO</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Direct preference optimization, no reward model</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="dpo">DPO</span> (Direct Preference Optimization) reparameterizes the RLHF objective to directly optimize on preference pairs without training a separate reward model. Only needs <strong>2 models</strong> (policy + reference). Simpler pipeline, more stable training.</p>
                    <p>Limitation: still requires pairwise comparisons (chosen/rejected responses), which are expensive to collect.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">DPO Data Contracts (Fireworks)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Prompt + preferred/non-preferred outputs, with optional tool context</div>
                  <div class="sub-topic-detail">
                    <p>Fireworks DPO expects explicit triplets: <span class="term" data-term="dpo-triplet">prompt</span>, preferred output, and non-preferred output. This makes preference data portable across experiments and easier to audit.</p>
                    <p>When workflows depend on function calling, datasets can include tool definitions and tool-call traces so preference optimization teaches both quality and tool-use behavior.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">GRPO</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Group Relative Policy Optimization</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="grpo">GRPO</span> (DeepSeek R1): no reward model, no value network. Generate multiple responses per prompt, use <strong>verifiable rewards</strong> (math correctness, code execution, format compliance) to rank them, then optimize the policy on the group-relative ranking.</p>
                    <p>Result on AIME 2024: <strong>15.6% &rarr; 71.0%</strong>. Dramatically simpler and more scalable than RLHF for domains with verifiable outcomes.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Constitutional AI &amp; KTO</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">AI judges and binary feedback</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="constitutional-ai">Constitutional AI</span> (Anthropic): replace human preference annotators with AI judges that evaluate responses against a written constitution. Enables RLAIF (RL from AI Feedback) at much lower cost.</p>
                    <p><span class="term" data-term="kto">KTO</span> (Kahneman-Tversky Optimization): uses only binary labels (good/bad) instead of pairwise comparisons. Easier to collect feedback &mdash; humans just rate individual responses as thumbs up or down.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- I3: Reinforcement Fine-Tuning -->
    <div class="pipeline-step reveal" data-step="rft" id="rft">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">I3</span>
          <span class="step-name">Reinforcement Fine-Tuning</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Domain-specific RL that combines expert knowledge with model reasoning &mdash; teaching models <em>how to think</em> in specialized fields, not just what to say.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Reinforcement Fine-Tuning (RFT) bridges the gap between general-purpose alignment and domain expertise. By using verifiable domain-specific rewards, RFT produces specialist models that reason through problems rather than pattern-match from training data. OpenAI, Google, and others now offer RFT as a product for enterprise customers.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like&hellip;</div>
              <p>SFT teaches a medical student the textbook answers. RFT gives them a residency &mdash; repeated practice with real cases and expert feedback on their reasoning process, not just their final answer.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">How It Works</div>
              <div class="detail-text">
                <p>RFT applies reinforcement learning after SFT, using <strong>domain-specific reward signals</strong> rather than general human preferences. The model generates multiple reasoning chains for each problem, and a reward function (often automated) scores the quality of the <em>reasoning process</em>, not just the final answer.</p>
                <p>Key difference from standard RLHF: rewards are <strong>verifiable and domain-grounded</strong> &mdash; math correctness, code execution results, legal citation accuracy, medical diagnosis criteria &mdash; rather than subjective human preference.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Operational RFT Workflow (Fireworks)</div>
              <div class="detail-text">
                <p>Fireworks RFT formalizes the loop into four assets: (1) an evaluator model or rubric, (2) a runtime environment for verification, (3) parameter tuning (rollouts/learning rate/steps), and (4) tracing for step-level debugging.</p>
                <p>This turns RFT from a one-off experiment into a repeatable production workflow: train &rarr; evaluate &rarr; tune &rarr; retrain, with cost estimators and telemetry at each stage.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">RFT Methods</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Expert-Guided RFT</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Domain experts design reward criteria</div>
                  <div class="sub-topic-detail">
                    <p>Domain experts provide <strong>grading rubrics</strong> rather than example answers. For a legal reasoning task, the rubric might score: correct statute identification (+1), proper precedent citation (+1), logical argument structure (+1), correct conclusion (+2). The model learns to maximize the rubric score through repeated practice.</p>
                    <p>OpenAI&rsquo;s RFT product uses this approach: customers provide 50-500 expert-graded examples with multi-dimensional scoring criteria. The model trains on thousands of self-generated attempts, evaluated against the rubric.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Verifiable Reward RFT</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Automated verification for math, code, and science</div>
                  <div class="sub-topic-detail">
                    <p>For domains with <strong>objectively verifiable</strong> answers, rewards can be fully automated: run the generated code, check the math proof, verify the chemical formula. DeepSeek R1&rsquo;s <span class="term" data-term="grpo">GRPO</span> is a form of verifiable-reward RFT.</p>
                    <p>Scales massively because no human annotation is needed. The model can train on millions of self-generated attempts. This is how OpenAI o1/o3 and DeepSeek R1 achieved breakthroughs on math and coding benchmarks.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Outcome vs Process Reward</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Scoring the final answer vs every step</div>
                  <div class="sub-topic-detail">
                    <p><strong>Outcome reward models (ORMs)</strong>: score only the final answer. Simple but provides sparse signal &mdash; the model doesn&rsquo;t know which reasoning step went wrong.</p>
                    <p><strong>Process reward models (PRMs)</strong>: score each intermediate reasoning step. Richer signal, better at teaching correct reasoning chains. OpenAI&rsquo;s &ldquo;Let&rsquo;s Verify Step by Step&rdquo; (2023) showed PRMs significantly outperform ORMs on math, reducing hallucinated reasoning.</p>
                    <p>The tradeoff: PRMs require step-level annotations, which are much more expensive to collect than outcome labels.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">RFT as a Product</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Enterprise offerings from OpenAI, Google, and others</div>
                  <div class="sub-topic-detail">
                    <p><strong>OpenAI RFT</strong> (2024-2025): enterprise product that fine-tunes o1/o3 models with customer-provided grading rubrics. Customers report 10-30% improvement on domain-specific tasks vs. prompting alone. Requires 50-500 graded examples.</p>
                    <p><strong>Google Gemini RFT</strong>: similar offering through Vertex AI, focusing on code generation and structured reasoning tasks.</p>
                    <p><strong>Fireworks RFT</strong>: integrates evaluator models, environment hooks, and secure fine-tuning options (including customer-controlled storage boundaries) for regulated use cases.</p>
                    <p>The business model: RFT is premium fine-tuning &mdash; higher margin than standard SFT because it produces genuinely differentiated models that reason better in the customer&rsquo;s domain.</p>
                  </div>
                </div>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">RFT vs Other Methods</div>
              <div class="detail-text">
                <table class="data-table">
                  <thead><tr><th>Method</th><th>What It Teaches</th><th>Data Needed</th><th>Best For</th></tr></thead>
                  <tbody>
                    <tr><td>SFT</td><td>Format &amp; knowledge</td><td>10K-100K examples</td><td>Style, format, basic skills</td></tr>
                    <tr><td>RLHF/DPO</td><td>Human preferences</td><td>Preference pairs</td><td>General helpfulness, safety</td></tr>
                    <tr><td>RFT</td><td>Domain reasoning</td><td>50-500 graded + self-play</td><td>Specialist reasoning tasks</td></tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- I4: Evaluation -->
    <div class="pipeline-step reveal" data-step="evaluation" id="evaluation">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">I4</span>
          <span class="step-name">Evaluation</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Measuring model capabilities across knowledge, code, math, and reasoning &mdash; while detecting contamination and gaming.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Static benchmarks saturate and leak into training data. The field is shifting to dynamic evaluation: LiveBench refreshes monthly, Chatbot Arena uses live ELO rankings from blind human comparisons.</p>
            </div>

            <div class="detail-section">
              <div class="detail-label">Benchmark Suites</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Knowledge &amp; Reasoning</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">MMLU, MMLU-Pro, HellaSwag</div>
                  <div class="sub-topic-detail">
                    <p><span class="term" data-term="mmlu">MMLU</span> (57 subjects, multiple choice): essentially saturated at &gt;90% for frontier models. <strong>MMLU-Pro</strong>: 10-option multiple choice with harder questions, 16-33% more difficult. <strong>HellaSwag</strong>: commonsense reasoning about physical situations.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Code &amp; Math</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">HumanEval, SWE-bench, AIME</div>
                  <div class="sub-topic-detail">
                    <p><strong>Code</strong>: HumanEval (function completion), LiveCodeBench (monthly-refreshed problems), SWE-bench (real GitHub issues &mdash; requires multi-file edits).</p>
                    <p><strong>Math</strong>: GSM8K (grade school, largely saturated), MATH (competition-level), AIME (AMC competition &mdash; still challenging for frontier models).</p>
                  </div>
                </div>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Contamination Detection</div>
              <div class="detail-text">
                <p><strong>Min-K% Prob</strong>: measures whether a model assigns suspiciously high probability to benchmark answers, suggesting memorization. <strong>CDD</strong> (Canonical Data Detection): checks if benchmark examples appear verbatim in training data. <strong>Perplexity-based</strong>: low perplexity on benchmark prompts suggests data leakage.</p>
              </div>
            </div>

            <div class="detail-section">
              <div class="detail-label">Dynamic Evaluation</div>
              <div class="detail-text">
                <p><strong>LiveBench</strong>: monthly refresh of questions with verifiable answers. Immune to contamination because questions didn&rsquo;t exist during training. <strong>Chatbot Arena</strong> (LMSYS): blind pairwise comparisons by real users, producing ELO rankings. Over 1M votes. The most trusted signal of real-world model quality.</p>
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Reasoning-Mode Regression Tests</div>
              <div class="detail-text">
                <p>For reasoning-capable models, evaluate at multiple runtime policies: low/high reasoning effort, budget-capped thinking, and preserved-history on/off. A model that looks strong in one policy can fail latency or quality targets in another.</p>
                <p>This is now part of production release criteria: benchmark quality plus policy-specific inference behavior.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

  </div><!-- /pipeline-flow -->
</section>

<!-- ─── FOOTER ─── -->
<footer class="footer">
  <p>LLM Training Anatomy &middot; A reference on how large language models are built</p>
</footer>

<script>
/* ─── THEME TOGGLE ─── */
var toggle = document.getElementById('theme-toggle');
if (toggle) {
  toggle.addEventListener('click', function() {
    var current = document.documentElement.getAttribute('data-theme') || 'dark';
    var next = current === 'dark' ? 'light' : 'dark';
    document.documentElement.setAttribute('data-theme', next);
    localStorage.setItem('theme', next);
  });
}

/* ─── REVEAL ON SCROLL ─── */
var revealObserver = new IntersectionObserver(function(entries) {
  entries.forEach(function(entry) {
    if (entry.isIntersecting) {
      entry.target.classList.add('visible');
      revealObserver.unobserve(entry.target);
    }
  });
}, { threshold: 0.1 });

document.querySelectorAll('.reveal').forEach(function(el) {
  revealObserver.observe(el);
});

/* ─── PIPELINE STEP TOGGLE ─── */
document.querySelectorAll('.pipeline-step').forEach(function(step) {
  step.querySelector('.step-card').addEventListener('click', function(e) {
    if (e.target.closest('.term, a, button, .step-detail, .sub-topic')) return;
    step.classList.toggle('active');
  });
});

/* ─── SUB-TOPIC TOGGLE ─── */
window.toggleSubTopic = function(el) {
  el.classList.toggle('expanded');
};

/* ─── TERM TOOLTIPS ─── */
var activeTooltip = null;
var activeTermEl = null;

var termDefs = {
  'data-quality': { def: 'The principle that training data quality (measured by relevance, accuracy, and diversity) has more impact on model capability than raw data quantity. DCLM demonstrated that keeping only 10% highest-quality data outperforms training on everything.' },
  'fineweb': { def: 'A 15-trillion token curated dataset derived from CommonCrawl, designed for LLM pre-training. Developed by Hugging Face as a high-quality open training corpus.' },
  'dclm': { def: 'DataComp for Language Models — a benchmark and dataset that curated 240T raw tokens down to 3.8T tokens using classifier-based quality filtering, demonstrating that aggressive data curation dramatically improves model quality.' },
  'minhash': { def: 'MinHash Locality-Sensitive Hashing — an efficient algorithm for estimating Jaccard similarity between documents. Used in training data deduplication to find near-duplicate text at scale without pairwise comparison.' },
  'bpe': { def: 'Byte-Pair Encoding — a tokenization algorithm that iteratively merges the most frequent adjacent byte/character pairs. Builds a vocabulary bottom-up from characters to subwords. Used by GPT-4 (tiktoken) and Llama 3 (SentencePiece).' },
  'unigram-model': { def: 'A tokenization approach that starts with a large candidate vocabulary and iteratively removes tokens that least affect the training corpus likelihood. Tends to produce more linguistically meaningful subwords than BPE.' },
  'rmsnorm': { def: 'Root Mean Square Layer Normalization — a simplified variant of LayerNorm that normalizes by RMS of activations only (no mean subtraction). Cheaper to compute and equally effective. Used in Llama, Gemma, and most modern LLMs.' },
  'rope': { def: 'Rotary Position Embeddings — encodes position by rotating query/key vectors in 2D subspaces. Position information is relative by construction, and supports extrapolation to longer sequences via NTK-aware scaling or YaRN.' },
  'swiglu': { def: 'SwiGLU activation — a gated linear unit using the SiLU (Swish) activation function. Replaces the standard 2-matrix FFN with a 3-matrix gated design, improving training efficiency at the cost of one additional weight matrix.' },
  'gqa': { def: 'Grouped-Query Attention — a compromise between MHA (full KV heads) and MQA (single KV head). Query heads are grouped, and each group shares one K/V head. Llama 3 uses 8 KV heads for 64 query heads.' },
  'mla': { def: 'Multi-head Latent Attention (DeepSeek V2/V3) — compresses keys and values into a low-rank latent space before caching, achieving 28x KV cache reduction with minimal quality loss.' },
  'moe': { def: 'Mixture of Experts — replaces dense FFN layers with N expert FFN layers plus a learned router. Only a subset of experts activate per token, enabling larger total parameter counts with constant compute cost.' },
  'adamw': { def: 'Adam optimizer with decoupled weight decay. The standard optimizer for LLM training — maintains first and second moment estimates of gradients, requiring 12 bytes of state per parameter.' },
  'muon': { def: 'A 2025 optimizer that achieves 2x the efficiency of AdamW using momentum in the orthogonal complement of the gradient. Now included in PyTorch core. Uses 8 bytes of state per parameter.' },
  'wsd': { def: 'Warmup-Stable-Decay learning rate schedule — warms up linearly, maintains a stable learning rate for most of training, then decays sharply at the end. Decouples the schedule from total training steps, enabling flexible checkpointing.' },
  'zero': { def: 'Zero Redundancy Optimizer (Microsoft DeepSpeed) — partitions optimizer states (Stage 1), gradients (Stage 2), and parameters (Stage 3) across GPUs to eliminate redundancy. Enables training models much larger than single-GPU memory.' },
  'fsdp': { def: 'Fully Sharded Data Parallelism (PyTorch) — native implementation of ZeRO Stage 3. Each GPU stores only a shard of parameters, gathering full parameters on-the-fly for computation. FSDP2 is the current version.' },
  'tensor-parallelism': { def: 'A parallelism strategy that splits individual weight matrices across GPUs within the same node. Requires high-bandwidth interconnect (NVLink) for per-layer all-reduce synchronization. Typically TP=8 for one GPU per node.' },
  'pipeline-parallelism': { def: 'A parallelism strategy that assigns different transformer layers to different nodes. Micro-batches flow through in a pipeline. 1F1B scheduling minimizes idle "bubble" time between forward and backward passes.' },
  'mfu': { def: 'Model FLOPS Utilization — the fraction of theoretical peak GPU FLOPS actually used during training. Llama 3 achieved ~40% MFU on H100s. Values above 50% are considered exceptional.' },
  'rlhf': { def: 'Reinforcement Learning from Human Feedback — a 3-stage alignment process: (1) supervised fine-tuning, (2) reward model training on human preferences, (3) PPO optimization. Requires 4 models in memory simultaneously.' },
  'dpo': { def: 'Direct Preference Optimization — reparameterizes the RLHF objective to directly optimize on preference pairs without a separate reward model. Simpler and more stable than PPO, requiring only 2 models.' },
  'grpo': { def: 'Group Relative Policy Optimization (DeepSeek R1) — generates multiple responses per prompt, uses verifiable rewards to rank them, and optimizes on group-relative rankings. No reward model or value network needed.' },
  'constitutional-ai': { def: 'Anthropic\'s approach to alignment using AI judges that evaluate responses against a written constitution of principles. Enables RLAIF (RL from AI Feedback) at lower cost than human annotation.' },
  'kto': { def: 'Kahneman-Tversky Optimization — an alignment method using only binary (good/bad) labels instead of pairwise comparisons. Named after prospect theory, it accounts for loss aversion in the optimization objective.' },
  'lora': { def: 'Low-Rank Adaptation — freezes base model weights and trains small low-rank adapter matrices that are added to existing weight matrices. Adds <1% parameters while achieving 90-95% of full fine-tuning quality.' },
  'mmlu': { def: 'Massive Multitask Language Understanding — a benchmark of 57 academic subjects testing broad knowledge. Essentially saturated at >90% for frontier models; MMLU-Pro adds harder 10-option questions.' },
  'rft': { def: 'Reinforcement Fine-Tuning — applies reinforcement learning with domain-specific, verifiable rewards after SFT to teach models specialized reasoning. Uses expert-designed rubrics or automated verification rather than general human preferences.' },
  'prm': { def: 'Process Reward Model — scores each intermediate reasoning step rather than just the final answer. Provides richer training signal than Outcome Reward Models (ORMs), significantly reducing hallucinated reasoning chains.' },
  'loss-mask': { def: 'A per-token training mask that decides which text spans contribute to fine-tuning loss. Useful for training only completion spans in mixed-format records.', src: 'https://docs.fireworks.ai/fine-tuning/supervised-fine-tuning' },
  'loss-role': { def: 'Role-aware chat fine-tuning that applies loss only to selected roles (typically assistant turns), preventing user/tool messages from being optimized as targets.', src: 'https://docs.fireworks.ai/fine-tuning/supervised-fine-tuning' },
  'dpo-triplet': { def: 'DPO dataset schema with prompt + preferred_output + non_preferred_output. Encodes explicit preference comparisons for direct optimization.', src: 'https://docs.fireworks.ai/fine-tuning/direct-preference-optimization' }
};

function dismissTooltip() {
  if (activeTooltip) { activeTooltip.remove(); activeTooltip = null; activeTermEl = null; }
}

function showTermTooltip(term) {
  if (activeTermEl === term) { dismissTooltip(); return; }
  dismissTooltip();
  var key = term.getAttribute('data-term');
  var def = termDefs[key];
  if (!def) return;
  activeTermEl = term;

  var tip = document.createElement('div');
  tip.className = 'term-tooltip';

  var closeBtn = document.createElement('button');
  closeBtn.className = 'term-tooltip-close';
  closeBtn.textContent = '\u00D7';
  closeBtn.addEventListener('click', function(e) { e.stopPropagation(); dismissTooltip(); });
  tip.appendChild(closeBtn);

  var title = document.createElement('div');
  title.className = 'term-tooltip-title';
  title.textContent = term.textContent;
  tip.appendChild(title);

  var body = document.createElement('div');
  body.className = 'term-tooltip-body';
  body.textContent = def.def;
  tip.appendChild(body);

  if (def.src) {
    var src = document.createElement('a');
    src.className = 'term-tooltip-source';
    src.href = def.src;
    src.target = '_blank';
    src.rel = 'noopener';
    src.textContent = 'Source \u2197';
    tip.appendChild(src);
  }

  document.body.appendChild(tip);
  activeTooltip = tip;

  var r = term.getBoundingClientRect();
  var tw = tip.offsetWidth;
  var th = tip.offsetHeight;
  var left = r.left + r.width / 2 - tw / 2;
  var top = r.bottom + 8;

  if (left < 8) left = 8;
  if (left + tw > window.innerWidth - 8) left = window.innerWidth - tw - 8;
  if (top + th > window.innerHeight - 8) top = r.top - th - 8;

  tip.style.left = left + 'px';
  tip.style.top = top + 'px';
}

document.querySelectorAll('.term').forEach(function(term) {
  term.addEventListener('click', function(e) {
    e.stopPropagation();
    e.preventDefault();
    showTermTooltip(term);
  });
});

document.addEventListener('click', function(e) {
  if (activeTooltip && !activeTooltip.contains(e.target) && !e.target.classList.contains('term')) {
    dismissTooltip();
  }
});

document.addEventListener('keydown', function(e) {
  if (e.key === 'Escape') dismissTooltip();
});

/* ─── MINIMAP ─── */
var minimap = document.getElementById('minimap');
var minimapItems = minimap.querySelectorAll('.minimap-item');
var scrollLock = false;

var minimapObserver = new IntersectionObserver(function(entries) {
  if (scrollLock) return;
  entries.forEach(function(entry) {
    if (entry.isIntersecting) {
      var id = entry.target.id;
      minimapItems.forEach(function(item) {
        item.classList.toggle('active', item.getAttribute('data-target') === id);
      });
    }
  });
}, { rootMargin: '-64px 0px -60% 0px', threshold: 0 });

document.querySelectorAll('.pipeline-step[id], .phase-divider[id]').forEach(function(section) {
  if (section.id) minimapObserver.observe(section);
});

minimapItems.forEach(function(item) {
  item.addEventListener('click', function() {
    var targetId = item.getAttribute('data-target');
    var el = document.getElementById(targetId);
    if (!el) return;

    minimapItems.forEach(function(mi) { mi.classList.remove('active'); });
    item.classList.add('active');
    scrollLock = true;

    window.scrollTo({ top: el.getBoundingClientRect().top + window.scrollY - 64, behavior: 'smooth' });

    setTimeout(function() { scrollLock = false; }, 800);
  });
});

/* Show minimap after hero */
var heroSection = document.querySelector('.hero');
var minimapShowObserver = new IntersectionObserver(function(entries) {
  entries.forEach(function(entry) {
    if (!entry.isIntersecting) {
      minimap.classList.add('visible');
    } else {
      minimap.classList.remove('visible');
    }
  });
}, { threshold: 0.5 });

if (heroSection) minimapShowObserver.observe(heroSection);

/* ─── INTERACTIVE VISUALS ─── */

// G1: Data Funnel
function initDataFunnel() {
  var el = document.getElementById('visual-data-funnel');
  if (!el) return;

  var stages = [
    { label: 'Raw Web Crawl', value: '240T tokens', pct: 100, color: 'var(--compute)' },
    { label: 'Quality Filtering', value: '38T tokens', pct: 15.8, color: 'var(--io)' },
    { label: 'Deduplication', value: '15T tokens', pct: 6.25, color: 'var(--network)' },
    { label: 'Curated Mix', value: '3.8T tokens', pct: 1.58, color: 'var(--logic)' },
    { label: 'Tokenized', value: '3.8T tokens', pct: 1.58, color: 'var(--accent)' }
  ];

  var html = '<div style="font-size:0.6rem;color:var(--text-muted);letter-spacing:0.15em;text-transform:uppercase;margin-bottom:1rem">Data Pipeline — DCLM Scale</div>';

  stages.forEach(function(s, i) {
    var barW = Math.max(s.pct, 8); // min visible width
    html += '<div class="funnel-stage">';
    html += '<div class="funnel-label">' + s.label + '</div>';
    html += '<div class="funnel-bar" style="width:' + barW + '%;background:' + s.color + '">' + s.value + '</div>';
    html += '</div>';
    if (i < stages.length - 1) {
      html += '<div class="funnel-arrow">&#9660;</div>';
    }
  });

  html += '<div style="display:flex;gap:0.75rem;margin-top:0.75rem;flex-wrap:wrap">';
  html += '<div style="font-size:0.65rem;color:var(--text-dim)">Keep rate: <strong style="color:var(--accent)">~1.6%</strong> of raw data</div>';
  html += '<div style="font-size:0.65rem;color:var(--text-dim)">Quality &gt; Quantity: top 10% by classifier score</div>';
  html += '</div>';

  el.innerHTML = html;
}

// H1: Optimizer Comparison
function initOptimizerCompare() {
  var el = document.getElementById('visual-optimizer-compare');
  if (!el) return;

  var opts = [
    { name: 'AdamW', steps: 100, memory: 12, color: 'var(--compute)', note: 'Baseline — 12 bytes/param' },
    { name: 'Muon', steps: 50, memory: 10, color: 'var(--accent)', note: '2x efficiency, PyTorch core (2025)' },
    { name: 'SOAP', steps: 60, memory: 10, color: 'var(--network)', note: '>40% fewer iterations' },
    { name: 'Lion', steps: 80, memory: 6, color: 'var(--logic)', note: '50% memory savings' }
  ];

  var html = '<div style="font-size:0.6rem;color:var(--text-muted);letter-spacing:0.15em;text-transform:uppercase;margin-bottom:1rem">Optimizer Comparison (Relative to AdamW Baseline)</div>';

  // Steps to converge
  html += '<div style="font-size:0.55rem;color:var(--text-muted);margin-bottom:0.5rem;letter-spacing:0.1em;text-transform:uppercase">Steps to Converge (fewer = better)</div>';
  html += '<div class="opt-chart">';
  opts.forEach(function(o) {
    html += '<div class="opt-row"><div class="opt-label">' + o.name + '</div>';
    html += '<div class="opt-bar-wrap"><div class="opt-bar" style="width:' + o.steps + '%;background:' + o.color + '">' + o.steps + '%</div></div>';
    html += '<div class="opt-val">' + o.note.split(',')[0] + '</div></div>';
  });
  html += '</div>';

  // Memory per param
  html += '<div style="font-size:0.55rem;color:var(--text-muted);margin:1rem 0 0.5rem;letter-spacing:0.1em;text-transform:uppercase">Memory per Parameter (bytes)</div>';
  html += '<div class="opt-chart">';
  opts.forEach(function(o) {
    var memPct = (o.memory / 12 * 100).toFixed(0);
    html += '<div class="opt-row"><div class="opt-label">' + o.name + '</div>';
    html += '<div class="opt-bar-wrap"><div class="opt-bar" style="width:' + memPct + '%;background:' + o.color + '">' + o.memory + 'B</div></div>';
    html += '<div class="opt-val">' + (o.memory === 12 ? 'Baseline' : (100 - parseInt(memPct)) + '% savings') + '</div></div>';
  });
  html += '</div>';

  // Legend
  html += '<div style="display:flex;flex-wrap:wrap;gap:0.75rem;margin-top:0.75rem">';
  opts.forEach(function(o) {
    html += '<div style="display:flex;align-items:center;gap:0.3rem"><span style="width:10px;height:10px;border-radius:2px;background:' + o.color + '"></span><span style="font-size:0.55rem;color:var(--text-dim)">' + o.name + '</span></div>';
  });
  html += '</div>';

  el.innerHTML = html;
}

// H2: Parallelism Strategy Diagram
function initParallelism() {
  var el = document.getElementById('visual-parallelism');
  if (!el) return;

  var modes = {
    dp: { title: 'Data Parallelism', desc: 'Each GPU holds full model, processes different data batches', colors: ['var(--compute)', 'var(--io)', 'var(--network)', 'var(--logic)'] },
    tp: { title: 'Tensor Parallelism', desc: 'Weight matrices split across GPUs within a node (NVLink)', colors: ['var(--accent)', 'var(--accent)', 'var(--accent)', 'var(--accent)'] },
    pp: { title: 'Pipeline Parallelism', desc: 'Different layers assigned to different nodes', colors: ['var(--compute)', 'var(--compute)', 'var(--io)', 'var(--io)'] },
    fourD: { title: '4D Parallelism (Llama 3)', desc: 'TP=8 × CP=16 × PP=16 × DP on 16,384 H100s', colors: ['var(--compute)', 'var(--io)', 'var(--network)', 'var(--logic)'] }
  };

  var html = '<div style="font-size:0.6rem;color:var(--text-muted);letter-spacing:0.15em;text-transform:uppercase;margin-bottom:1rem">Parallelism Strategies — 16-GPU Grid</div>';

  // Buttons
  html += '<div class="par-controls">';
  html += '<button class="par-btn active" data-mode="dp">Data ∥</button>';
  html += '<button class="par-btn" data-mode="tp">Tensor ∥</button>';
  html += '<button class="par-btn" data-mode="pp">Pipeline ∥</button>';
  html += '<button class="par-btn" data-mode="fourD">4D ∥</button>';
  html += '</div>';

  html += '<div id="par-desc" style="text-align:center;font-size:0.7rem;color:var(--text-dim);margin-bottom:0.75rem">' + modes.dp.desc + '</div>';
  html += '<div class="par-grid" id="par-grid"></div>';

  // Legend
  html += '<div id="par-legend" style="display:flex;flex-wrap:wrap;gap:0.75rem;margin-top:0.75rem;justify-content:center"></div>';

  el.innerHTML = html;

  var grid = document.getElementById('par-grid');
  var desc = document.getElementById('par-desc');
  var legend = document.getElementById('par-legend');

  function renderGrid(mode) {
    var m = modes[mode];
    var cells = '';
    var legendItems = {};

    for (var i = 0; i < 16; i++) {
      var color, label, legendLabel;
      if (mode === 'dp') {
        // Each GPU gets same model, different data — color by data batch
        var batch = i % 4;
        color = m.colors[batch];
        label = 'D' + batch;
        legendLabel = ['Batch 0','Batch 1','Batch 2','Batch 3'][batch];
      } else if (mode === 'tp') {
        // All GPUs work on same layer — same color, labeled by slice
        color = m.colors[0];
        var row = Math.floor(i / 4);
        label = 'S' + (i % 4);
        legendLabel = 'Weight Slice';
      } else if (mode === 'pp') {
        // Rows = pipeline stages
        var stage = Math.floor(i / 4);
        color = stage < 2 ? m.colors[0] : m.colors[2];
        label = 'L' + (stage * 2 + Math.floor((i % 4) / 2));
        legendLabel = stage < 2 ? 'Layers 0-3' : 'Layers 4-7';
      } else {
        // 4D: combine TP (cols) + PP (row pairs) + DP (color variation)
        var col = i % 4;
        var row = Math.floor(i / 4);
        color = m.colors[row];
        label = 'G' + i;
        legendLabel = ['TP group 0','TP group 1','PP stage 0','PP stage 1'][row];
      }
      legendItems[legendLabel] = color;
      cells += '<div class="par-cell" style="background:' + color + '">' + label + '</div>';
    }

    grid.innerHTML = cells;
    desc.textContent = m.desc;

    var legHtml = '';
    Object.keys(legendItems).forEach(function(k) {
      legHtml += '<div style="display:flex;align-items:center;gap:0.3rem"><span style="width:10px;height:10px;border-radius:2px;background:' + legendItems[k] + '"></span><span style="font-size:0.55rem;color:var(--text-dim)">' + k + '</span></div>';
    });
    legend.innerHTML = legHtml;
  }

  renderGrid('dp');

  el.querySelectorAll('.par-btn').forEach(function(btn) {
    btn.addEventListener('click', function(e) {
      e.stopPropagation();
      el.querySelectorAll('.par-btn').forEach(function(b) { b.classList.remove('active'); });
      btn.classList.add('active');
      renderGrid(btn.getAttribute('data-mode'));
    });
  });
}

initDataFunnel();
initOptimizerCompare();
initParallelism();

</script>


<script>
// ─── SEARCH ───
var CURRENT_PAGE = 'training.html';
var SEARCH_INDEX = [
  {page:'index.html',pageLabel:'Inference',anchor:'step-routing',title:'Request Routing',desc:'Load balancing, KV cache-aware routing, geo-aware, gateway frameworks'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-preprocessing',title:'Preprocessing',desc:'Prompt templates, RAG retrieval, rate limiting, input validation'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-tokenization',title:'Tokenization',desc:'BPE, SentencePiece, vocabulary sizes, tiktoken'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-embedding',title:'Embedding & Position',desc:'Token embeddings, RoPE, ALiBi, positional encoding'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-scheduling',title:'Scheduling & Batching',desc:'Continuous batching, chunked prefill, thinking model scheduling'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-prefill',title:'Prefill Phase',desc:'Parallel forward pass, TTFT, prefix caching, KV-Runahead'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-kvcache',title:'KV Cache & Memory',desc:'PagedAttention, KV compression, MLA, memory management'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-attention',title:'Attention Mechanisms',desc:'MHA, GQA, MQA, FlashAttention, MLA'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-decode',title:'Decode Phase',desc:'Autoregressive loop, speculative decoding, thinking model decode'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-sampling',title:'Sampling & Selection',desc:'Temperature, top-k, top-p, min-p, repetition penalties'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-streaming',title:'Detokenization & Streaming',desc:'UTF-8 buffering, SSE protocol, streaming pipeline'},
  {page:'index.html',pageLabel:'Inference',anchor:'inference-metrics',title:'Inference Metrics',desc:'TTFT, TPOT, throughput, GPU utilization, SLOs, cost per token'},
  {page:'index.html',pageLabel:'Inference',anchor:'cross-cutting',title:'Quantization & Parallelism',desc:'FP8, AWQ, GPTQ, tensor/pipeline/expert parallelism, LoRA serving'},
  {page:'index.html',pageLabel:'Inference',anchor:'frameworks',title:'Serving Frameworks',desc:'vLLM, SGLang, TensorRT-LLM, NVIDIA Dynamo'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'cost-stack',title:'The Cost Stack',desc:'GPU CapEx, power, data center, networking, operations'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'throughput',title:'Throughput as Margin',desc:'3-5x throughput variance; thinking model throughput impact'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'pricing',title:'Pricing Structures',desc:'Per-token, per-GPU-hour, reserved, batch, thinking token pricing'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'managed-vs-rental',title:'Managed vs GPU Rental',desc:'Statistical multiplexing, outcomes vs infrastructure'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'buy-vs-rent',title:'Buy vs Rent GPUs',desc:'Ownership cost model, breakeven calculator'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'data-centers',title:'Data Centers',desc:'Power costs, PUE, facility economics'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'equity-vs-debt',title:'Equity vs Debt',desc:'Risk, information asymmetry, tax shields'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'contracted-revenue',title:'Contracted Revenue',desc:'How pricing decisions unlock debt capacity'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'stage-by-stage',title:'Stage-by-Stage Capital',desc:'Series A to public capital structure evolution'},
  {page:'training.html',pageLabel:'Training',anchor:'data-pipeline',title:'Data Pipeline',desc:'Collection, filtering, dedup, mixing; DCLM, FineWeb'},
  {page:'training.html',pageLabel:'Training',anchor:'tokenizer',title:'Tokenizer Training',desc:'BPE, SentencePiece, vocabulary sizes, 128K tokens'},
  {page:'training.html',pageLabel:'Training',anchor:'architecture',title:'Model Architecture',desc:'Attention variants, SwiGLU, MoE, RoPE, RMSNorm'},
  {page:'training.html',pageLabel:'Training',anchor:'optimization',title:'Optimization',desc:'AdamW, Muon, WSD schedule, mixed precision, FP8'},
  {page:'training.html',pageLabel:'Training',anchor:'distributed',title:'Distributed Training',desc:'ZeRO, tensor/pipeline/4D parallelism, Megatron-LM'},
  {page:'training.html',pageLabel:'Training',anchor:'monitoring',title:'Monitoring & Recovery',desc:'MFU, loss spikes, checkpointing, failure recovery'},
  {page:'training.html',pageLabel:'Training',anchor:'sft',title:'Supervised Fine-Tuning',desc:'LoRA, supervised text/chat/vision, loss masking, instruction tuning'},
  {page:'training.html',pageLabel:'Training',anchor:'alignment',title:'Alignment',desc:'RLHF, DPO triplets, GRPO, Constitutional AI, KTO, thinking models'},
  {page:'training.html',pageLabel:'Training',anchor:'rft',title:'Reinforcement Fine-Tuning',desc:'Evaluator-driven RFT, environment hooks, parameter tuning, o1/o3/R1'},
  {page:'training.html',pageLabel:'Training',anchor:'evaluation',title:'Evaluation',desc:'MMLU, benchmarks, contamination detection, LiveBench'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'hardware-costs',title:'Hardware & Compute Costs',desc:'GPU pricing, cloud rental decline, cost benchmarks'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'scaling-laws',title:'Scaling Laws & Efficiency',desc:'Chinchilla, over-training, test-time compute scaling'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'failure-costs',title:'Failure & Wasted Compute',desc:'Training failures, wasted GPU-hours, checkpoint overhead'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'build-vs-buy',title:'Build vs Fine-Tune vs API',desc:'When to train, fine-tune, or use API — cost comparison'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'training-providers',title:'Training Providers',desc:'Foundation model companies, training-as-a-service'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'cloud-vs-onprem',title:'Cloud vs On-Premise',desc:'Cloud advantages, on-prem breakeven, hidden costs'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'gpu-financing',title:'GPU Financing',desc:'CoreWeave debt, sale-leaseback, GPU depreciation'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'model-funding',title:'Foundation Model Funding',desc:'Mega-rounds, Big Tech infrastructure spend'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'training-vs-inference',title:'Training vs Inference Spend',desc:'Spend ratio shift 2023–2026, revenue gap'},
];

var searchSelectedIdx = -1;
var searchSelectedEl = null;
var searchFlatResults = [];
var searchDebounceTimer = null;
var searchCrossPageRequested = false;
var SEARCH_PAGE_ORDER = ['Inference', 'Inference Economics', 'Training', 'Training Economics'];

function normalizeSearchText(text) {
  return (text || '')
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, ' ')
    .replace(/\s+/g, ' ')
    .trim();
}

function escapeHtml(text) {
  return (text || '')
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&#39;');
}

function tokenizeQuery(queryNorm) {
  if (!queryNorm) return [];
  return queryNorm.split(' ').filter(function(token) { return token.length > 1; });
}

function resolveAnchorTarget(root, anchor) {
  if (!root || !anchor) return null;
  var target = root.getElementById(anchor);
  if (target) return target;
  var stepKey = anchor.replace(/^step-/, '');
  target = root.querySelector('.pipeline-step[data-step="' + stepKey + '"]');
  if (target) return target;
  return root.querySelector('[data-step="' + stepKey + '"]');
}

function extractSectionText(root, anchor) {
  var target = resolveAnchorTarget(root, anchor);
  if (!target) return '';
  var text = (target.textContent || '').replace(/\s+/g, ' ').trim();
  if (text.length > 5000) text = text.slice(0, 5000);
  return text;
}

function rebuildSearchBlob(item) {
  var base = [item.title, item.desc, item.pageLabel, item.anchor.replace(/[-_]/g, ' ')].join(' ');
  item._titleNorm = normalizeSearchText(item.title);
  item._descNorm = normalizeSearchText(item.desc);
  item._contentNorm = normalizeSearchText(item._content || '');
  item._blobNorm = normalizeSearchText(base + ' ' + (item._content || ''));
}

function prepareSearchIndex() {
  SEARCH_INDEX.forEach(function(item) {
    if (item.page === CURRENT_PAGE) {
      item._content = extractSectionText(document, item.anchor);
    } else {
      item._content = item._content || '';
    }
    rebuildSearchBlob(item);
  });
}

function fuzzySubsequenceScore(query, text) {
  if (!query || !text) return 0;
  var qi = 0;
  var ti = 0;
  var start = -1;
  var last = -1;
  var gapPenalty = 0;

  while (qi < query.length && ti < text.length) {
    if (query.charAt(qi) === text.charAt(ti)) {
      if (start === -1) start = ti;
      if (last !== -1 && ti > last + 1) gapPenalty += (ti - last - 1);
      last = ti;
      qi++;
    }
    ti++;
  }

  if (qi !== query.length || start === -1 || last === -1) return 0;

  var span = last - start + 1;
  var compactness = query.length / Math.max(span, 1);
  var startBonus = start === 0 ? 0.2 : Math.max(0, 0.12 - start * 0.003);
  var penalty = Math.min(0.45, gapPenalty * 0.01);
  var score = compactness + startBonus - penalty;
  return Math.max(0, Math.min(1, score));
}

function scoreSearchItem(item, queryNorm, tokens) {
  if (!queryNorm) return 1;

  var titleNorm = item._titleNorm || '';
  var descNorm = item._descNorm || '';
  var blobNorm = item._blobNorm || '';

  var score = 0;
  var matched = false;

  if (titleNorm.indexOf(queryNorm) === 0) {
    score += 140;
    matched = true;
  } else if (titleNorm.indexOf(queryNorm) !== -1) {
    score += 110;
    matched = true;
  }

  if (descNorm.indexOf(queryNorm) !== -1) {
    score += 48;
    matched = true;
  }

  if (blobNorm.indexOf(queryNorm) !== -1) {
    score += 78;
    matched = true;
  }

  var tokenHits = 0;
  tokens.forEach(function(token) {
    if (titleNorm.indexOf(token) !== -1) {
      score += 34;
      tokenHits += 1;
      matched = true;
      return;
    }
    if (blobNorm.indexOf(token) !== -1) {
      score += 18;
      tokenHits += 1;
      matched = true;
      return;
    }

    var fuzzyTitle = fuzzySubsequenceScore(token, titleNorm);
    var fuzzyBlob = fuzzySubsequenceScore(token, blobNorm);
    var fuzzyToken = Math.max(fuzzyTitle, fuzzyBlob * 0.9);
    if (fuzzyToken >= 0.74) {
      score += fuzzyToken * 16;
      tokenHits += 0.6;
      matched = true;
    }
  });

  if (tokens.length) score += (tokenHits / tokens.length) * 20;

  var fuzzyWhole = Math.max(
    fuzzySubsequenceScore(queryNorm, titleNorm),
    fuzzySubsequenceScore(queryNorm, blobNorm) * 0.88
  );
  if (fuzzyWhole >= 0.76) {
    score += fuzzyWhole * 34;
    matched = true;
  }

  if (!matched || score < 22) return 0;
  if (item.page === CURRENT_PAGE) score += 6;
  return score;
}

function buildSearchSnippet(item, queryNorm, tokens) {
  var fallback = item.desc || '';
  var content = (item._content || '').replace(/\s+/g, ' ').trim();
  if (!content) return fallback;

  var lower = content.toLowerCase();
  var hit = -1;

  for (var i = 0; i < tokens.length; i++) {
    if (tokens[i].length < 3) continue;
    hit = lower.indexOf(tokens[i]);
    if (hit !== -1) break;
  }

  if (hit === -1 && queryNorm.length >= 3) {
    hit = lower.indexOf(queryNorm);
  }

  if (hit === -1) {
    return content.length > 160 ? content.slice(0, 160).trim() + '…' : content;
  }

  var start = Math.max(0, hit - 64);
  var end = Math.min(content.length, hit + 120);
  var prefix = start > 0 ? '…' : '';
  var suffix = end < content.length ? '…' : '';
  return prefix + content.slice(start, end).trim() + suffix;
}

function getSearchResults(query) {
  var queryNorm = normalizeSearchText(query);
  var tokens = tokenizeQuery(queryNorm);

  if (!queryNorm) {
    return SEARCH_INDEX.map(function(item) {
      return {
        item: item,
        score: item.page === CURRENT_PAGE ? 1 : 0,
        snippet: item.desc
      };
    }).sort(function(a, b) {
      if (b.score !== a.score) return b.score - a.score;
      var pageDelta = SEARCH_PAGE_ORDER.indexOf(a.item.pageLabel) - SEARCH_PAGE_ORDER.indexOf(b.item.pageLabel);
      if (pageDelta !== 0) return pageDelta;
      return a.item.title.localeCompare(b.item.title);
    });
  }

  var matches = [];
  SEARCH_INDEX.forEach(function(item) {
    var score = scoreSearchItem(item, queryNorm, tokens);
    if (score <= 0) return;
    matches.push({
      item: item,
      score: score,
      snippet: buildSearchSnippet(item, queryNorm, tokens)
    });
  });

  matches.sort(function(a, b) {
    if (b.score !== a.score) return b.score - a.score;
    var pageDelta = SEARCH_PAGE_ORDER.indexOf(a.item.pageLabel) - SEARCH_PAGE_ORDER.indexOf(b.item.pageLabel);
    if (pageDelta !== 0) return pageDelta;
    return a.item.title.localeCompare(b.item.title);
  });

  return matches.slice(0, 120);
}

function openSearch() {
  var overlay = document.getElementById('search-overlay');
  var input = document.getElementById('search-input');
  if (!overlay || !input) return;
  overlay.classList.add('open');
  input.value = '';
  input.focus();
  searchSelectedIdx = -1;
  searchSelectedEl = null;
  renderSearchResults('');
  warmCrossPageContent();
}

function closeSearch() {
  var overlay = document.getElementById('search-overlay');
  if (overlay) overlay.classList.remove('open');
}

function setSearchSelected(idx) {
  if (idx < 0 || idx >= searchFlatResults.length) return;
  if (searchSelectedIdx === idx) return;

  if (searchSelectedEl) searchSelectedEl.classList.remove('selected');

  searchSelectedIdx = idx;
  var container = document.getElementById('search-results');
  searchSelectedEl = container ? container.querySelector('.search-result[data-idx="' + idx + '"]') : null;
  if (searchSelectedEl) searchSelectedEl.classList.add('selected');
}

function renderSearchResults(query) {
  var container = document.getElementById('search-results');
  if (!container) return;

  var results = getSearchResults(query || '');
  searchFlatResults = results;
  searchSelectedIdx = -1;
  searchSelectedEl = null;

  if (!results.length) {
    container.innerHTML = '<div class="search-no-results">No results for &ldquo;' + escapeHtml(query || '') + '&rdquo;</div>';
    return;
  }

  var grouped = {};
  results.forEach(function(entry, idx) {
    var label = entry.item.pageLabel;
    if (!grouped[label]) grouped[label] = [];
    grouped[label].push({ entry: entry, idx: idx });
  });

  var labels = SEARCH_PAGE_ORDER.slice();
  Object.keys(grouped).forEach(function(label) {
    if (labels.indexOf(label) === -1) labels.push(label);
  });

  var html = '';
  labels.forEach(function(label) {
    if (!grouped[label] || !grouped[label].length) return;
    html += '<div class="search-group-header">' + escapeHtml(label) + '</div>';

    grouped[label].forEach(function(row) {
      var item = row.entry.item;
      var snippet = row.entry.snippet || item.desc || '';
      html += '<div class="search-result" data-idx="' + row.idx + '" data-page="' + item.page + '" data-anchor="' + item.anchor + '">' +
        '<div class="search-result-dot"></div>' +
        '<div class="search-result-body"><div class="search-result-title">' + escapeHtml(item.title) + '</div>' +
        '<div class="search-result-desc">' + escapeHtml(snippet) + '</div></div>' +
        '<span class="search-result-badge">' + (item.page === CURRENT_PAGE ? 'this page' : item.pageLabel.toLowerCase()) + '</span>' +
      '</div>';
    });
  });

  container.innerHTML = html;
}

function scheduleSearchRender(query) {
  if (searchDebounceTimer) clearTimeout(searchDebounceTimer);
  searchDebounceTimer = setTimeout(function() {
    renderSearchResults(query);
  }, 70);
}

function navigateToResult(el) {
  var page = el.getAttribute('data-page');
  var anchor = el.getAttribute('data-anchor');
  if (!page || !anchor) return;

  closeSearch();

  if (page === CURRENT_PAGE) {
    var target = resolveAnchorTarget(document, anchor);
    if (target) {
      if (target.classList.contains('pipeline-step') && !target.classList.contains('active')) {
        target.classList.add('active');
      }
      window.scrollTo({ top: target.getBoundingClientRect().top + window.scrollY - 64, behavior: 'smooth' });
    }
  } else {
    window.location.href = page + '#' + anchor;
  }
}

function warmCrossPageContent() {
  if (searchCrossPageRequested || typeof fetch !== 'function' || typeof DOMParser === 'undefined') return;
  searchCrossPageRequested = true;

  var pages = [];
  var seen = {};
  SEARCH_INDEX.forEach(function(item) {
    if (item.page === CURRENT_PAGE || seen[item.page]) return;
    seen[item.page] = true;
    pages.push(item.page);
  });

  if (!pages.length) {
    return;
  }

  Promise.allSettled(pages.map(function(page) {
    return fetch(page, { cache: 'force-cache', credentials: 'same-origin' })
      .then(function(response) {
        if (!response.ok) return '';
        return response.text();
      })
      .then(function(html) {
        if (!html) return;
        var parser = new DOMParser();
        var doc = parser.parseFromString(html, 'text/html');
        SEARCH_INDEX.forEach(function(item) {
          if (item.page !== page) return;
          var extracted = extractSectionText(doc, item.anchor);
          if (extracted) {
            item._content = extracted;
            rebuildSearchBlob(item);
          }
        });
      })
      .catch(function() {});
  })).then(function() {
    var overlay = document.getElementById('search-overlay');
    var input = document.getElementById('search-input');
    if (overlay && overlay.classList.contains('open') && input && input.value.trim()) {
      renderSearchResults(input.value);
    }
  });
}

prepareSearchIndex();

document.addEventListener('keydown', function(e) {
  if ((e.metaKey || e.ctrlKey) && e.key.toLowerCase() === 'k') {
    e.preventDefault();
    var overlay = document.getElementById('search-overlay');
    if (!overlay) return;
    if (overlay.classList.contains('open')) { closeSearch(); } else { openSearch(); }
    return;
  }

  var overlay = document.getElementById('search-overlay');
  if (!overlay || !overlay.classList.contains('open')) return;

  if (e.key === 'Escape') {
    closeSearch();
    return;
  }

  if (!searchFlatResults.length) return;

  if (e.key === 'ArrowDown') {
    e.preventDefault();
    var next = searchSelectedIdx < 0 ? 0 : Math.min(searchSelectedIdx + 1, searchFlatResults.length - 1);
    setSearchSelected(next);
    if (searchSelectedEl) searchSelectedEl.scrollIntoView({ block: 'nearest' });
  } else if (e.key === 'ArrowUp') {
    e.preventDefault();
    var prev = searchSelectedIdx <= 0 ? 0 : searchSelectedIdx - 1;
    setSearchSelected(prev);
    if (searchSelectedEl) searchSelectedEl.scrollIntoView({ block: 'nearest' });
  } else if (e.key === 'Enter') {
    e.preventDefault();
    var idx = searchSelectedIdx >= 0 ? searchSelectedIdx : 0;
    var container = document.getElementById('search-results');
    var target = container ? container.querySelector('.search-result[data-idx="' + idx + '"]') : null;
    if (target) navigateToResult(target);
  }
});

var searchInputEl = document.getElementById('search-input');
if (searchInputEl) {
  searchInputEl.addEventListener('input', function(e) {
    scheduleSearchRender(e.target.value);
  });
}

var searchResultsEl = document.getElementById('search-results');
if (searchResultsEl) {
  searchResultsEl.addEventListener('click', function(e) {
    var resultEl = e.target.closest('.search-result');
    if (!resultEl || !searchResultsEl.contains(resultEl)) return;
    navigateToResult(resultEl);
  });

  searchResultsEl.addEventListener('mouseover', function(e) {
    var resultEl = e.target.closest('.search-result');
    if (!resultEl || !searchResultsEl.contains(resultEl)) return;
    var idx = parseInt(resultEl.getAttribute('data-idx'), 10);
    if (isNaN(idx)) return;
    setSearchSelected(idx);
  });
}

var searchOverlayEl = document.getElementById('search-overlay');
if (searchOverlayEl) {
  searchOverlayEl.addEventListener('click', function(e) {
    if (e.target === searchOverlayEl) closeSearch();
  });
}

var searchTriggerEl = document.getElementById('search-trigger');
if (searchTriggerEl) {
  searchTriggerEl.addEventListener('click', function() { openSearch(); });
}

// Update shortcut label for non-Mac
(function() {
  var isMac = /Mac|iPhone|iPad|iPod/.test(navigator.platform || navigator.userAgent);
  if (!isMac) {
    var hint = document.getElementById('search-key-hint');
    if (hint) hint.textContent = 'Ctrl+K';
  }
})();

// Handle hash anchor from cross-page search navigation
(function() {
  if (!window.location.hash) return;
  var anchor = window.location.hash.slice(1);
  setTimeout(function() {
    var target = resolveAnchorTarget(document, anchor);
    if (!target) return;
    if (target.classList.contains('pipeline-step') && !target.classList.contains('active')) target.classList.add('active');
    window.scrollTo({ top: target.getBoundingClientRect().top + window.scrollY - 64, behavior: 'smooth' });
  }, 180);
})();

</script>

<script>
// ─── KNOWLEDGE MARKDOWN DOWNLOAD ───
(function() {
  var downloadBtn = document.getElementById('download-knowledge');
  if (!downloadBtn) return;

  var KNOWLEDGE_PAGES = [
    { path: 'index.html', label: 'Inference Technical Pipeline' },
    { path: 'economics.html', label: 'Inference Economics' },
    { path: 'training.html', label: 'Training Technical Pipeline' },
    { path: 'training-economics.html', label: 'Training Economics' }
  ];

  function normalizeWhitespace(text) {
    return (text || '').replace(/\s+/g, ' ').trim();
  }

  function resolveHref(href, baseUrl) {
    if (!href) return '';
    if (href.charAt(0) === '#') return baseUrl + href;
    try {
      return new URL(href, baseUrl).href;
    } catch (err) {
      return href;
    }
  }

  function inlineFromNode(node, baseUrl) {
    if (!node) return '';
    if (node.nodeType === Node.TEXT_NODE) return node.nodeValue || '';
    if (node.nodeType !== Node.ELEMENT_NODE) return '';

    var tag = node.tagName.toLowerCase();
    if (tag === 'br') return '  \n';

    var childText = '';
    node.childNodes.forEach(function(child) {
      childText += inlineFromNode(child, baseUrl);
    });

    childText = normalizeWhitespace(childText);

    if (!childText && tag !== 'a') return '';
    if (tag === 'strong' || tag === 'b') return '**' + childText + '**';
    if (tag === 'em' || tag === 'i') return '*' + childText + '*';
    if (tag === 'code') return '`' + childText.replace(/`/g, '\\`') + '`';
    if (tag === 'a') {
      var href = resolveHref(node.getAttribute('href') || '', baseUrl);
      var label = childText || href;
      return href ? '[' + label + '](' + href + ')' : label;
    }

    return childText;
  }

  function inlineText(element, baseUrl) {
    if (!element) return '';
    var text = '';
    element.childNodes.forEach(function(child) {
      text += inlineFromNode(child, baseUrl) + ' ';
    });
    return normalizeWhitespace(text);
  }

  function markdownFromList(listEl, baseUrl, depth) {
    var level = depth || 0;
    var isOrdered = listEl.tagName.toLowerCase() === 'ol';
    var lines = [];

    Array.from(listEl.children).forEach(function(li, idx) {
      if (!li || li.tagName.toLowerCase() !== 'li') return;

      var itemTextParts = [];
      li.childNodes.forEach(function(node) {
        if (node.nodeType === Node.ELEMENT_NODE) {
          var tag = node.tagName.toLowerCase();
          if (tag === 'ul' || tag === 'ol') return;
        }
        itemTextParts.push(inlineFromNode(node, baseUrl));
      });

      var itemText = normalizeWhitespace(itemTextParts.join(' '));
      if (!itemText) itemText = '(item)';
      var prefix = isOrdered ? ((idx + 1) + '.') : '-';
      lines.push(Array(level + 1).join('  ') + prefix + ' ' + itemText);

      Array.from(li.children).forEach(function(child) {
        var childTag = child.tagName.toLowerCase();
        if (childTag === 'ul' || childTag === 'ol') {
          var nested = markdownFromList(child, baseUrl, level + 1);
          if (nested) lines.push(nested);
        }
      });
    });

    return lines.join('\n');
  }

  function escapeTableCell(text) {
    return (text || '').replace(/\|/g, '\\|');
  }

  function markdownFromTable(tableEl, baseUrl) {
    var rows = Array.from(tableEl.querySelectorAll('tr')).map(function(tr) {
      return Array.from(tr.children)
        .filter(function(cell) {
          var tag = cell.tagName.toLowerCase();
          return tag === 'th' || tag === 'td';
        })
        .map(function(cell) {
          return escapeTableCell(inlineText(cell, baseUrl));
        });
    }).filter(function(row) {
      return row.length > 0;
    });

    if (!rows.length) return '';

    var colCount = rows.reduce(function(max, row) {
      return Math.max(max, row.length);
    }, 0);

    rows = rows.map(function(row) {
      var copy = row.slice(0);
      while (copy.length < colCount) copy.push('');
      return copy;
    });

    var header = rows[0];
    var body = rows.slice(1);
    var sep = [];
    for (var i = 0; i < colCount; i++) sep.push('---');

    var md = [];
    md.push('| ' + header.join(' | ') + ' |');
    md.push('| ' + sep.join(' | ') + ' |');
    body.forEach(function(row) {
      md.push('| ' + row.join(' | ') + ' |');
    });

    return md.join('\n');
  }

  function shouldSkipBlock(el) {
    if (!el) return true;
    if (el.closest('nav, .minimap, .search-overlay, .footer, .journey-detail-panel, .term-tooltip')) return true;
    return false;
  }

  function sectionToMarkdown(section, baseUrl, headingToSkip) {
    var blocks = section.querySelectorAll('h1,h2,h3,h4,h5,p,ul,ol,pre,table');
    var out = [];
    var previous = '';

    blocks.forEach(function(el) {
      if (el === headingToSkip) return;
      if (shouldSkipBlock(el)) return;

      var tag = el.tagName.toLowerCase();
      var chunk = '';

      if ((tag === 'p' && el.closest('li')) || (tag === 'p' && el.closest('table'))) return;
      if ((tag === 'ul' || tag === 'ol') && el.closest('li')) return;
      if ((tag === 'pre' || tag === 'code') && el.closest('table')) return;

      if (tag === 'h1' || tag === 'h2' || tag === 'h3' || tag === 'h4' || tag === 'h5') {
        var headingText = inlineText(el, baseUrl);
        if (!headingText) return;
        var level = parseInt(tag.charAt(1), 10) + 2;
        if (level > 6) level = 6;
        chunk = Array(level + 1).join('#') + ' ' + headingText;
      } else if (tag === 'p') {
        chunk = inlineText(el, baseUrl);
      } else if (tag === 'ul' || tag === 'ol') {
        chunk = markdownFromList(el, baseUrl, 0);
      } else if (tag === 'pre') {
        var codeText = (el.textContent || '').trim();
        if (codeText) chunk = '```\n' + codeText + '\n```';
      } else if (tag === 'table') {
        chunk = markdownFromTable(el, baseUrl);
      }

      chunk = (chunk || '').replace(/\n\s+\n/g, '\n\n').replace(/\n{3,}/g, '\n\n').trim();
      if (!chunk || chunk === previous) return;
      out.push(chunk);
      previous = chunk;
    });

    return out.join('\n\n').trim();
  }

  function parsePageToMarkdown(html, pageMeta, pageUrl) {
    var parser = new DOMParser();
    var doc = parser.parseFromString(html, 'text/html');
    var lines = [];

    lines.push('## ' + pageMeta.label);
    lines.push('Source: [' + pageMeta.path + '](' + pageUrl + ')');

    var heroTitleEl = doc.querySelector('.hero h1');
    var heroTitle = heroTitleEl ? inlineText(heroTitleEl, pageUrl) : normalizeWhitespace(doc.title || pageMeta.label);
    if (heroTitle) lines.push('### ' + heroTitle);

    var heroSubEl = doc.querySelector('.hero-sub') || doc.querySelector('.hero p');
    var heroSub = heroSubEl ? inlineText(heroSubEl, pageUrl) : '';
    if (heroSub) lines.push(heroSub);

    var sections = Array.from(doc.querySelectorAll('section'));
    sections.forEach(function(section) {
      if (section.classList.contains('hero')) return;
      if (shouldSkipBlock(section)) return;

      var sectionHeadingEl = section.querySelector('h2,h3');
      var sectionHeading = sectionHeadingEl ? inlineText(sectionHeadingEl, pageUrl) : '';
      if (!sectionHeading && section.id) sectionHeading = section.id.replace(/[-_]/g, ' ');
      if (sectionHeading) lines.push('### ' + sectionHeading);

      var sectionBody = sectionToMarkdown(section, pageUrl, sectionHeadingEl);
      if (sectionBody) lines.push(sectionBody);
    });

    return lines.join('\n\n').replace(/\n{3,}/g, '\n\n').trim();
  }

  async function buildKnowledgeMarkdown() {
    var generatedAt = new Date();
    var parts = [];

    parts.push('# LLM Anatomy Knowledge Base');
    parts.push('');
    parts.push('Generated: ' + generatedAt.toISOString());
    parts.push('Origin: ' + window.location.origin);
    parts.push('');
    parts.push('This markdown file is generated from the live website at download time.');
    parts.push('');

    for (var i = 0; i < KNOWLEDGE_PAGES.length; i++) {
      var page = KNOWLEDGE_PAGES[i];
      var pageUrl = new URL(page.path, window.location.href).href;

      try {
        var response = await fetch(pageUrl, { cache: 'no-store', credentials: 'same-origin' });
        if (!response.ok) throw new Error('HTTP ' + response.status);

        var html = await response.text();
        var pageMd = parsePageToMarkdown(html, page, pageUrl);
        parts.push(pageMd || ('## ' + page.label + '\n\nSource: [' + page.path + '](' + pageUrl + ')\n\n_No extractable content found._'));
      } catch (err) {
        parts.push('## ' + page.label);
        parts.push('Source: [' + page.path + '](' + pageUrl + ')');
        parts.push('');
        parts.push('_Failed to fetch this page while generating knowledge markdown: ' + (err && err.message ? err.message : 'Unknown error') + '._');
      }

      if (i < KNOWLEDGE_PAGES.length - 1) {
        parts.push('');
        parts.push('---');
        parts.push('');
      }
    }

    return parts.join('\n').replace(/\n{3,}/g, '\n\n').trim() + '\n';
  }

  function downloadMarkdown(content) {
    var stamp = new Date().toISOString().replace(/:/g, '-').replace(/\.\d+Z$/, 'Z');
    var fileName = 'llm-anatomy-knowledge-' + stamp + '.md';

    var blob = new Blob([content], { type: 'text/markdown;charset=utf-8' });
    var url = URL.createObjectURL(blob);
    var a = document.createElement('a');
    a.href = url;
    a.download = fileName;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  }

  downloadBtn.addEventListener('click', async function() {
    if (downloadBtn.disabled) return;

    var labelEl = downloadBtn.querySelector('.search-trigger-label');
    var originalLabel = labelEl ? labelEl.textContent : '';

    downloadBtn.disabled = true;
    if (labelEl) labelEl.textContent = 'Preparing...';

    try {
      var markdown = await buildKnowledgeMarkdown();
      downloadMarkdown(markdown);
    } catch (err) {
      console.error('Knowledge markdown generation failed:', err);
      window.alert('Failed to generate knowledge markdown. Please try again.');
    } finally {
      downloadBtn.disabled = false;
      if (labelEl) labelEl.textContent = originalLabel;
    }
  });
})();
</script>

<!-- ─── SEARCH MODAL ─── -->
<div class="search-overlay" id="search-overlay" role="dialog" aria-modal="true" aria-label="Site search">
  <div class="search-modal">
    <div class="search-input-wrap">
      <svg class="search-icon-svg" viewBox="0 0 16 16" stroke="currentColor" stroke-width="1.5"><circle cx="6.5" cy="6.5" r="4.5"/><path stroke-linecap="round" d="m10 10 3.5 3.5"/></svg>
      <input type="text" class="search-input" id="search-input" placeholder="Search sections, topics, terms..." autocomplete="off" spellcheck="false">
      <kbd class="search-esc-hint">ESC</kbd>
    </div>
    <div class="search-results" id="search-results"></div>
    <div class="search-footer">
      <span class="search-footer-hint"><span class="search-footer-key">&#x2191;&#x2193;</span>&nbsp;navigate</span>
      <span class="search-footer-hint"><span class="search-footer-key">&#x23CE;</span>&nbsp;select</span>
      <span class="search-footer-hint"><span class="search-footer-key">ESC</span>&nbsp;close</span>
    </div>
  </div>
</div>

</body>
</html>
