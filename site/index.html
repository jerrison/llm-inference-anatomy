<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Anatomy of LLM Inference</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:wght@400;700&family=DM+Mono:wght@300;400;500&family=Instrument+Serif:ital@0;1&display=swap" rel="stylesheet">
<style>
/* ─── RESET & BASE ─── */
*, *::before, *::after { margin:0; padding:0; box-sizing:border-box; }

:root {
  --bg: #0a0a0c;
  --bg-card: #111115;
  --bg-card-hover: #18181e;
  --border: #1e1e28;
  --border-active: #3a3a50;
  --text: #e8e6e3;
  --text-dim: #9a98a0;
  --text-muted: #4a4850;
  --accent: #56B4E9;
  --accent-dim: #3a8dc4;
  --accent-glow: rgba(86,180,233,0.08);
  --compute: #D55E00;
  --memory: #0090D6;
  --io: #E69F00;
  --logic: #CC79A7;
  --network: #56B4E9;
  --font-display: 'Instrument Serif', serif;
  --font-body: 'Atkinson Hyperlegible', sans-serif;
  --font-mono: 'DM Mono', monospace;
}

html { scroll-behavior: smooth; }
body {
  background: var(--bg);
  color: var(--text);
  font-family: var(--font-body);
  font-weight: 400;
  line-height: 1.8;
  overflow-x: hidden;
  -webkit-font-smoothing: antialiased;
  padding-top: 48px;
}

::selection { background: var(--accent); color: var(--bg); }

/* ─── HERO ─── */
.hero {
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  text-align: center;
  padding: 2rem;
  position: relative;
  overflow: hidden;
}

.hero::before {
  content: '';
  position: absolute;
  inset: 0;
  background:
    radial-gradient(ellipse 80% 50% at 50% 0%, rgba(86,180,233,0.04) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 20% 80%, rgba(0,144,214,0.03) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 80% 80%, rgba(204,121,167,0.03) 0%, transparent 50%);
  pointer-events: none;
}

.hero-label {
  font-family: var(--font-mono);
  font-size: 0.7rem;
  letter-spacing: 0.3em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 2rem;
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.2s;
}

.hero h1 {
  font-family: var(--font-display);
  font-size: clamp(3rem, 8vw, 7rem);
  font-weight: 400;
  line-height: 1.05;
  letter-spacing: -0.02em;
  max-width: 900px;
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.4s;
}

.hero h1 em {
  font-style: italic;
  color: var(--accent);
}

.hero-sub {
  font-size: clamp(1rem, 2vw, 1.2rem);
  color: var(--text-dim);
  max-width: 600px;
  margin-top: 1.5rem;
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.6s;
}

.hero-cta {
  margin-top: 3rem;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  font-family: var(--font-mono);
  font-size: 0.8rem;
  color: var(--text-muted);
  opacity: 0;
  animation: fadeUp 0.8s ease forwards 0.8s;
}

.hero-cta .arrow {
  display: inline-block;
  animation: bounce 2s ease infinite;
}

@keyframes fadeUp {
  from { opacity: 0; transform: translateY(20px); }
  to { opacity: 1; transform: translateY(0); }
}

@keyframes bounce {
  0%, 100% { transform: translateY(0); }
  50% { transform: translateY(6px); }
}

/* ─── PHASE OVERVIEW ─── */
.phase-overview {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 1.5rem;
  max-width: 1200px;
  margin: 0 auto;
  padding: 0 2rem 4rem;
}

.phase-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 1.5rem;
  cursor: pointer;
  transition: border-color 0.18s ease, background-color 0.18s ease, transform 0.18s ease, box-shadow 0.18s ease;
  position: relative;
}

.phase-card:hover {
  border-color: var(--border-active);
  background: var(--bg-card-hover);
}

.phase-letter {
  font-family: var(--font-display);
  font-size: 2.5rem;
  color: var(--accent);
  line-height: 1;
  margin-bottom: 0.5rem;
}

.phase-name {
  font-family: var(--font-display);
  font-size: 1.3rem;
  margin-bottom: 0.25rem;
}

.phase-subtitle {
  font-size: 0.85rem;
  color: var(--text-dim);
  margin-bottom: 1rem;
}

.phase-steps {
  display: flex;
  flex-direction: column;
  gap: 0.25rem;
}

.phase-steps span {
  font-family: var(--font-mono);
  font-size: 0.72rem;
  color: var(--text-muted);
  letter-spacing: 0.05em;
}

.phase-card:not(:last-child)::after {
  content: '\2192';
  position: absolute;
  right: -1rem;
  top: 50%;
  transform: translate(50%, -50%);
  color: var(--text-muted);
  font-size: 1.2rem;
  z-index: 3;
}

/* ─── PIPELINE OVERVIEW ─── */
.pipeline-section {
  padding: 4rem 2rem 6rem;
  max-width: 1200px;
  margin: 0 auto;
}

.section-label {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  letter-spacing: 0.3em;
  text-transform: uppercase;
  color: var(--text-muted);
  margin-bottom: 0.5rem;
}

.section-title {
  font-family: var(--font-display);
  font-size: clamp(2rem, 4vw, 3.5rem);
  font-weight: 400;
  margin-bottom: 1rem;
}

.section-desc {
  color: var(--text-dim);
  max-width: 650px;
  margin-bottom: 3rem;
  font-size: 1rem;
}

/* ─── PHASE DIVIDERS ─── */
.phase-divider {
  display: grid;
  grid-template-columns: 48px 1fr;
  padding: 2rem 0 0.5rem;
  position: relative;
}

.phase-divider:first-child { padding-top: 0; }

.phase-divider-marker {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  border-radius: 8px;
  background: var(--accent);
  color: var(--bg);
  font-family: var(--font-mono);
  font-weight: 700;
  font-size: 0.75rem;
  margin-left: 8px;
  z-index: 2;
}

.phase-divider-content {
  padding: 0.25rem 0 0 0.75rem;
}

.phase-divider-name {
  font-family: var(--font-display);
  font-size: 1.15rem;
  color: var(--accent);
}

.phase-divider-desc {
  font-size: 0.8rem;
  color: var(--text-muted);
}

/* ─── PIPELINE FLOW DIAGRAM ─── */
.pipeline-flow {
  display: flex;
  flex-direction: column;
  gap: 0;
  position: relative;
}

.pipeline-flow::before {
  content: '';
  position: absolute;
  left: 24px;
  top: 0;
  bottom: 0;
  width: 1px;
  background: linear-gradient(to bottom, transparent, var(--border) 5%, var(--border) 95%, transparent);
}

.pipeline-step {
  display: grid;
  grid-template-columns: 48px 1fr;
  gap: 0;
  cursor: pointer;
  position: relative;
}

.step-marker {
  display: flex;
  flex-direction: column;
  align-items: center;
  position: relative;
  z-index: 2;
}

.step-dot {
  width: 10px;
  height: 10px;
  border-radius: 50%;
  border: 2px solid var(--border);
  background: var(--bg);
  margin-top: 1.5rem;
  transition: border-color 0.18s ease, background-color 0.18s ease, box-shadow 0.18s ease;
  flex-shrink: 0;
}

.pipeline-step:hover .step-dot,
.pipeline-step.active .step-dot {
  border-color: var(--accent);
  background: var(--accent);
  box-shadow: 0 0 12px rgba(86,180,233,0.3);
}

.step-card {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 1.25rem 1.5rem;
  margin: 0.5rem 0;
  transition: border-color 0.18s ease, background-color 0.18s ease;
  position: relative;
  overflow: hidden;
}

.step-card::before {
  content: '';
  position: absolute;
  inset: 0;
  background: linear-gradient(135deg, var(--accent-glow) 0%, transparent 50%);
  opacity: 0;
  transition: opacity 0.18s ease;
}

.pipeline-step:hover .step-card,
.pipeline-step.active .step-card {
  border-color: var(--border-active);
  background: var(--bg-card-hover);
}

.pipeline-step:hover .step-card::before,
.pipeline-step.active .step-card::before {
  opacity: 1;
}

.step-header {
  display: flex;
  align-items: center;
  gap: 0.75rem;
  position: relative;
  z-index: 1;
  cursor: pointer;
}

.step-number {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  color: var(--accent-dim);
  letter-spacing: 0.1em;
  min-width: 24px;
}

.step-name {
  font-family: var(--font-display);
  font-size: 1.4rem;
  flex: 1;
}

.step-badge {
  font-family: var(--font-mono);
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  padding: 0.2rem 0.5rem;
  border-radius: 4px;
  border: 1px solid;
}

.step-badge::before { margin-right: 0.3em; }
.badge-compute { color: var(--compute); border-color: rgba(213,94,0,0.3); background: rgba(213,94,0,0.05); }
.badge-compute::before { content: '\26A1'; }
.badge-memory { color: var(--memory); border-color: rgba(0,144,214,0.3); background: rgba(0,144,214,0.05); }
.badge-memory::before { content: '\2630'; }
.badge-io { color: var(--io); border-color: rgba(230,159,0,0.3); background: rgba(230,159,0,0.05); }
.badge-io::before { content: '\21C4'; }
.badge-logic { color: var(--logic); border-color: rgba(204,121,167,0.3); background: rgba(204,121,167,0.05); }
.badge-logic::before { content: '\2699'; }
.badge-network { color: var(--network); border-color: rgba(86,180,233,0.3); background: rgba(86,180,233,0.05); }
.badge-network::before { content: '\25CE'; }

.step-summary {
  color: var(--text-dim);
  font-size: 0.9rem;
  margin-top: 0.5rem;
  position: relative;
  z-index: 1;
}

.step-expand-icon {
  color: var(--text-muted);
  font-size: 1.2rem;
  transition: transform 0.18s ease;
  font-family: var(--font-mono);
}

.pipeline-step.active .step-expand-icon {
  transform: rotate(45deg);
  color: var(--accent);
}

/* ─── EXPANDED DETAIL ─── */
.step-detail {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.18s ease-out;
  position: relative;
  z-index: 1;
}

.pipeline-step.active .step-detail {
  max-height: 8000px;
}

.detail-inner {
  padding: 1.5rem 0 0.5rem;
  border-top: 1px solid var(--border);
  margin-top: 1rem;
}

.detail-section {
  margin-bottom: 1.5rem;
}

.detail-section:last-child { margin-bottom: 0; }

.detail-label {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--accent-dim);
  margin-bottom: 0.5rem;
}

.detail-text {
  font-size: 0.92rem;
  color: var(--text-dim);
  line-height: 1.8;
}

.detail-text strong {
  color: var(--text);
  font-weight: 400;
}

.detail-text code {
  font-family: var(--font-mono);
  font-size: 0.78rem;
  background: rgba(86,180,233,0.06);
  color: var(--accent);
  padding: 0.15rem 0.4rem;
  border-radius: 4px;
}

/* ─── TAKEAWAY & ANALOGY ─── */
.callout-box {
  border-radius: 8px;
  padding: 0.85rem 1rem;
  margin-bottom: 1.25rem;
  font-size: 0.9rem;
  line-height: 1.7;
}

.callout-takeaway {
  background: rgba(86,180,233,0.06);
  border-left: 3px solid var(--accent);
}

.callout-takeaway .callout-label {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.3rem;
}

.callout-takeaway p {
  color: var(--text);
  font-weight: 700;
}

.callout-analogy {
  background: rgba(204,121,167,0.05);
  border-left: 3px solid var(--logic);
}

.callout-analogy .callout-label {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--logic);
  margin-bottom: 0.3rem;
}

.callout-analogy p {
  color: var(--text-dim);
  font-style: italic;
}

/* Sub-topics (drill-in level 2) */
.sub-topics {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
  gap: 0.75rem;
  margin-top: 0.75rem;
}

.sub-topic {
  background: rgba(255,255,255,0.02);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 1rem;
  cursor: pointer;
  transition: border-color 0.18s ease, background-color 0.18s ease, transform 0.18s ease, box-shadow 0.18s ease;
}

.sub-topic:hover {
  border-color: var(--border-active);
  background: rgba(255,255,255,0.04);
}

.sub-topic.expanded {
  grid-column: 1 / -1;
  border-color: var(--accent-dim);
}

.sub-topic-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
}

.sub-topic-name {
  font-family: var(--font-display);
  font-size: 1.1rem;
}

.sub-topic-icon {
  color: var(--text-muted);
  font-family: var(--font-mono);
  font-size: 0.9rem;
  transition: transform 0.3s ease;
}

.sub-topic.expanded .sub-topic-icon {
  transform: rotate(45deg);
  color: var(--accent);
}

.sub-topic-preview {
  color: var(--text-muted);
  font-size: 0.8rem;
  margin-top: 0.35rem;
}

.sub-topic-detail {
  display: none;
  margin-top: 1rem;
  padding-top: 1rem;
  border-top: 1px solid var(--border);
}

.sub-topic.expanded .sub-topic-detail {
  display: block;
}

.sub-topic-detail p {
  font-size: 0.9rem;
  color: var(--text-dim);
  margin-bottom: 0.75rem;
  line-height: 1.8;
}

.sub-topic-detail p:last-child { margin-bottom: 0; }

/* Code blocks */
.code-block {
  background: #0d0d10;
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 1rem 1.25rem;
  margin: 0.75rem 0;
  font-family: var(--font-mono);
  font-size: 0.75rem;
  line-height: 1.8;
  color: var(--text-dim);
  overflow-x: auto;
  white-space: pre;
}

.code-block .kw { color: var(--accent); }
.code-block .cm { color: var(--text-muted); }
.code-block .fn { color: var(--logic); }
.code-block .num { color: var(--io); }
.code-block .str { color: var(--compute); }

/* Data tables */
.data-table {
  width: 100%;
  border-collapse: collapse;
  margin: 0.75rem 0;
  font-size: 0.8rem;
}

.data-table th {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  color: var(--text-muted);
  text-align: left;
  padding: 0.5rem 0.75rem;
  border-bottom: 1px solid var(--border);
}

.data-table td {
  padding: 0.5rem 0.75rem;
  color: var(--text-dim);
  border-bottom: 1px solid rgba(30,30,40,0.5);
}

.data-table td:first-child { color: var(--text); }
.data-table tr:last-child td { border-bottom: none; }

/* Metric callouts */
.metrics-row {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
  gap: 0.75rem;
  margin: 0.75rem 0;
}

.metric {
  background: rgba(255,255,255,0.02);
  border: 1px solid var(--border);
  border-radius: 8px;
  padding: 0.75rem 1rem;
  text-align: center;
}

.metric-value {
  font-family: var(--font-display);
  font-size: 1.6rem;
  color: var(--accent);
}

.metric-label {
  font-family: var(--font-mono);
  font-size: 0.55rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  color: var(--text-muted);
  margin-top: 0.25rem;
}

/* ─── TERM DEFINITIONS ─── */
.term {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-style: dotted;
  text-decoration-color: rgba(86,180,233,0.4);
  text-underline-offset: 3px;
  cursor: help;
}

.term-tooltip {
  position: fixed;
  max-width: 340px;
  background: #1a1a22;
  border: 1px solid var(--border-active);
  border-radius: 10px;
  padding: 0.85rem 1rem;
  z-index: 1000;
  box-shadow: 0 8px 32px rgba(0,0,0,0.5);
  animation: tooltipIn 0.2s ease;
}

.term-tooltip-title {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.35rem;
}

.term-tooltip-body {
  font-size: 0.85rem;
  color: var(--text-dim);
  line-height: 1.6;
}

.term-tooltip-close {
  position: absolute;
  top: 0.5rem;
  right: 0.5rem;
  width: 1.25rem;
  height: 1.25rem;
  display: flex;
  align-items: center;
  justify-content: center;
  background: transparent;
  border: 1px solid var(--border);
  border-radius: 4px;
  color: var(--text-muted);
  font-size: 0.65rem;
  cursor: pointer;
  padding: 0;
  line-height: 1;
  transition: color 0.15s, border-color 0.15s;
}
.term-tooltip-close:hover {
  color: var(--text-dim);
  border-color: var(--border-active);
}

.term-tooltip-footer {
  display: flex;
  align-items: center;
  gap: 0.75rem;
  margin-top: 0.5rem;
  padding-top: 0.4rem;
  border-top: 1px solid var(--border);
}

.term-tooltip-nav {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  color: var(--accent-dim);
  opacity: 0.7;
  text-decoration: none;
  letter-spacing: 0.05em;
  transition: opacity 0.2s ease;
  white-space: nowrap;
  cursor: pointer;
}
.term-tooltip-nav:hover { opacity: 1; color: var(--accent); }

@keyframes tooltipIn {
  from { opacity: 0; transform: translateY(4px); }
  to { opacity: 1; transform: translateY(0); }
}

/* ─── FULL JOURNEY DIAGRAM ─── */
.journey-section {
  padding: 4rem 2rem 6rem;
  max-width: 1200px;
  margin: 0 auto;
}

.journey-diagram {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 16px;
  padding: 2rem;
  margin-top: 2rem;
  font-family: var(--font-mono);
  font-size: 0.72rem;
  line-height: 2.2;
  color: var(--text-dim);
  overflow-x: auto;
  white-space: pre;
}

.journey-diagram .phase-label {
  color: var(--accent);
  font-weight: 500;
}

.journey-diagram .component {
  color: var(--text);
}

.journey-diagram .detail {
  color: var(--text-muted);
}

.journey-diagram .arrow-line {
  color: var(--text-muted);
}

.journey-diagram .phase-group {
  color: var(--accent);
  font-weight: 700;
}

/* ─── JOURNEY DETAIL PANEL ─── */
.journey-container {
  display: flex;
  gap: 1.5rem;
  align-items: flex-start;
  margin-top: 2rem;
}

.journey-container .journey-diagram {
  margin-top: 0;
  flex: 1 1 auto;
  min-width: 0;
}

.journey-hint {
  font-family: var(--font-mono);
  font-size: 0.6rem;
  letter-spacing: 0.2em;
  text-transform: uppercase;
  color: var(--text-muted);
  margin-top: 0.75rem;
}

.journey-diagram .phase-label[data-step] {
  cursor: pointer;
  padding: 0 0.25em;
  margin: 0 -0.25em;
  border-radius: 3px;
  transition: background 0.15s ease, color 0.15s ease;
  border-bottom: 1px dotted transparent;
}

.journey-diagram .phase-label[data-step]:hover {
  background: var(--accent-glow);
  border-bottom-color: var(--accent-dim);
}

.journey-diagram .phase-label[data-step]:focus-visible {
  outline: 2px solid var(--accent);
  outline-offset: 2px;
}

.journey-diagram .phase-label.active-label {
  background: var(--accent-glow);
  border-left: 2px solid var(--accent);
  padding-left: calc(0.25em + 2px);
  margin-left: calc(-0.25em - 2px);
}

.journey-detail-panel {
  flex: 1 1 480px;
  max-width: 560px;
  min-width: 340px;
  position: sticky;
  top: 1.5rem;
  max-height: calc(100vh - 3rem);
  overflow-y: auto;
  display: none;
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 16px;
}

.journey-detail-panel.visible {
  display: block;
  animation: panelSlideIn 0.25s ease;
}

@keyframes panelSlideIn {
  from { opacity: 0; transform: translateX(12px); }
  to { opacity: 1; transform: translateX(0); }
}

.journey-detail-header {
  position: sticky;
  top: 0;
  background: var(--bg-card);
  z-index: 2;
  display: flex;
  align-items: center;
  gap: 0.75rem;
  padding: 1.25rem 1.5rem 1rem;
  border-bottom: 1px solid var(--border);
}

.journey-detail-step-number {
  font-family: var(--font-mono);
  font-size: 0.7rem;
  color: var(--accent);
  letter-spacing: 0.05em;
}

.journey-detail-step-name {
  font-family: var(--font-display);
  font-size: 1.2rem;
  color: var(--text);
  flex: 1;
}

.journey-detail-close {
  background: none;
  border: 1px solid var(--border);
  border-radius: 6px;
  color: var(--text-dim);
  font-size: 1.2rem;
  width: 2rem;
  height: 2rem;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: border-color 0.15s ease, color 0.15s ease;
}

.journey-detail-close:hover {
  border-color: var(--border-active);
  color: var(--text);
}

.journey-detail-content {
  padding: 1.5rem;
}

.journey-detail-content .detail-inner {
  display: block !important;
}

.journey-detail-divider {
  border: none;
  border-top: 1px solid var(--border);
  margin: 1.5rem 0;
}

.journey-detail-panel::-webkit-scrollbar { width: 6px; }
.journey-detail-panel::-webkit-scrollbar-track { background: transparent; }
.journey-detail-panel::-webkit-scrollbar-thumb {
  background: var(--border-active);
  border-radius: 3px;
}

/* ─── FOOTER ─── */
.footer {
  padding: 4rem 2rem;
  text-align: center;
  border-top: 1px solid var(--border);
}

.footer p {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  color: var(--text-muted);
  letter-spacing: 0.1em;
}

/* ─── SCROLL ANIMATIONS ─── */
.reveal {
  opacity: 0;
  transform: translateY(20px);
  transition: all 0.6s ease;
}

.reveal.visible {
  opacity: 1;
  transform: translateY(0);
}

/* ─── MINI-MAP ─── */
.minimap {
  position: fixed;
  right: max(1.5rem, calc((100vw - 1200px) / 2 - 10rem));
  top: 50%;
  transform: translateY(-50%);
  z-index: 400;
  opacity: 0;
  pointer-events: none;
  transition: opacity 0.35s ease;
  display: flex;
  flex-direction: column;
  gap: 0;
}
.minimap.visible { opacity: 1; pointer-events: auto; }

.minimap-phase { padding: 0.4rem 0; position: relative; }
.minimap-phase + .minimap-phase { border-top: 1px solid var(--border); margin-top: 0.15rem; padding-top: 0.55rem; }

.minimap-phase-label {
  font-family: var(--font-mono);
  font-size: 0.5rem;
  letter-spacing: 0.18em;
  text-transform: uppercase;
  color: var(--accent-dim);
  margin-bottom: 0.35rem;
  padding-left: 2px;
}

.minimap-item {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  padding: 0.2rem 0;
  text-decoration: none;
  cursor: pointer;
  outline: none;
}

.minimap-dot {
  width: 8px;
  height: 8px;
  border-radius: 50%;
  border: 1.5px solid var(--border);
  background: var(--bg);
  flex-shrink: 0;
  transition: border-color 0.2s ease, background 0.2s ease, box-shadow 0.2s ease;
}

.minimap-label {
  font-family: var(--font-mono);
  font-size: 0.55rem;
  letter-spacing: 0.06em;
  color: var(--text-muted);
  white-space: nowrap;
  transition: color 0.2s ease;
}

.minimap-item:hover .minimap-dot { border-color: var(--border-active); }
.minimap-item:hover .minimap-label { color: var(--text-dim); }

.minimap-item:focus-visible .minimap-dot {
  outline: 2px solid var(--accent);
  outline-offset: 2px;
}

.minimap-item.active .minimap-dot {
  background: var(--accent);
  border-color: var(--accent);
  box-shadow: 0 0 6px var(--accent-glow), 0 0 12px var(--accent-glow);
}
.minimap-item.active .minimap-label { color: var(--text); }

[data-theme="light"] .minimap-dot { background: var(--bg-card); }
[data-theme="light"] .minimap-item.active .minimap-dot {
  box-shadow: 0 0 8px rgba(26,122,181,0.2);
}

.minimap-label { opacity: 0; width: 0; overflow: hidden; transition: opacity 0.2s ease, width 0.2s ease; }
.minimap-phase-label { opacity: 0; width: 0; overflow: hidden; transition: opacity 0.2s ease, width 0.2s ease; }
.minimap:hover { background: var(--bg); box-shadow: -4px 0 16px rgba(0,0,0,0.3); border: 1px solid var(--border); padding: 0.75rem 1rem 0.75rem 0.5rem; border-radius: 8px; }
.minimap:hover .minimap-label { opacity: 1; width: auto; }
.minimap:hover .minimap-phase-label { opacity: 1; width: auto; }
[data-theme="light"] .minimap:hover { box-shadow: -4px 0 16px rgba(0,0,0,0.08); }
@media (max-width: 768px) {
  .minimap { display: none; }
}

/* ─── RESPONSIVE ─── */
@media (max-width: 768px) {
  .site-nav { padding: 0 0.75rem; }
  .nav-logo { font-size: 0.85rem; }
  .nav-link { font-size: 0.5rem; letter-spacing: 0.08em; }
  .nav-links { gap: 0.5rem; }
  .nav-divider { height: 14px; }
  .pipeline-flow::before { left: 18px; }
  .pipeline-step { grid-template-columns: 36px 1fr; }
  .step-card { padding: 1rem; }
  .step-name { font-size: 1.15rem; }
  .sub-topics { grid-template-columns: 1fr; }
  .step-badge { display: none; }
  .metrics-row { grid-template-columns: repeat(2, 1fr); }
  .phase-overview { grid-template-columns: 1fr; }
  .phase-card:not(:last-child)::after {
    content: '\2193';
    right: 50%;
    top: auto;
    bottom: -1rem;
    transform: translate(50%, 50%);
  }
  .journey-container { flex-direction: column; }
  .journey-detail-panel {
    position: relative;
    top: 0;
    max-width: 100%;
    min-width: 0;
    max-height: 60vh;
  }
}

/* ─── LIGHT THEME ─── */
[data-theme="light"] {
  --bg: #f5f3ef;
  --bg-card: #ffffff;
  --bg-card-hover: #f0eee9;
  --border: #d8d5ce;
  --border-active: #b0abb8;
  --text: #1a1a1f;
  --text-dim: #4a4750;
  --text-muted: #8a8690;
  --accent: #1a7ab5;
  --accent-dim: #155e8c;
  --accent-glow: rgba(26,122,181,0.07);
  --compute: #b34a00;
  --memory: #006fa8;
  --io: #9d6e00;
  --logic: #a85d87;
  --network: #1a7ab5;
}

[data-theme="light"] .hero::before {
  background:
    radial-gradient(ellipse 80% 50% at 50% 0%, rgba(26,122,181,0.06) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 20% 80%, rgba(0,111,168,0.04) 0%, transparent 50%),
    radial-gradient(ellipse 60% 40% at 80% 80%, rgba(168,93,135,0.04) 0%, transparent 50%);
}

[data-theme="light"] .step-dot { background: var(--bg-card); }

[data-theme="light"] .pipeline-step:hover .step-dot,
[data-theme="light"] .pipeline-step.active .step-dot {
  box-shadow: 0 0 12px rgba(26,122,181,0.25);
}

[data-theme="light"] .badge-compute { border-color: rgba(179,74,0,0.3); background: rgba(179,74,0,0.06); }
[data-theme="light"] .badge-memory { border-color: rgba(0,111,168,0.3); background: rgba(0,111,168,0.06); }
[data-theme="light"] .badge-io { border-color: rgba(157,110,0,0.3); background: rgba(157,110,0,0.06); }
[data-theme="light"] .badge-logic { border-color: rgba(168,93,135,0.3); background: rgba(168,93,135,0.06); }
[data-theme="light"] .badge-network { border-color: rgba(26,122,181,0.3); background: rgba(26,122,181,0.06); }

[data-theme="light"] .sub-topic { background: rgba(0,0,0,0.02); }
[data-theme="light"] .sub-topic:hover { background: rgba(0,0,0,0.04); }
[data-theme="light"] .metric { background: rgba(0,0,0,0.02); }
[data-theme="light"] .code-block { background: #f0ede8; }

[data-theme="light"] .term-tooltip {
  background: #ffffff;
  box-shadow: 0 8px 32px rgba(0,0,0,0.12);
}
[data-theme="light"] .term-tooltip-close { border-color: rgba(0,0,0,0.15); color: rgba(0,0,0,0.3); }
[data-theme="light"] .term-tooltip-close:hover { border-color: rgba(0,0,0,0.3); color: rgba(0,0,0,0.5); }
[data-theme="light"] .term-tooltip-footer { border-top-color: rgba(0,0,0,0.08); }

[data-theme="light"] .callout-takeaway { background: rgba(26,122,181,0.06); }
[data-theme="light"] .callout-analogy { background: rgba(168,93,135,0.06); }
[data-theme="light"] .detail-text code { background: rgba(26,122,181,0.08); }
[data-theme="light"] .data-table td { border-bottom-color: rgba(0,0,0,0.06); }
[data-theme="light"] .term { text-decoration-color: rgba(26,122,181,0.4); }
[data-theme="light"] ::selection { background: var(--accent); color: #ffffff; }

/* ─── SITE NAV ─── */
.site-nav {
  position: fixed;
  top: 0; left: 0; right: 0;
  z-index: 600;
  height: 48px;
  background: rgba(10,10,12,0.85);
  backdrop-filter: blur(12px);
  -webkit-backdrop-filter: blur(12px);
  border-bottom: 1px solid var(--border);
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 0 2rem;
}

.nav-logo {
  font-family: var(--font-display);
  font-size: 1rem;
  color: var(--text);
  text-decoration: none;
  white-space: nowrap;
}

.nav-links {
  display: flex;
  align-items: center;
  gap: 1.5rem;
}

.nav-link {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  color: var(--text-muted);
  text-decoration: none;
  padding: 0.25rem 0;
  transition: color 0.2s ease;
}

.nav-link:hover { color: var(--text-dim); }

.nav-link.active {
  color: var(--accent);
  border-bottom: 1.5px solid var(--accent);
}

.nav-divider {
  width: 1px;
  height: 18px;
  background: var(--border);
  flex-shrink: 0;
}

[data-theme="light"] .site-nav {
  background: rgba(245,243,239,0.85);
}

/* ─── THEME TOGGLE (in nav) ─── */
.theme-toggle {
  width: 32px;
  height: 32px;
  border-radius: 50%;
  border: 1px solid var(--border);
  background: transparent;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  color: var(--text-dim);
  font-size: 1rem;
  transition: border-color 0.2s ease, color 0.2s ease;
}

.theme-toggle:hover {
  border-color: var(--border-active);
  color: var(--text);
}

.theme-toggle::after { content: '\263C'; }
[data-theme="light"] .theme-toggle::after { content: '\263E'; }

/* ─── TERM SOURCE LINK ─── */
.term-tooltip-source {
  display: inline-block;
  margin-top: 0.4rem;
  font-family: var(--font-mono);
  font-size: 0.65rem;
  color: var(--accent-dim);
  opacity: 0.7;
  text-decoration: none;
  letter-spacing: 0.05em;
  transition: opacity 0.2s ease;
}

.term-tooltip-source:hover { opacity: 1; }

[data-theme="light"] .journey-detail-panel {
  box-shadow: 0 2px 16px rgba(0,0,0,0.08);
}

[data-theme="light"] .journey-diagram .phase-label[data-step]:hover {
  background: rgba(26,122,181,0.07);
}

/* ─── THEME TRANSITION ─── */
body, .code-block, .journey-diagram, .callout-box, .metric, .journey-detail-panel, .journey-detail-header, .minimap-phase, .site-nav {
  transition: background-color 0.3s ease, color 0.3s ease, border-color 0.3s ease;
}

/* ─── STEP VISUALS ─── */
.step-visual {
  position: relative;
  height: 260px;
  background: rgba(255,255,255,0.015);
  border: 1px solid var(--border);
  border-radius: 10px;
  overflow: hidden;
  margin: 0.75rem 0;
  font-family: var(--font-mono);
  font-size: 0.7rem;
}

.sv-node {
  position: absolute;
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: 6px;
  padding: 0.35rem 0.55rem;
  color: var(--text-dim);
  font-size: 0.65rem;
  white-space: nowrap;
  transition: border-color 0.2s, background 0.2s;
}

.sv-node.highlight {
  border-color: var(--accent);
  background: var(--accent-glow);
  color: var(--text);
}

.sv-label {
  font-family: var(--font-mono);
  font-size: 0.5rem;
  letter-spacing: 0.15em;
  text-transform: uppercase;
  color: var(--text-muted);
  position: absolute;
}

.sv-line {
  position: absolute;
  background: var(--border);
}
.sv-line.h { height: 1px; }
.sv-line.v { width: 1px; }
.sv-line.active { background: var(--accent-dim); }

.sv-packet {
  position: absolute;
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background: var(--accent);
  box-shadow: 0 0 8px rgba(86,180,233,0.4);
  z-index: 2;
}

.sv-bar {
  height: 14px;
  background: rgba(255,255,255,0.04);
  border-radius: 3px;
  overflow: hidden;
  position: relative;
}
.sv-bar-fill {
  height: 100%;
  border-radius: 3px;
  transition: width 0.4s ease;
}

.sv-grid {
  display: inline-grid;
  gap: 2px;
}
.sv-cell {
  width: 16px;
  height: 16px;
  border: 1px solid var(--border);
  border-radius: 2px;
  transition: background 0.15s, border-color 0.15s;
}
.sv-cell.lit { background: var(--accent); border-color: var(--accent-dim); }
.sv-cell.waste { background: rgba(213,94,0,0.25); border-color: rgba(213,94,0,0.4); }
.sv-cell.dim { background: rgba(86,180,233,0.08); border-color: rgba(86,180,233,0.15); }

.sv-btn {
  font-family: var(--font-mono);
  font-size: 0.55rem;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  padding: 0.25rem 0.6rem;
  border: 1px solid var(--border);
  border-radius: 4px;
  background: var(--bg-card);
  color: var(--text-dim);
  cursor: pointer;
  transition: border-color 0.2s, color 0.2s;
  outline: none;
}
.sv-btn:hover { border-color: var(--accent-dim); color: var(--accent); }
.sv-btn.active { border-color: var(--accent); color: var(--accent); background: var(--accent-glow); }

.sv-controls {
  position: absolute;
  bottom: 0.6rem;
  left: 50%;
  transform: translateX(-50%);
  display: flex;
  gap: 0.35rem;
  z-index: 3;
}

.sv-stat {
  position: absolute;
  top: 0.5rem;
  right: 0.6rem;
  font-size: 0.55rem;
  color: var(--accent);
  z-index: 3;
}

.sv-token {
  display: inline-block;
  padding: 0.2rem 0.4rem;
  border-radius: 4px;
  font-size: 0.65rem;
  margin: 0.1rem;
  border: 1px solid;
}

@keyframes svPulse { 0%, 100% { opacity: 0.3; } 50% { opacity: 1; } }
@keyframes svAppear { from { opacity: 0; transform: scale(0.85); } to { opacity: 1; transform: scale(1); } }

[data-theme="light"] .step-visual { background: rgba(0,0,0,0.02); }
[data-theme="light"] .sv-node { background: #fff; }
[data-theme="light"] .sv-packet { box-shadow: 0 0 8px rgba(26,122,181,0.3); }
[data-theme="light"] .sv-bar { background: rgba(0,0,0,0.04); }

@media (max-width: 768px) {
  .step-visual { height: 200px; }
}

/* ─── SEARCH ─── */
.search-overlay { position: fixed; inset: 0; background: rgba(0,0,0,0.55); z-index: 2000; display: flex; align-items: flex-start; justify-content: center; padding-top: 12vh; opacity: 0; pointer-events: none; transition: opacity 0.15s ease; }
.search-overlay.open { opacity: 1; pointer-events: auto; }
.search-modal { background: var(--bg-card); border: 1px solid var(--border-active); border-radius: 12px; width: min(560px, calc(100vw - 2rem)); box-shadow: 0 24px 64px rgba(0,0,0,0.5); transform: translateY(-8px); transition: transform 0.15s ease; }
.search-overlay.open .search-modal { transform: translateY(0); }
.search-input-wrap { display: flex; align-items: center; gap: 0.75rem; padding: 0.875rem 1rem; border-bottom: 1px solid var(--border); }
.search-icon-svg { width: 16px; height: 16px; color: var(--text-muted); flex-shrink: 0; fill: none; }
.search-input { flex: 1; background: none; border: none; outline: none; font-family: var(--font-body); font-size: 0.95rem; color: var(--text); }
.search-input::placeholder { color: var(--text-muted); }
.search-esc-hint { font-family: var(--font-mono); font-size: 0.6rem; color: var(--text-muted); background: var(--bg); border: 1px solid var(--border); border-radius: 4px; padding: 0.15rem 0.4rem; flex-shrink: 0; }
.search-results { max-height: 380px; overflow-y: auto; padding: 0.4rem 0; contain: content; }
.search-group-header { font-family: var(--font-mono); font-size: 0.5rem; letter-spacing: 0.18em; text-transform: uppercase; color: var(--text-muted); padding: 0.6rem 1rem 0.2rem; }
.search-result { display: flex; align-items: center; gap: 0.75rem; padding: 0.55rem 1rem; cursor: pointer; transition: background 0.1s ease; }
.search-result:hover, .search-result.selected { background: var(--bg-card-hover); }
.search-result-dot { width: 6px; height: 6px; border-radius: 50%; border: 1.5px solid var(--border-active); background: transparent; flex-shrink: 0; transition: background 0.1s ease, border-color 0.1s ease; }
.search-result.selected .search-result-dot { background: var(--accent); border-color: var(--accent); }
.search-result-body { flex: 1; min-width: 0; }
.search-result-title { font-size: 0.85rem; color: var(--text-dim); }
.search-result.selected .search-result-title { color: var(--text); }
.search-result-desc { font-size: 0.7rem; color: var(--text-muted); margin-top: 0.05rem; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
.search-result-badge { font-family: var(--font-mono); font-size: 0.45rem; letter-spacing: 0.1em; text-transform: uppercase; color: var(--accent-dim); border: 1px solid rgba(86,180,233,0.25); border-radius: 3px; padding: 0.1rem 0.3rem; flex-shrink: 0; }
.search-no-results { padding: 2.5rem 1rem; text-align: center; color: var(--text-muted); font-size: 0.85rem; }
.search-footer { display: flex; gap: 1.5rem; padding: 0.5rem 1rem; border-top: 1px solid var(--border); justify-content: flex-end; align-items: center; }
.search-footer-hint { font-family: var(--font-mono); font-size: 0.5rem; color: var(--text-muted); display: flex; align-items: center; gap: 0.3rem; }
.search-footer-key { background: var(--bg); border: 1px solid var(--border); border-radius: 3px; padding: 0.1rem 0.3rem; }
.search-trigger { background: none; border: 1px solid var(--border); border-radius: 6px; display: flex; align-items: center; gap: 0.4rem; padding: 0.25rem 0.6rem; cursor: pointer; color: var(--text-muted); font-family: var(--font-mono); font-size: 0.55rem; letter-spacing: 0.05em; transition: border-color 0.2s ease, color 0.2s ease; }
.search-trigger:hover { border-color: var(--border-active); color: var(--text-dim); }
.search-trigger svg { width: 12px; height: 12px; fill: none; }
.search-trigger .search-trigger-label { white-space: nowrap; }
.search-trigger kbd { font-family: var(--font-mono); font-size: 0.5rem; color: var(--text-muted); background: var(--bg); border: 1px solid var(--border); border-radius: 3px; padding: 0.05rem 0.3rem; }
.knowledge-trigger[disabled] { opacity: 0.65; cursor: wait; }
[data-theme="light"] .search-overlay { background: rgba(0,0,0,0.25); }
[data-theme="light"] .search-modal { box-shadow: 0 24px 64px rgba(0,0,0,0.15); }
@media (max-width: 768px) {
  .search-trigger .search-trigger-label, .search-trigger kbd { display: none; }
  .search-trigger { padding: 0.35rem; }
}
</style>
<script>
(function(){
  var s = localStorage.getItem('theme');
  if (s) { document.documentElement.setAttribute('data-theme', s); }
  else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: light)').matches) {
    document.documentElement.setAttribute('data-theme', 'light');
  } else { document.documentElement.setAttribute('data-theme', 'dark'); }
})();
</script>
</head>
<body>

<nav class="site-nav">
  <a href="index.html" class="nav-logo">LLM Anatomy</a>
  <div class="nav-links">
    <a href="training.html" class="nav-link">Training</a>
    <a href="training-economics.html" class="nav-link">Training Economics</a>
    <span class="nav-divider"></span>
    <a href="index.html" class="nav-link active">Inference</a>
    <a href="economics.html" class="nav-link">Inference Economics</a>
    <button class="search-trigger" id="search-trigger" aria-label="Search all pages">
      <svg viewBox="0 0 16 16" stroke="currentColor" stroke-width="1.5"><circle cx="6.5" cy="6.5" r="4.5"></circle><path stroke-linecap="round" d="m10 10 3.5 3.5"></path></svg>
      <span class="search-trigger-label">Search</span>
      <kbd id="search-key-hint">&#8984;K</kbd>
    </button>
    <button class="search-trigger knowledge-trigger" id="download-knowledge" aria-label="Download website knowledge as Markdown">
      <svg viewBox="0 0 16 16" stroke="currentColor" stroke-width="1.5" fill="none"><path stroke-linecap="round" stroke-linejoin="round" d="M8 2.5v7"></path><path stroke-linecap="round" stroke-linejoin="round" d="m5.5 7.5 2.5 2.5 2.5-2.5"></path><path stroke-linecap="round" stroke-linejoin="round" d="M3 12.5h10"></path></svg>
      <span class="search-trigger-label">Knowledge</span>
      <kbd>.md</kbd>
    </button>
    <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme"></button>
  </div>
</nav>

<nav class="minimap" id="minimap" aria-label="Pipeline navigation">
  <div class="minimap-phase">
    <div class="minimap-phase-label">A</div>
    <a class="minimap-item" href="#" data-target="routing"><span class="minimap-dot"></span><span class="minimap-label">Routing</span></a>
    <a class="minimap-item" href="#" data-target="preprocessing"><span class="minimap-dot"></span><span class="minimap-label">Preprocessing</span></a>
    <a class="minimap-item" href="#" data-target="tokenization"><span class="minimap-dot"></span><span class="minimap-label">Tokenization</span></a>
    <a class="minimap-item" href="#" data-target="embedding"><span class="minimap-dot"></span><span class="minimap-label">Embedding</span></a>
  </div>
  <div class="minimap-phase">
    <div class="minimap-phase-label">B</div>
    <a class="minimap-item" href="#" data-target="scheduling"><span class="minimap-dot"></span><span class="minimap-label">Scheduling</span></a>
    <a class="minimap-item" href="#" data-target="prefill"><span class="minimap-dot"></span><span class="minimap-label">Prefill</span></a>
    <a class="minimap-item" href="#" data-target="kvcache"><span class="minimap-dot"></span><span class="minimap-label">KV Cache</span></a>
    <a class="minimap-item" href="#" data-target="attention"><span class="minimap-dot"></span><span class="minimap-label">Attention</span></a>
    <a class="minimap-item" href="#" data-target="decode"><span class="minimap-dot"></span><span class="minimap-label">Decode</span></a>
  </div>
  <div class="minimap-phase">
    <div class="minimap-phase-label">C</div>
    <a class="minimap-item" href="#" data-target="sampling"><span class="minimap-dot"></span><span class="minimap-label">Sampling</span></a>
    <a class="minimap-item" href="#" data-target="streaming"><span class="minimap-dot"></span><span class="minimap-label">Streaming</span></a>
  </div>
  <div class="minimap-phase">
    <div class="minimap-phase-label">&bull;</div>
    <a class="minimap-item" href="#" data-target="inference-metrics"><span class="minimap-dot"></span><span class="minimap-label">Metrics</span></a>
  </div>
</nav>

<!-- ═══════════ HERO ═══════════ -->
<section class="hero">
  <div class="hero-label">Interactive Technical Reference</div>
  <h1>Anatomy of <em>LLM Inference</em></h1>
  <p class="hero-sub">Every step from user request to streamed response. Click any stage to drill in.</p>
  <div class="hero-cta">Scroll to explore <span class="arrow">&#8595;</span></div>
</section>

<!-- ═══════════ PHASE OVERVIEW ═══════════ -->
<section class="phase-overview reveal">
  <div class="phase-card" onclick="document.getElementById('phase-a').scrollIntoView({behavior:'smooth'})">
    <div class="phase-letter">A</div>
    <div class="phase-name">Request Preparation</div>
    <div class="phase-subtitle">Getting the input ready for the GPU</div>
    <div class="phase-steps">
      <span>01 Request Routing</span>
      <span>02 Preprocessing</span>
      <span>03 Tokenization</span>
      <span>04 Embedding &amp; Position</span>
    </div>
  </div>
  <div class="phase-card" onclick="document.getElementById('phase-b').scrollIntoView({behavior:'smooth'})">
    <div class="phase-letter">B</div>
    <div class="phase-name">GPU Computation</div>
    <div class="phase-subtitle">The core forward pass</div>
    <div class="phase-steps">
      <span>05 Scheduling &amp; Batching</span>
      <span>06 Prefill Phase</span>
      <span>07 KV Cache</span>
      <span>08 Attention Mechanisms</span>
      <span>09 Decode Phase</span>
    </div>
  </div>
  <div class="phase-card" onclick="document.getElementById('phase-c').scrollIntoView({behavior:'smooth'})">
    <div class="phase-letter">C</div>
    <div class="phase-name">Output &amp; Delivery</div>
    <div class="phase-subtitle">Turning logits into a response</div>
    <div class="phase-steps">
      <span>10 Sampling &amp; Selection</span>
      <span>11 Detokenization &amp; Streaming</span>
    </div>
  </div>
</section>

<!-- ═══════════ PIPELINE ═══════════ -->
<section class="pipeline-section" id="pipeline">
  <div class="section-label reveal">The Request Lifecycle</div>
  <h2 class="section-title reveal">Pipeline Stages</h2>
  <p class="section-desc reveal">An LLM inference request passes through 11 distinct stages grouped into three phases. Each solves a different problem, has different bottlenecks, and uses different optimisation strategies. Click any stage to explore.</p>

  <div class="pipeline-flow" id="pipeline-flow">

    <!-- ═══ PHASE A ═══ -->
    <div class="phase-divider" id="phase-a">
      <div class="phase-divider-marker">A</div>
      <div class="phase-divider-content">
        <div class="phase-divider-name">Request Preparation</div>
        <div class="phase-divider-desc">Getting the input ready for the GPU</div>
      </div>
    </div>

    <!-- ─── 01 ROUTING ─── -->
    <div class="pipeline-step reveal" data-step="routing" id="step-routing">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">01</span>
          <span class="step-name">Request Routing</span>
          <span class="step-badge badge-network">Network</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">API gateway receives the request, LLM-aware load balancer routes to the optimal GPU worker based on <span class="term" data-term="kv-cache">KV cache</span> state, queue depth, and model/<span class="term" data-term="lora">LoRA</span> affinity.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>LLM routing must be GPU-state-aware &mdash; traditional load balancers fail because they can't see KV cache utilization, queue depth, or model placement on each worker.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>A restaurant host seating a returning customer at the same table where their appetizers are already waiting, instead of assigning a random empty seat.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="routing"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Why it matters</div>
              <div class="detail-text">
                Unlike typical web requests, LLM inference is <strong>long-running and stateful</strong> (due to KV cache). Standard HTTP load balancers fail because they lack awareness of GPU state. Modern LLM-aware routers consider <strong>KV cache utilization</strong>, <strong>queue length</strong>, and <strong>LoRA adapter</strong> presence on each worker.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">KV-Cache Aware Routing</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Route to GPUs that already hold relevant context</div>
                  <div class="sub-topic-detail">
                    <p>If a user sends a follow-up message in a conversation, the KV cache for previous turns may still reside on a specific GPU. <strong>KV-cache aware routing</strong> directs the request back to that GPU, skipping prefill for the cached prefix entirely. This can eliminate <strong>95% of <span class="term" data-term="ttft">TTFT</span></strong>.</p>
                    <p>Frameworks like <strong>llm-d</strong> (Kubernetes-native) and <strong>vLLM Router</strong> (Rust-based) implement this by polling each worker's cache state and using hash-based or prefix-matching lookups.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Prefill-Decode Disaggregation</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Separate GPU pools for prefill vs decode</div>
                  <div class="sub-topic-detail">
                    <p>The <strong>defining architecture of 2025</strong>. <span class="term" data-term="prefill">Prefill</span> is compute-bound (parallel matrix ops); <span class="term" data-term="decode">decode</span> is memory-bound (sequential KV reads). Running both on the same GPU causes interference &mdash; a long prefill blocks decode iterations, spiking latency for in-progress generations.</p>
                    <p>The solution: <strong>dedicated prefill GPUs</strong> compute the initial KV cache, then transfer it (via <span class="term" data-term="rdma">RDMA</span>/<span class="term" data-term="nvlink">NVLink</span>) to <strong>dedicated decode GPUs</strong>. Each tier scales independently. Meta, Mistral, and Hugging Face run this in production. Gains: <strong>2&ndash;7x throughput</strong>.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Gateway Frameworks (2025)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">vLLM Router, K8s Gateway API, NVIDIA Dynamo</div>
                  <div class="sub-topic-detail">
                    <p><strong>vLLM Router</strong>: Rust-based, lightweight load balancer engineered for vLLM. State-aware, understands prefill/decode disaggregation patterns.</p>
                    <p><strong>Kubernetes Gateway API Inference Extension</strong>: Model-aware routing at the K8s ingress level, supporting per-request criticalities and GPU-specific metrics.</p>
                    <p><strong>NVIDIA Dynamo</strong>: Next-gen distributed inference framework with built-in disaggregation, dynamic GPU scheduling, and LLM-aware request routing. Up to <strong>30x</strong> more requests served (DeepSeek-R1 on Blackwell).</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Rate Limiting as Pricing Lever</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Not just protection &mdash; a business strategy</div>
                  <div class="sub-topic-detail">
                    <p>Per-customer rate limits (requests/sec, tokens/min) map directly to <strong>pricing tiers</strong>. Queue vs. reject during spikes is a business decision: queuing adds latency but preserves revenue, rejecting loses it. Rate limits are <strong>pricing levers</strong>, not just technical constraints.</p>
                    <p style="margin-top:0.5rem"><a href="economics.html#pricing" style="color:var(--accent-dim);font-family:var(--font-mono);font-size:0.75rem;text-decoration:none;letter-spacing:0.05em">How pricing structures work &rarr;</a></p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Multi-LoRA Hot-Swapping</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Base model stays loaded, adapters swap per-request</div>
                  <div class="sub-topic-detail">
                    <p>The base model stays resident on GPU while <span class="term" data-term="lora">LoRA</span> adapters are swapped per-request. This enables <strong>hundreds of fine-tuned variants</strong> from a single GPU deployment. Router efficiency &mdash; how well requests are matched to loaded adapters &mdash; is the <strong>most important unit economics driver</strong>.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Geo-Aware Routing</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Route to nearest region with available capacity</div>
                  <div class="sub-topic-detail">
                    <p>Multi-region deployments route requests to the <strong>nearest data center with available GPU capacity</strong>. This reduces network round-trip latency (50-200ms savings for cross-continent requests) and enables failover across regions. Providers like Baseten, Anyscale, and major cloud LLM APIs use Anycast DNS or edge routing to direct traffic to the optimal region based on both proximity and current load.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="cold-start">Cold Start</span> Times</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">30-120 seconds for large models</div>
                  <div class="sub-topic-detail">
                    <p>When a model isn't already loaded on a GPU, loading weights from disk takes <strong>30&ndash;120 seconds</strong> for large models. Hot models are load-balanced across replicas; cold models either incur this latency or require pre-warmed standby GPUs (costly idle capacity).</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 02 PREPROCESSING ─── -->
    <div class="pipeline-step reveal" data-step="preprocessing" id="step-preprocessing">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">02</span>
          <span class="step-name">Preprocessing</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Input validation, prompt template assembly, RAG retrieval, rate limiting. CPU-bound, scales linearly with input length.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Prompt construction order matters &mdash; static content first, dynamic content last &mdash; to maximize <span class="term" data-term="prefix-caching">prefix cache</span> hit rates in downstream GPU stages.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>A chef's mise en place &mdash; washing, chopping, and measuring all ingredients before the stove turns on. No GPU time is used here.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="preprocessing"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">What happens here</div>
              <div class="detail-text">
                Before any GPU work begins, the API server assembles the final prompt on CPU. This includes applying <strong>prompt templates</strong> (system instructions, chat formatting), performing <strong>RAG retrieval</strong> (if applicable), validating input constraints (max tokens, stop sequences), and <strong>rate limiting</strong>. The request metadata (ID, sampling params, timestamp) is prepared for the scheduler.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Key considerations</div>
              <div class="detail-text">
                This stage is entirely <strong>CPU-bound</strong> and scales with input length. For RAG-heavy workloads, embedding generation and vector search can dominate preprocessing time. Smart prompt construction &mdash; placing <strong>static content first</strong> and <strong>dynamic content last</strong> &mdash; is critical for maximising prefix cache hit rates downstream.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Content Moderation Before GPU</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Filter harmful requests before spending compute</div>
                  <div class="sub-topic-detail">
                    <p>Optional content moderation runs <strong>before any GPU spend</strong>. A lightweight safety classifier screens requests for harmful content, preventing expensive GPU cycles on requests that would be filtered anyway. This is especially important at scale where abusive traffic can waste significant compute budget.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 03 TOKENIZATION ─── -->
    <div class="pipeline-step reveal" data-step="tokenization" id="step-tokenization">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">03</span>
          <span class="step-name">Tokenization</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Raw text is converted into integer token IDs via <span class="term" data-term="bpe">BPE</span> or SentencePiece. Vocabulary sizes have grown 8x in 3 years (32K &rarr; 262K).</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Larger vocabularies produce fewer tokens per input, directly reducing cost and latency &mdash; at the trade-off of a bigger embedding table.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>Learning common phrases in a foreign language: 'good morning' becomes one unit instead of eleven letters, so your conversations get shorter and faster.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="tokenization"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">How BPE works</div>
              <div class="detail-text">
                <strong>Byte Pair Encoding</strong> starts with individual bytes (256 base tokens) and iteratively merges the most frequent adjacent pair into a new token. This repeats until the desired vocabulary size is reached. Modern LLMs use <strong>byte-level BPE</strong>, meaning any input &mdash; regardless of language or special characters &mdash; can be tokenized with zero unknown tokens.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Vocabulary Size Trends</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">From 32K to 262K in 3 years</div>
                  <div class="sub-topic-detail">
                    <table class="data-table">
                      <thead><tr><th>Model</th><th>Year</th><th>Vocab Size</th></tr></thead>
                      <tbody>
                        <tr><td>Llama 2</td><td>2023</td><td>32,000</td></tr>
                        <tr><td>Llama 3</td><td>2024</td><td>128,256</td></tr>
                        <tr><td>Mistral Nemo</td><td>2025</td><td>~131,000</td></tr>
                        <tr><td>Gemini 3</td><td>2025</td><td>262,000</td></tr>
                      </tbody>
                    </table>
                    <p>Larger vocabularies mean fewer tokens per input (lower cost, faster inference) but larger embedding matrices. There is a <strong>log-linear relationship</strong> between vocabulary size and training loss.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">SentencePiece</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Language-agnostic tokenization without pre-tokenization</div>
                  <div class="sub-topic-detail">
                    <p>SentencePiece treats input as a <strong>raw character stream</strong> with no pre-tokenization, encoding spaces as the metasymbol <code>&#x2581;</code>. It supports both BPE and Unigram algorithms and handles any language without language-specific preprocessing.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">2025 Innovations</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">SuperBPE, BoundlessBPE, LiteToken</div>
                  <div class="sub-topic-detail">
                    <p><strong>SuperBPE</strong> (COLM 2025): Two-pass BPE that learns cross-word "superword" tokens. Produces <strong>33% fewer tokens</strong> and improves performance by 4.0% across 30 benchmarks.</p>
                    <p><strong>BoundlessBPE</strong>: Relaxes word boundary constraints, achieving up to <strong>15% improvement</strong> in bytes-per-token.</p>
                    <p><strong>LiteToken</strong> (Feb 2026): Identifies and removes "intermediate merge residues" &mdash; tokens frequent during BPE training but rarely used in final output.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Fast Tokenizers</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">tiktoken is 3-6x faster than alternatives</div>
                  <div class="sub-topic-detail">
                    <p><strong>tiktoken</strong> (OpenAI, Rust core) is the fastest tokenizer at <strong>3&ndash;6x faster</strong> than alternatives. It is inference-only (no training support) and powers OpenAI models, Llama 3+, and Mistral's Tekken tokenizer.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Token Economics</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">~4 chars/token EN, CJK languages pay 2x more</div>
                  <div class="sub-topic-detail">
                    <p>Tokens are the <strong>fundamental unit of cost and latency</strong>. Average characters per token vary dramatically by content type:</p>
                    <div class="metrics-row">
                      <div class="metric"><div class="metric-value">~4</div><div class="metric-label">Chars/Token (English)</div></div>
                      <div class="metric"><div class="metric-value">~3</div><div class="metric-label">Chars/Token (Code)</div></div>
                      <div class="metric"><div class="metric-value">~1.5</div><div class="metric-label">Chars/Token (CJK)</div></div>
                    </div>
                    <p>Japanese users pay <strong>~2x more per word</strong> due to tokenizer inefficiency. Different models use different tokenizers, so the same text produces different token counts. Most providers charge input and output tokens separately because output tokens (sequential decode) cost more to serve.</p>
                  </div>
                </div>
                <a href="training.html#tokenizer" class="cross-link">&rarr; How tokenizers are trained</a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 04 EMBEDDING ─── -->
    <div class="pipeline-step reveal" data-step="embedding" id="step-embedding">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">04</span>
          <span class="step-name">Embedding &amp; Positional Encoding</span>
          <span class="step-badge badge-memory">Memory</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Token IDs are mapped to dense vectors via a lookup table, then combined with positional information (<span class="term" data-term="rope">RoPE</span>) so the model knows token order.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Embeddings give tokens meaning; positional encoding gives them order. Without position, 'dog bites man' and 'man bites dog' look identical to the model.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>Giving each word a GPS coordinate in meaning-space, then stamping it with a sequence number so the model knows what came first.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="embedding"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Token embedding</div>
              <div class="detail-text">
                A <strong>lookup table</strong> of shape <code>[vocab_size, d_model]</code> maps each token ID to a learned vector. For Llama 2 7B: 32,000 &times; 4,096 = <strong>~131M parameters</strong>. This is a simple table index, not a matrix multiplication. The vectors encode semantic meaning learned during training.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">RoPE (Rotary Position Embedding)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">The dominant positional encoding in 2025</div>
                  <div class="sub-topic-detail">
                    <p>RoPE encodes position by <strong>rotating query and key vectors</strong> in 2D subspaces using sinusoidal functions. It is parameter-free, inherently captures relative positions, and scales gracefully to long contexts.</p>
                    <p>Extensions like <strong>YaRN</strong> and <strong>NTK-aware scaling</strong> allow context lengths far beyond training length. Used by LLaMA, Mistral, GPT-NeoX, and most open-weight models.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="alibi">ALiBi</span></span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Attention with Linear Biases</div>
                  <div class="sub-topic-detail">
                    <p>Instead of modifying embeddings, ALiBi adds a <strong>linear bias directly to attention scores</strong> based on token distance. It shows better extrapolation beyond the training window and trains faster than RoPE. Used in MPT and some specialized models.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Per-Layer Embeddings (PLE)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Google Gemma 3N innovation for mobile</div>
                  <div class="sub-topic-detail">
                    <p>Google's <strong>Gemma 3N</strong> introduced PLE for mobile inference. Rather than one large initial embedding, PLE generates <strong>smaller, layer-specific embeddings</strong> cached to slower storage (flash memory) and loaded as each layer runs. This dramatically reduces active memory footprint for on-device models.</p>
                  </div>
                </div>
                <a href="training.html#architecture" class="cross-link">&rarr; How model architectures are designed</a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ═══ PHASE B ═══ -->
    <div class="phase-divider" id="phase-b">
      <div class="phase-divider-marker">B</div>
      <div class="phase-divider-content">
        <div class="phase-divider-name">GPU Computation</div>
        <div class="phase-divider-desc">The core forward pass</div>
      </div>
    </div>

    <!-- ─── 05 SCHEDULING ─── -->
    <div class="pipeline-step reveal" data-step="scheduling" id="step-scheduling">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">05</span>
          <span class="step-name">Scheduling &amp; Batching</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">The scheduler decides which requests enter the next GPU iteration. <span class="term" data-term="continuous-batching">Continuous batching</span> replaces completed sequences instantly, achieving 80-95% GPU utilisation.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Continuous batching is the single biggest throughput optimization &mdash; it keeps GPUs busy by replacing finished sequences with new ones every iteration.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>A city bus where passengers board and exit at different stops &mdash; when someone gets off, that seat opens immediately for a new rider waiting at the next stop.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="scheduling"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Why batching is critical</div>
              <div class="detail-text">
                With no batching, each request processes alone &mdash; GPU compute utilisation sits at just <strong>2&ndash;5%</strong> because the GPU spends most time loading weights for a single sequence. Batching amortises this cost: reading weights once and applying them to many concurrent requests. At batch size 32, utilisation reaches <strong>60&ndash;80%</strong> &mdash; same weights, dramatically more work per byte loaded.
                <br><a href="economics.html#throughput" style="color:var(--accent-dim);font-family:var(--font-mono);font-size:0.75rem;text-decoration:none;letter-spacing:0.05em">Utilization drives unit economics &rarr;</a>
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Static Batching</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Fixed groups, wait for slowest request</div>
                  <div class="sub-topic-detail">
                    <p>Requests are grouped into fixed-size batches. The entire batch waits until the <strong>slowest request finishes</strong>. If one request generates 10 tokens and another generates 500, the short request idles for the long one. Short sequences are padded to match the longest, wasting compute on empty tokens. GPU utilisation: <strong>30-60%</strong>.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="dynamic-batching">Dynamic Batching</span></span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Time or size trigger, still waits for slowest</div>
                  <div class="sub-topic-detail">
                    <p>A middle ground between static and continuous. A batch starts when either a <strong>max batch size</strong> is reached <em>or</em> a <strong>timeout expires</strong> &mdash; whichever comes first. This reduces queuing latency compared to static batching (no need to wait for a full batch), but the batch still runs as a unit: all requests finish together, so shorter sequences sit idle while the longest completes.</p>
                    <div class="metrics-row">
                      <div class="metric"><div class="metric-value">40-70%</div><div class="metric-label">GPU Utilisation</div></div>
                      <div class="metric"><div class="metric-value">~1.5x</div><div class="metric-label">vs Static Throughput</div></div>
                    </div>
                    <p>Most appropriate for interactive workloads like image generation where requests arrive unevenly and latency matters more than maximum throughput.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Continuous Batching</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Replace finished sequences every iteration</div>
                  <div class="sub-topic-detail">
                    <p>The breakthrough technique. Each sequence finishes independently and is <strong>immediately replaced</strong> with a new request at every decode iteration. The batch composition changes dynamically.</p>
                    <div class="metrics-row">
                      <div class="metric"><div class="metric-value">80-95%</div><div class="metric-label">GPU Utilisation</div></div>
                      <div class="metric"><div class="metric-value">2-8x</div><div class="metric-label">Throughput Gain</div></div>
                    </div>
                    <p>All major frameworks support it: vLLM, SGLang, TensorRT-LLM ("in-flight batching"), LMDeploy, TGI.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="chunked-prefill">Chunked Prefill</span></span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Interleave prefill with decode steps</div>
                  <div class="sub-topic-detail">
                    <p>Long prompts are split into chunks processed iteratively, <strong>interleaved with decode steps</strong>. This prevents a single large prefill from blocking all in-progress decode iterations (head-of-line blocking). Critical for maintaining low <span class="term" data-term="tpot">TPOT</span> under mixed workloads.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="interleaved-thinking">Interleaved Thinking</span> Scheduling</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Reasoning, tool calls, and answer tokens share one decode loop</div>
                  <div class="sub-topic-detail">
                    <p>Reasoning models now run mixed workloads inside one turn: internal reasoning tokens, tool calls, and answer generation can alternate. Schedulers must preserve per-request state through these micro-phases instead of treating each request as a single uninterrupted decode stream.</p>
                    <p><strong>Operational implication:</strong> if long reasoning segments monopolize decode slots, tail latency spikes for everyone else. Continuous batching plus chunked prefill keeps non-reasoning and reasoning traffic coexisting without catastrophic head-of-line blocking.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 06 PREFILL ─── -->
    <div class="pipeline-step reveal" data-step="prefill" id="step-prefill">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">06</span>
          <span class="step-name">Prefill Phase</span>
          <span class="step-badge badge-compute">Compute</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">All input tokens are processed through every Transformer layer in parallel. Computes and stores the KV cache. Determines Time to First Token (TTFT).</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Prefill processes all input tokens in parallel and is compute-bound. Its speed directly determines Time to First Token (<span class="term" data-term="ttft">TTFT</span>).</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>Reading an entire exam question before writing your answer &mdash; you must process everything first, but at least you can read all words simultaneously.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="prefill"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">TTFT benchmarks</div>
              <div class="metrics-row">
                <div class="metric"><div class="metric-value">50-200ms</div><div class="metric-label">8B Model, Short Prompt</div></div>
                <div class="metric"><div class="metric-value">200-800ms</div><div class="metric-label">70B Model, Short Prompt</div></div>
                <div class="metric"><div class="metric-value">2-10s</div><div class="metric-label">70B, 100K+ Context</div></div>
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">How it works</div>
              <div class="detail-text">
                All input token embeddings (with positional encoding) are fed through the Transformer layers simultaneously. At each attention layer, the model computes <strong>Query (Q), Key (K), and Value (V)</strong> matrices for every token. Self-attention is computed: <code>Attention(Q,K,V) = <span class="term" data-term="softmax">softmax</span>(QK&#x1D40;/&radic;d&#x2096;)V</code>. The resulting <strong>K and V tensors are stored</strong> in the KV cache.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Characteristics</div>
              <div class="metrics-row">
                <div class="metric"><div class="metric-value">Compute</div><div class="metric-label">Bottleneck Type</div></div>
                <div class="metric"><div class="metric-value">O(n&sup2;)</div><div class="metric-label">Attention Complexity</div></div>
                <div class="metric"><div class="metric-value">TTFT</div><div class="metric-label">Latency Metric</div></div>
              </div>
              <div class="detail-text">
                Prefill is <strong>compute-bound</strong> and <strong>highly parallelisable</strong>. All tokens are known upfront, so matrix multiplications are fully batched &mdash; ideal for GPU utilisation. For a 10K-token prompt, prefill on a single GPU can take seconds. This time directly determines the <strong>Time to First Token</strong>.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="prefix-caching">Prefix Cache</span> Hit</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Skip prefill entirely for cached prefixes</div>
                  <div class="sub-topic-detail">
                    <p>When the KV cache for a prompt prefix already exists, prefill goes from <strong>O(n&sup2;) GPU compute to O(n) storage I/O</strong>, eliminating 95% of TTFT. vLLM's Automatic Prefix Caching achieves <strong>87%+ cache hit rates</strong> with well-structured prompts and <strong>88% faster TTFT</strong> for warm cache hits.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">KV-Runahead</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Apple's parallel layer prefill (2025)</div>
                  <div class="sub-topic-detail">
                    <p>Apple's <strong>KV-Runahead</strong> generates KV caches for later layers in parallel while earlier layers are still processing, overlapping computation and reducing total prefill time.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="disaggregated-serving">Disaggregated</span> Prefill/Decode</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Different hardware for each phase</div>
                  <div class="sub-topic-detail">
                    <p>Run prefill on <strong>compute-optimised hardware</strong> and decode on <strong>memory-bandwidth-optimised hardware</strong>. Prefill GPUs handle the heavy parallel matrix multiplications, then transfer the KV cache to decode GPUs via <span class="term" data-term="rdma">RDMA</span>. Each tier scales independently, improving both TTFT and throughput for high-volume, latency-sensitive workloads.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 07 KV CACHE ─── -->
    <div class="pipeline-step reveal" data-step="kvcache" id="step-kvcache">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">07</span>
          <span class="step-name">KV Cache &amp; Memory Management</span>
          <span class="step-badge badge-memory">Memory</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Stores Key and Value tensors from all layers to avoid O(n&sup2;) recomputation. Often consumes more GPU memory than the model weights. <span class="term" data-term="pagedattention">PagedAttention</span> reduces waste from 60-80% to under 4%.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>The KV cache trades memory for speed &mdash; it avoids recomputing attention for previous tokens, but often consumes more GPU memory than the model weights themselves.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>Keeping sticky notes of every previous conversation turn so you don't re-read the entire chat history &mdash; efficient, but your desk fills up fast.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="kvcache"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Memory formula</div>
              <div class="code-block"><span class="cm">// Per token</span>
<span class="fn">KV_per_token</span> = <span class="num">2</span> &times; num_layers &times; num_kv_heads &times; head_dim &times; bytes_per_param

<span class="cm">// Example: Llama 2 7B (<span class="term" data-term="fp16">FP16</span>)</span>
<span class="cm">// 2 &times; 32 &times; 32 &times; 128 &times; 2 = 524 KB per token</span>
<span class="cm">// 4096 ctx &times; 524KB = ~2 GB per sequence</span>
<span class="cm">// Batch of 32 = ~64 GB &mdash; can exceed a single GPU!</span></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">PagedAttention</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">OS-style virtual memory for KV cache</div>
                  <div class="sub-topic-detail">
                    <p>Borrows <strong>virtual memory and paging</strong> from operating systems. GPU memory is divided into fixed-size physical blocks (e.g., 16 tokens each). Each sequence's KV cache maps to logical blocks that point to scattered physical blocks via a <strong>block table</strong> (like a page table).</p>
                    <p>Blocks are allocated <strong>on demand</strong> as tokens are generated, not pre-allocated for max length. Multiple requests sharing a prefix can <strong>point to the same physical block</strong> (copy-on-write).</p>
                    <div class="metrics-row">
                      <div class="metric"><div class="metric-value">&lt;4%</div><div class="metric-label">Memory Waste</div></div>
                      <div class="metric"><div class="metric-value">2-4x</div><div class="metric-label">More Concurrent Reqs</div></div>
                    </div>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">KV Cache Compression</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Eviction, quantization, merging</div>
                  <div class="sub-topic-detail">
                    <p><strong>Token-level</strong>: Evict unimportant tokens, dynamically allocate memory budget, merge similar KV pairs, quantise cached values to INT8/<span class="term" data-term="int4">INT4</span>.</p>
                    <p><strong>Offloading</strong>: Move KV cache to CPU DRAM or disk when GPU memory is full, with intelligent prefetching.</p>
                    <p><strong>Multi-tier storage</strong>: LMCache supports GPU DRAM &rarr; CPU DRAM &rarr; local disk &rarr; remote storage hierarchy.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="mla">Multi-Head Latent Attention</span></span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">DeepSeek's 93% KV cache reduction</div>
                  <div class="sub-topic-detail">
                    <p>DeepSeek V2/V3's <strong>MLA</strong> compresses K and V into a low-dimensional latent vector. Only the compressed latent is stored in the KV cache; at inference, it is projected back to full K/V space. Result: <strong>93.3% KV cache reduction</strong> vs MHA, slightly outperforming in quality.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Cache Persistence Between Requests</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Stateless default vs. prefix caching</div>
                  <div class="sub-topic-detail">
                    <p><strong>Naive stateless</strong> (default): KV cache discarded after each request. Simple but wasteful &mdash; every turn re-prefills the entire conversation history.</p>
                    <p><strong>Prefix caching</strong> (optimised): Server keeps recent KV caches in GPU memory. When the next turn arrives with an identical prefix, only new tokens are prefilled. Classic cache eviction problem &mdash; <strong>LRU or TTL-based</strong> eviction policies decide what stays.</p>
                    <p><strong>Routing complexity</strong>: KV cache lives on specific GPUs. The next request must route to the <strong>same GPU(s)</strong>, creating session affinity and potential hot spots. If the user edits a previous message, cached KV is stale and must be invalidated.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 08 ATTENTION ─── -->
    <div class="pipeline-step reveal" data-step="attention" id="step-attention">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">08</span>
          <span class="step-name">Attention Mechanisms</span>
          <span class="step-badge badge-compute">Compute</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">The core computation. <span class="term" data-term="gqa">GQA</span> is the 2025 standard. <span class="term" data-term="flashattention">FlashAttention</span> makes it IO-efficient by tiling through GPU <span class="term" data-term="sram">SRAM</span> (19 TB/s) instead of <span class="term" data-term="hbm">HBM</span> (2 TB/s).</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>GQA is the 2025 standard: near-full-quality attention with a fraction of the memory cost. FlashAttention makes any variant faster via hardware-aware tiling.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>A study group sharing notes &mdash; instead of everyone writing independent copies (<span class="term" data-term="mha">MHA</span>), small groups share one set (GQA). Less paper, nearly the same understanding.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="attention"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">MHA &rarr; GQA &rarr; <span class="term" data-term="mqa">MQA</span></span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">The evolution of attention head sharing</div>
                  <div class="sub-topic-detail">
                    <p><strong>MHA</strong> (Multi-Head Attention): Original Transformer. Each head has independent Q, K, V. Maximum expressivity, highest KV cache cost.</p>
                    <p><strong>MQA</strong> (Multi-Query Attention): All query heads share <strong>one</strong> K/V head. KV cache reduced by num_heads&times; (e.g., 32x). Lower quality.</p>
                    <p><strong>GQA</strong> (Grouped-Query Attention): <strong>The 2025 standard</strong>. Query heads grouped, each group shares one K/V head. Example: 32 Q heads with 8 KV heads = 4x KV cache reduction. Optimal quality-efficiency balance. Used in LLaMA 3, Mistral, most open models.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">FlashAttention</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">IO-aware tiled attention: up to 7.6x faster</div>
                  <div class="sub-topic-detail">
                    <p>Not a different attention variant &mdash; an <strong>IO-aware implementation</strong> that makes any attention pattern faster. Core insight: standard attention is bottlenecked by memory bandwidth, not compute.</p>
                    <p><strong>Solution</strong>: Tile Q, K, V into blocks. Load blocks from HBM to SRAM (fast, small on-chip memory). Compute attention entirely in SRAM. Write only the final output back &mdash; never materialising the full N&times;N attention matrix.</p>
                    <div class="metrics-row">
                      <div class="metric"><div class="metric-value">19 TB/s</div><div class="metric-label">SRAM Bandwidth</div></div>
                      <div class="metric"><div class="metric-value">2 TB/s</div><div class="metric-label">HBM Bandwidth</div></div>
                      <div class="metric"><div class="metric-value">7.6x</div><div class="metric-label">Speedup (GPT-2)</div></div>
                    </div>
                    <p><strong>FlashAttention-3</strong> (2025): Adds async loading (overlap data transfer with compute), <span class="term" data-term="fp8">FP8</span> support, and Hopper-specific optimisations.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Multi-Head Latent Attention</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">DeepSeek's compressed attention</div>
                  <div class="sub-topic-detail">
                    <p>MLA compresses K and V into a <strong>low-dimensional latent vector</strong> before caching. At inference, the latent is projected back. In "absorb mode": <strong>71x less memory per layer</strong> (98.6% reduction). Slightly outperforms MHA in quality.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 09 DECODE ─── -->
    <div class="pipeline-step reveal" data-step="decode" id="step-decode">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">09</span>
          <span class="step-name">Decode Phase</span>
          <span class="step-badge badge-memory">Memory</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Tokens generated one at a time, <span class="term" data-term="autoregressive">autoregressively</span>. Each step reads the entire KV cache but does little math. Memory-bandwidth-bound, inherently sequential.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Decode generates one token at a time and is memory-bandwidth-bound &mdash; the GPU spends most of its time waiting for data reads, not computing.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>Writing a story one word at a time, where for each word you must re-read all your previous notes &mdash; the bottleneck isn't thinking, it's flipping through pages.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="decode"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">The autoregressive loop</div>
              <div class="detail-text">
                For each new token: the single embedding is fed through all layers. Only the <strong>Query for the new token</strong> is computed fresh. The Key and Value are computed and <strong>appended to the KV cache</strong>. Attention is computed between the new Q and all cached K/V pairs. Output <span class="term" data-term="logits">logits</span> represent probability over the vocabulary.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Why it's slow</div>
              <div class="detail-text">
                Each decode step involves <strong>reading the entire KV cache</strong> from GPU HBM but performs little arithmetic. The arithmetic intensity is very low &mdash; the GPU is mostly waiting for memory reads. This is why decode is <strong>memory-bandwidth-bound</strong>, not compute-bound. The key metric is <strong>TPOT</strong> (Time Per Output Token).
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Reasoning Modes in 2026 APIs</div>
              <div class="detail-text">
                <p>Reasoning-capable APIs can emit a separate reasoning channel (often exposed as <span class="term" data-term="reasoning-content">reasoning_content</span>) while generating user-visible output. Providers expose knobs like <span class="term" data-term="reasoning-effort">reasoning effort</span> and <span class="term" data-term="thinking-budget">thinking budget</span> to trade off quality, latency, and cost.</p>
                <p><strong><span class="term" data-term="interleaved-thinking">Interleaved thinking</span></strong> keeps reasoning and tool execution within one turn. <strong><span class="term" data-term="preserved-thinking">Preserved thinking</span></strong> carries reasoning context across turns via controls like <span class="term" data-term="reasoning-history">reasoning history</span> (or equivalent clear-thinking flags).</p>
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="speculative-decoding">Speculative Decoding</span></span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Draft model + verification: 2-3x speedup, lossless</div>
                  <div class="sub-topic-detail">
                    <p>A small "draft model" (e.g., 1B params) generates K candidate tokens quickly. The large target model (e.g., 70B) <strong>verifies all K tokens in a single forward pass</strong> (prefill-like parallelism). Tokens are accepted left-to-right; the first rejected token is resampled.</p>
                    <p>The output distribution is <strong>mathematically identical</strong> to running the target model alone &mdash; this is lossless acceleration.</p>
                    <div class="metrics-row">
                      <div class="metric"><div class="metric-value">2-3x</div><div class="metric-label">Typical Speedup</div></div>
                      <div class="metric"><div class="metric-value">0%</div><div class="metric-label">Quality Loss</div></div>
                    </div>
                    <p><strong>2025 advances</strong>: Block verification (5-8% additional speedup), Online Speculative Decoding (adapts draft to query distribution), Self-Speculative Decoding (uses early-exit layers, no separate model), Medusa (parallel draft heads).</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="arithmetic-intensity">Arithmetic Intensity</span> Problem</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">GPU has ~990 TFLOPS but can barely use them</div>
                  <div class="sub-topic-detail">
                    <p>Arithmetic intensity = compute operations &divide; bytes loaded. During decode, this ratio is <strong>abysmal</strong>: the GPU loads the entire weight matrix for a single matrix-vector multiply. For 70B at FP16, that's loading ~140 GB from <span class="term" data-term="hbm">HBM</span> at 3.35 TB/s &rarr; ~42ms minimum per token &rarr; ~24 tokens/sec. The GPU's ~990 TFLOPS sit mostly idle, starved for data.</p>
                    <p><strong>Batching fixes this</strong>: loading weights once and processing 32 requests turns the vector ops into efficient matrix ops. Same bandwidth cost, 32x more useful compute.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">GPU Memory Hierarchy</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Disk &rarr; RAM &rarr; HBM &rarr; SRAM &rarr; Compute</div>
                  <div class="sub-topic-detail">
                    <p>For every output token, the GPU processes layer by layer:</p>
                    <table class="data-table">
                      <thead><tr><th>Level</th><th>Size</th><th>Speed</th><th>Role</th></tr></thead>
                      <tbody>
                        <tr><td>NVMe Disk</td><td>TBs</td><td>~7 GB/s</td><td>Weight storage (cold start)</td></tr>
                        <tr><td>CPU RAM</td><td>100s GB</td><td>~64 GB/s</td><td>Transit to GPU</td></tr>
                        <tr><td>HBM</td><td>80-192 GB</td><td>3,350 GB/s</td><td>Weights + KV cache (hot)</td></tr>
                        <tr><td>SRAM</td><td>~50 MB</td><td>~19 TB/s</td><td>On-chip compute cache</td></tr>
                      </tbody>
                    </table>
                    <p>SRAM is 5.7x faster than HBM but holds only <strong>~0.036%</strong> of a 70B model. Each layer's weights must be tiled through SRAM in ~34 chunks, then evicted for the next layer. <strong>Prefetching</strong> overlaps loading Layer N+1 while computing Layer N.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ═══ PHASE C ═══ -->
    <div class="phase-divider" id="phase-c">
      <div class="phase-divider-marker">C</div>
      <div class="phase-divider-content">
        <div class="phase-divider-name">Output &amp; Delivery</div>
        <div class="phase-divider-desc">Turning logits into a response</div>
      </div>
    </div>

    <!-- ─── 10 SAMPLING ─── -->
    <div class="pipeline-step reveal" data-step="sampling" id="step-sampling">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">10</span>
          <span class="step-name">Sampling &amp; Token Selection</span>
          <span class="step-badge badge-logic">Logic</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Logits are transformed via temperature, truncated (<span class="term" data-term="top-k">top-k</span>, <span class="term" data-term="top-p">top-p</span>, <span class="term" data-term="min-p">min-p</span>), penalised for repetition, then sampled. Order matters.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>The order of sampling operations matters: penalties &rarr; temperature &rarr; truncation &rarr; softmax &rarr; sample. Min-P is the recommended truncation method for 2025.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>Choosing a restaurant: first eliminate closed ones (truncation), adjust for how adventurous you feel (temperature), then pick from what's left.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="sampling"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">The sampling pipeline</div>
              <div class="code-block"><span class="fn">Raw Logits</span>
  &rarr; <span class="kw">Repetition/Penalty Adjustments</span>
  &rarr; <span class="kw">Temperature Scaling</span>
  &rarr; <span class="kw">Truncation</span> (top-k / top-p / min-p)
  &rarr; <span class="fn">Softmax</span>
  &rarr; <span class="str">Random Sample</span></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Temperature</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Scales logits before softmax</div>
                  <div class="sub-topic-detail">
                    <p>Temperature scales logits: <code>p_i = exp(z_i / T) / &Sigma;exp(z_j / T)</code></p>
                    <p><strong>T = 1.0</strong>: Default. <strong>T &lt; 1.0</strong>: Sharper, more deterministic. <strong>T &gt; 1.0</strong>: Flatter, more creative. <strong>T &rarr; 0</strong>: Greedy decoding (always pick highest probability).</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Top-K, Top-P, Min-P</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Truncation strategies compared</div>
                  <div class="sub-topic-detail">
                    <p><strong>Top-K</strong>: Keep K highest-probability tokens. Problem: fixed K is context-inappropriate.</p>
                    <p><strong>Top-P</strong> (Nucleus): Keep smallest set whose cumulative probability exceeds P. Dynamically adaptive, but coupled to temperature.</p>
                    <p><strong>Min-P</strong> (ICLR 2025): Filter tokens below <code>min_p &times; max_probability</code>. Consistently outperforms Top-P, especially at higher temperatures. <strong>The recommended truncation method for 2025.</strong></p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Repetition Penalties</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Frequency, presence, and LZ penalties</div>
                  <div class="sub-topic-detail">
                    <p><strong>Repetition penalty</strong>: Multiplicative penalty on logits of recently-seen tokens.</p>
                    <p><strong>Frequency penalty</strong>: Additive penalty proportional to token occurrence count.</p>
                    <p><strong>Presence penalty</strong>: Flat additive penalty on any token that has appeared at all (binary).</p>
                    <p><strong>LZ penalty</strong> (2025): Information-theoretic penalty based on Lempel-Ziv complexity, detecting repeated n-gram patterns.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Top-n-Sigma (ACL 2025)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Temperature-decoupled truncation in logit space</div>
                  <div class="sub-topic-detail">
                    <p>The newest method. Addresses <strong>temperature coupling</strong> &mdash; probability-based truncation produces identical token sets regardless of temperature. Top-n-sigma operates in <strong>logit space</strong>, keeping tokens within n standard deviations of the mean logit. Fully decoupled from temperature.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name"><span class="term" data-term="fsm-decoding">Constrained Decoding</span> (FSM)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Force valid JSON/schema output via grammar tracking</div>
                  <div class="sub-topic-detail">
                    <p>Uses <strong>finite state machines</strong> to mask invalid tokens at each decode step. The FSM tracks position in the output grammar (e.g., "just opened a JSON key string &mdash; only valid characters or closing quote allowed") and zeros out tokens that would produce invalid output.</p>
                    <p>Actually uses <strong>pushdown automata</strong> (FSMs with a stack) to handle nested structures like JSON braces/brackets. Performance: <strong>O(1) per token</strong> &mdash; microseconds of overhead. After filtering, remaining tokens are <strong>renormalized</strong> to sum to 1, subtly concentrating probability mass.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- ─── 11 DETOKENIZATION & STREAMING ─── -->
    <div class="pipeline-step reveal" data-step="streaming" id="step-streaming">
      <div class="step-marker"><div class="step-dot"></div></div>
      <div class="step-card">
        <div class="step-header">
          <span class="step-number">11</span>
          <span class="step-name">Detokenization &amp; Streaming</span>
          <span class="step-badge badge-io">IO</span>
          <span class="step-expand-icon">+</span>
        </div>
        <div class="step-summary">Token IDs are converted back to text and streamed to the client via <span class="term" data-term="sse">Server-Sent Events (SSE)</span>. Includes content filtering and format enforcement.</div>
        <div class="step-detail">
          <div class="detail-inner">
            <div class="callout-box callout-takeaway">
              <div class="callout-label">Key Takeaway</div>
              <p>Tokens can't always be decoded independently &mdash; partial characters must be buffered until a valid text boundary is reached before sending to the client.</p>
            </div>
            <div class="callout-box callout-analogy">
              <div class="callout-label">Think of it like...</div>
              <p>A simultaneous translator who sometimes needs to hear the next few syllables before they can translate the current word &mdash; they buffer until meaning is clear.</p>
            </div>
            <div class="detail-section">
              <div class="detail-label">Interactive Visual</div>
              <div class="step-visual" data-visual="streaming"></div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Detokenization subtleties</div>
              <div class="detail-text">
                During streaming, tokens cannot be detokenized independently. Some tokens represent <strong>partial UTF-8 characters</strong> or subwords that only form valid text when combined. The detokenizer must <strong>buffer tokens</strong> until a valid text boundary is reached. Special tokens (<code>&lt;|endoftext|&gt;</code>, tool-call markers) must be stripped.
              </div>
            </div>
            <div class="detail-section">
              <div class="detail-label">Drill into specifics</div>
              <div class="sub-topics">
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Server-Sent Events (SSE)</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">The dominant streaming protocol</div>
                  <div class="sub-topic-detail">
                    <p>SSE is the standard for streaming LLM responses. Each token is packaged as an SSE message and flushed immediately:</p>
                    <div class="code-block"><span class="kw">data:</span> <span class="str">{"choices":[{"delta":{"content":" Hello"}}]}</span>
<span class="kw">data:</span> <span class="str">{"choices":[{"delta":{"content":" world"}}]}</span>
<span class="kw">data:</span> <span class="str">[DONE]</span></div>
                    <p><strong>Why SSE over WebSockets?</strong> Simpler (HTTP-based, unidirectional), built-in reconnection, works with standard infrastructure. "90% of the benefit with 10% of the headache."</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Postprocessing Pipeline</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Safety, formatting, stop conditions</div>
                  <div class="sub-topic-detail">
                    <p>After detokenization, responses pass through: <strong>stop condition checking</strong> (stop sequences, max tokens, <span class="term" data-term="eos">EOS</span>), <strong>content filtering</strong> (toxicity, safety classifiers), <strong>response scoring</strong> (reward models), <strong>format enforcement</strong> (JSON schema validation), and <strong>citation linking</strong>.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Multi-Turn Statefulness</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Each turn re-sends full conversation history</div>
                  <div class="sub-topic-detail">
                    <p>Each turn is a <strong>completely separate HTTP request</strong>. The SSE connection closes after <code>[DONE]</code>. The server retains <strong>no state between turns</strong>. The client assembles the full conversation history and sends the entire thing as the prompt in each new request. Every turn gets more expensive &mdash; the entire history is re-processed as input tokens (unless <span class="term" data-term="prefix-caching">prefix caching</span> is available).</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">LB Idle Timeouts</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Load balancers can kill long generations</div>
                  <div class="sub-topic-detail">
                    <p>SSE connections stay open <strong>10&ndash;30 seconds</strong> for long responses. Load balancers and proxies often have <strong>idle timeouts</strong> that kill connections before generation completes. This manifests as <strong>mysterious truncated responses</strong> &mdash; a common production issue that's hard to diagnose without proper timeout configuration.</p>
                  </div>
                </div>
                <div class="sub-topic" onclick="toggleSubTopic(this)">
                  <div class="sub-topic-header">
                    <span class="sub-topic-name">Usage Tracking</span>
                    <span class="sub-topic-icon">+</span>
                  </div>
                  <div class="sub-topic-preview">Token counts, latency, GPU time, billing</div>
                  <div class="sub-topic-detail">
                    <p>After generation, the system records: <strong>token counts</strong> (input, output, cached), <strong>latency telemetry</strong> (TTFT, TPOT, total), <strong>GPU time consumed</strong>, and <strong>model/parameters used</strong>. Revenue recognition differs for per-token vs. per-GPU-hour billing models.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

  </div><!-- /pipeline-flow -->
</section>

<!-- ═══════════ INFERENCE METRICS ═══════════ -->
<section class="pipeline-section" id="inference-metrics">
  <div class="section-label reveal">What End Users Measure</div>
  <h2 class="section-title reveal">Inference Metrics</h2>
  <p class="section-desc reveal">The numbers that matter when evaluating LLM inference quality, cost, and reliability &mdash; from latency SLOs to GPU utilization.</p>

  <div class="sub-topics reveal">
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Latency Metrics</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">TTFT, TPOT, ITL, E2E latency, queue time</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Metric</th><th>What It Measures</th><th>Typical Target</th></tr></thead>
          <tbody>
            <tr><td><span class="term" data-term="ttft">TTFT</span></td><td>Time from request to first token. Dominated by prefill compute + queue wait time.</td><td>&lt;200ms (8B), &lt;800ms (70B)</td></tr>
            <tr><td><span class="term" data-term="tpot">TPOT</span></td><td>Average time between consecutive output tokens during decode.</td><td>&lt;30ms (streaming feel)</td></tr>
            <tr><td><strong>Reasoning Time Share</strong></td><td>Fraction of decode wall time spent on internal reasoning tokens before/alongside visible output. Controlled by reasoning-effort and budget settings.</td><td>Use policy caps per tier</td></tr>
            <tr><td><strong>ITL</strong></td><td>Inter-Token Latency &mdash; actual time between each pair of consecutive tokens. Unlike TPOT (average), ITL captures variance and jitter from batching interference.</td><td>p99 &lt;50ms</td></tr>
            <tr><td><strong>E2E Latency</strong></td><td>Total wall-clock time from request send to final token received. E2E = TTFT + (output_tokens &times; TPOT) + network overhead.</td><td>Varies by use case</td></tr>
            <tr><td><strong>Queue Time</strong></td><td>Time spent waiting before prefill begins. Spikes when GPU capacity is saturated. Two queues: prefill queue and generation queue.</td><td>&lt;50ms at p99</td></tr>
          </tbody>
        </table>
        <div class="callout-box callout-takeaway" style="margin-top:0.75rem">
          <div class="callout-label">Key Insight</div>
          <p>TTFT and TPOT measure different bottlenecks: TTFT is <strong>compute-bound</strong> (prefill), TPOT is <strong>memory-bandwidth-bound</strong> (decode). Optimizing one often trades off against the other &mdash; batching more requests improves throughput but increases individual TTFT.</p>
        </div>
      </div>
    </div>

    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Throughput Metrics</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Tokens/sec, requests/sec, concurrent requests</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Metric</th><th>What It Measures</th><th>Why It Matters</th></tr></thead>
          <tbody>
            <tr><td><strong>Tokens/sec (Output)</strong></td><td>Total output tokens generated per second across all concurrent requests on a deployment.</td><td>The primary capacity metric. Directly determines revenue per GPU-hour at per-token pricing.</td></tr>
            <tr><td><strong>Visible vs Reasoning Tokens</strong></td><td>Split between user-visible tokens and internal reasoning tokens. Both consume decode capacity even when only one is shown.</td><td>Required for reasoning workload capacity planning.</td></tr>
            <tr><td><strong>Tokens/sec (Prompt)</strong></td><td>Input tokens processed per second. Higher during prefill bursts. Includes cached prompt tokens processed from prefix cache hits.</td><td>Shows prefill throughput and prefix cache effectiveness.</td></tr>
            <tr><td><strong>Requests/sec</strong></td><td>Completed inference requests per second. A function of concurrent batch size and average generation length.</td><td>Key for capacity planning and autoscaling triggers.</td></tr>
            <tr><td><strong>Concurrent Requests</strong></td><td>Number of in-flight requests being processed simultaneously. Bounded by KV cache memory.</td><td>Directly correlates with GPU utilization and revenue.</td></tr>
          </tbody>
        </table>
        <p style="font-size:0.85rem;color:var(--text-dim);margin-top:0.5rem"><strong>Throughput vs latency tradeoff</strong>: increasing batch size (more concurrent requests) improves tokens/sec but increases per-request TTFT and ITL. The art of inference optimization is finding the batch size that maximizes throughput while meeting latency SLOs.</p>
      </div>
    </div>

    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Resource Utilization</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">GPU utilization, KV cache, memory, model forward time</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Metric</th><th>What It Measures</th><th>Healthy Range</th></tr></thead>
          <tbody>
            <tr><td><strong>GPU Compute Utilization</strong></td><td>Fraction of GPU FLOPS actually used. Low during decode (memory-bound), high during prefill (compute-bound).</td><td>40-70% sustained</td></tr>
            <tr><td><strong>KV Cache Utilization</strong></td><td>Fraction of allocated KV cache blocks/slots in use. When 100% is reached, new requests must wait (queue) or be rejected.</td><td>70-90% (headroom for bursts)</td></tr>
            <tr><td><strong>HBM Usage</strong></td><td>Total GPU memory consumed: model weights + KV cache + activations + CUDA overhead.</td><td>&lt;90% of HBM capacity</td></tr>
            <tr><td><strong>Model Forward Time</strong></td><td>Duration of the actual model forward pass (excluding scheduling and communication overhead).</td><td>Varies by model size</td></tr>
            <tr><td><strong>Prefix Cache TTL</strong></td><td>How long cached prompt KV states remain available for reuse. Longer TTL = more cache hits = lower TTFT for repeated prefixes.</td><td>Deployment-dependent</td></tr>
          </tbody>
        </table>
        <p style="font-size:0.85rem;color:var(--text-dim);margin-top:0.5rem">KV cache utilization is the most actionable metric: it determines how many concurrent requests you can serve. When KV cache fills up, you&rsquo;re either dropping requests or queuing them &mdash; both hurt revenue or SLOs.</p>
      </div>
    </div>

    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Reliability &amp; SLOs</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Error rates, percentiles, availability, SLO frameworks</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Metric</th><th>What It Measures</th><th>Typical SLO</th></tr></thead>
          <tbody>
            <tr><td><strong>Error Rate</strong></td><td>Fraction of requests returning HTTP 4xx/5xx. Broken down by error type: rate-limited (429), server error (500), timeout (504), context too long (413).</td><td>&lt;0.1% for 5xx</td></tr>
            <tr><td><strong>p50/p95/p99 Latency</strong></td><td>Latency at the 50th, 95th, and 99th percentile. p99 captures tail latency that averages hide &mdash; one slow request in 100 still frustrates users.</td><td>p99 TTFT &lt;2x median</td></tr>
            <tr><td><strong>Availability</strong></td><td>Percentage of time the endpoint returns successful responses. Downtime includes both outages and periods of &gt;100% error rate.</td><td>99.9% (8.7h/yr downtime)</td></tr>
            <tr><td><strong>Goodput</strong></td><td>Successful tokens generated per second &mdash; throughput minus wasted work (errors, preempted requests, speculative decoding rejections).</td><td>Track ratio to raw throughput</td></tr>
          </tbody>
        </table>
        <div class="callout-box callout-takeaway" style="margin-top:0.75rem">
          <div class="callout-label">SLO Framework</div>
          <p>Production inference SLOs typically specify: (1) <strong>TTFT p99 &lt; X ms</strong> for responsiveness, (2) <strong>TPOT p99 &lt; Y ms</strong> for streaming smoothness, (3) <strong>Error rate &lt; Z%</strong> for reliability, and (4) <strong>Availability &gt; 99.9%</strong>. These four metrics together define the quality of service contract. NVIDIA Dynamo&rsquo;s &ldquo;SLO-driven&rdquo; scheduling automatically adjusts batching to meet these targets.</p>
        </div>
      </div>
    </div>

    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Cost &amp; Efficiency Metrics</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Cost per token, GPU-hours per request, cache hit rate</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Metric</th><th>What It Measures</th><th>Why It Matters</th></tr></thead>
          <tbody>
            <tr><td><strong>Cost per 1M Tokens</strong></td><td>Total infrastructure cost divided by tokens served. The fundamental unit economics metric.</td><td>Determines gross margin at per-token pricing.</td></tr>
            <tr><td><strong>Reasoning Tokens / Response</strong></td><td>Average hidden reasoning token volume per completed request. For reasoning models, this can dominate decode cost if uncapped.</td><td>Core guardrail for reasoning tiers.</td></tr>
            <tr><td><strong>GPU-hours per Request</strong></td><td>Compute time consumed per inference request. Varies 100x+ depending on prompt length and output tokens.</td><td>Base for per-request cost accounting.</td></tr>
            <tr><td><strong>Prefix Cache Hit Rate</strong></td><td>Fraction of prompt tokens served from KV cache vs. recomputed from scratch. Higher = lower TTFT and lower compute cost.</td><td>System prompts and multi-turn conversations benefit most.</td></tr>
            <tr><td><strong>Tokens per GPU-hour</strong></td><td>Total output tokens generated per hour of GPU time. The supply-side efficiency metric.</td><td>Determines how many customers one GPU can serve.</td></tr>
          </tbody>
        </table>
        <a href="economics.html#throughput" class="cross-link">&rarr; How throughput optimization creates 3-5x cost variance</a>
      </div>
    </div>

    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Observability in Practice</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Prometheus, Grafana, alerting, and provider dashboards</div>
      <div class="sub-topic-detail">
        <p>Production inference deployments export metrics via <strong>Prometheus</strong> endpoints and visualize with <strong>Grafana</strong> dashboards. Key metric categories from providers like Fireworks AI:</p>
        <p><strong>Rate metrics</strong> (counters, per-second): request rate, error rate by HTTP status, prompt tokens/sec, cached tokens/sec.</p>
        <p><strong>Latency histograms</strong> (buckets): TTFT distribution, TPOT distribution, queue time distribution, overall E2E latency. Histograms enable computing arbitrary percentiles (p50, p95, p99) without pre-aggregation.</p>
        <p><strong>Resource gauges</strong> (point-in-time): KV cache block utilization, KV cache slot utilization, concurrent request count, model forward time, prefix cache TTL.</p>
        <p><strong>Token distributions</strong>: generated tokens per request and prompt tokens per request &mdash; essential for understanding workload mix and capacity planning.</p>
        <p><strong>Reasoning channel telemetry</strong>: count and rate of reasoning chunks (for APIs that stream <code>reasoning_content</code>), plus visible/reasoning token splits. Without this split, reasoning tiers look profitable until decode saturation appears.</p>
        <p style="margin-top:0.5rem"><strong>Alerting best practice</strong>: alert on <em>SLO burn rate</em> (error budget consumption speed) rather than raw thresholds. A brief latency spike that stays within your monthly error budget doesn&rsquo;t need to wake someone up at 3 AM.</p>
      </div>
    </div>
  </div>
</section>

<!-- ═══════════ CROSS-CUTTING OPTIMIZATIONS ═══════════ -->
<section class="pipeline-section" id="cross-cutting">
  <div class="section-label reveal">Applied Across All Phases</div>
  <h2 class="section-title reveal">Cross-Cutting Optimizations</h2>
  <p class="section-desc reveal">These techniques aren't sequential pipeline steps &mdash; they apply across the entire forward pass to reduce memory footprint and distribute computation across GPUs.</p>
  <div class="sub-topics reveal">
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Quantization Methods</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview"><span class="term" data-term="gptq">GPTQ</span>, <span class="term" data-term="awq">AWQ</span>, <span class="term" data-term="gguf">GGUF</span>, FP8</div>
      <div class="sub-topic-detail">
        <p>LLM inference is <strong>memory-bandwidth bound</strong>. Smaller weights = less data to transfer = faster inference.</p>
        <table class="data-table">
          <thead><tr><th>Method</th><th>Bits</th><th>Best For</th><th>Quality</th></tr></thead>
          <tbody>
            <tr><td>FP8 (E4M3)</td><td>8</td><td>Hopper GPUs</td><td>~99%</td></tr>
            <tr><td>AWQ</td><td>4</td><td>GPU inference</td><td>~95%</td></tr>
            <tr><td>GPTQ</td><td>4</td><td>GPU inference</td><td>~90%</td></tr>
            <tr><td>GGUF</td><td>2-8</td><td>CPU / edge</td><td>~92%</td></tr>
          </tbody>
        </table>
        <p><strong>AWQ</strong> key insight: not all weights are equally important. It identifies salient weights by analysing <strong>activation magnitudes</strong> and skips them during quantization. Consistently outperforms GPTQ.</p>
        <p><strong>FP8</strong> on Hopper GPUs is nearly lossless: 2x performance, 2x memory reduction vs FP16.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name"><span class="term" data-term="tp">Tensor Parallelism</span> (TP)</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Slice individual layers across GPUs</div>
      <div class="sub-topic-detail">
        <p>Weight matrices are split column-wise or row-wise across GPUs. Each GPU holds a fraction of every layer and computes its slice in parallel. Requires <strong>AllReduce after every layer</strong> &mdash; needs high-bandwidth interconnect (NVLink). Best <strong>within a single node</strong>.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name"><span class="term" data-term="pp">Pipeline Parallelism</span> (PP)</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Divide layers into sequential stages</div>
      <div class="sub-topic-detail">
        <p>A 32-layer model with PP=4 assigns layers 0-7 to GPU 0, 8-15 to GPU 1, etc. Communication is only between adjacent stages (point-to-point), much less frequent than TP. <strong>Pipeline bubbles</strong> (idle stages) are mitigated with micro-batching. Best for <strong>scaling across nodes</strong>.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name"><span class="term" data-term="ep">Expert Parallelism</span> (EP)</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">For <span class="term" data-term="moe">MoE</span> models: route experts to GPUs</div>
      <div class="sub-topic-detail">
        <p>Specialised for <strong>Mixture of Experts</strong> models (DeepSeek-V3: 256 experts, Mixtral: 8). Different experts placed on different GPUs. Requires <strong>All-to-All communication</strong> to route tokens to correct expert GPU. Since only a fraction of experts activate per token (e.g., 2/64), each GPU does less work while the total model can be massive.</p>
        <p style="margin-top:0.5rem"><a href="training.html#distributed" class="cross-link">&rarr; How parallelism works during training (4D parallelism, ZeRO, FSDP)</a></p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">GPU Memory Hierarchy</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Full path: Disk &rarr; CPU RAM &rarr; HBM &rarr; SRAM &rarr; Compute</div>
      <div class="sub-topic-detail">
        <p>Model weights travel through a 4-level hierarchy on every token generation. The critical bottleneck is HBM &rarr; SRAM &mdash; even <span class="term" data-term="hbm">HBM</span>'s 3,350 GB/s can't keep pace with GPU compute capacity.</p>
        <table class="data-table">
          <thead><tr><th>Level</th><th>Capacity</th><th>Bandwidth</th></tr></thead>
          <tbody>
            <tr><td>NVMe SSD</td><td>TBs</td><td>~7 GB/s</td></tr>
            <tr><td>CPU RAM (PCIe 5.0)</td><td>100s GB</td><td>~64 GB/s</td></tr>
            <tr><td>HBM3 (H100)</td><td>80 GB</td><td>3,350 GB/s</td></tr>
            <tr><td>SRAM (on-chip)</td><td>~50 MB</td><td>~19 TB/s</td></tr>
          </tbody>
        </table>
        <p><span class="term" data-term="sram">SRAM</span> holds only <strong>0.036%</strong> of a 70B model. Each layer's weights (~1.7 GB) are tiled through SRAM in ~34 chunks, then evicted for the next layer. Cannot keep weights on-chip between tokens.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Why Smaller Models Are Faster</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Compounding penalties from 8B to 405B</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Factor</th><th>8B</th><th>405B</th><th>Penalty</th></tr></thead>
          <tbody>
            <tr><td>Weight data/token</td><td>~16 GB</td><td>~810 GB</td><td>~50x</td></tr>
            <tr><td>GPUs required</td><td>1</td><td>8-16</td><td>8-16x cost</td></tr>
            <tr><td>Communication</td><td>Zero</td><td>126 all-reduce ops</td><td>Pure penalty</td></tr>
            <tr><td>Batch size</td><td>64+</td><td>8-16</td><td>4-8x less throughput</td></tr>
            <tr><td>Cost per M tokens</td><td>$0.03-0.05</td><td>$0.50-1.00</td><td>10-20x</td></tr>
          </tbody>
        </table>
        <p>These factors <strong>compound</strong>: 405B requires ~50x more weight data, split across 8-16 GPUs with 126 synchronisation points per token, and can only batch 8-16 requests vs 64+.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">Performance Optimisation Stack</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">6 layers: naive 300 &rarr; optimised 5,000 tok/s</div>
      <div class="sub-topic-detail">
        <table class="data-table">
          <thead><tr><th>Layer</th><th>Multiplier</th><th>Cumulative</th></tr></thead>
          <tbody>
            <tr><td>Continuous Batching</td><td>~2.5x</td><td>~750 tok/s</td></tr>
            <tr><td>PagedAttention</td><td>~1.7x</td><td>~1,275 tok/s</td></tr>
            <tr><td>FlashAttention</td><td>~1.3x</td><td>~1,658 tok/s</td></tr>
            <tr><td>Quantization (FP8)</td><td>~1.8x</td><td>~2,984 tok/s</td></tr>
            <tr><td>Custom CUDA Kernels</td><td>~1.3x</td><td>~3,879 tok/s</td></tr>
            <tr><td>Speculative Decoding</td><td>~1.3x</td><td>~5,043 tok/s</td></tr>
          </tbody>
        </table>
        <p><strong>15-17x total improvement</strong> on identical hardware. This is why the same model can have <strong>10x+ price variance</strong> across providers. Custom CUDA kernels are the hardest to replicate &mdash; years of accumulated GPU programming expertise form the core moat of inference platforms.</p>
        <p style="margin-top:0.5rem"><a href="economics.html#throughput" style="color:var(--accent-dim);font-family:var(--font-mono);font-size:0.75rem;text-decoration:none;letter-spacing:0.05em">15-17&times; optimization = 10&times; price variance &rarr;</a></p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">LoRA Adapter Multiplexing</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Serve many fine-tuned models on one base deployment</div>
      <div class="sub-topic-detail">
        <p>Fine-tuning economics now depends heavily on inference multiplexing. Fireworks-style LoRA deployment supports hot-loading and one-click deployment of adapters, with up to <strong>100 adapters on one base model</strong> in a single serving endpoint.</p>
        <p>Operational impact: better GPU utilization for long-tail custom models, faster experimentation cycles, and lower per-adapter idle cost than running dedicated deployments per fine-tune.</p>
        <a href="training.html#sft" class="cross-link">&rarr; How adapters are created during SFT</a>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name"><span class="term" data-term="infiniband">InfiniBand</span> Networking</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">400 Gbps, ~1&micro;s latency, 15-25% of cluster cost</div>
      <div class="sub-topic-detail">
        <p>When models are split via tensor parallelism, every forward pass requires GPUs to exchange data via all-reduce <strong>at every layer</strong>. InfiniBand provides:</p>
        <div class="metrics-row">
          <div class="metric"><div class="metric-value">400 Gbps</div><div class="metric-label">NDR Bandwidth</div></div>
          <div class="metric"><div class="metric-value">~1 &micro;s</div><div class="metric-label">Port-to-Port Latency</div></div>
          <div class="metric"><div class="metric-value">$5-15M</div><div class="metric-label">Per 1,000 GPU Cluster</div></div>
        </div>
        <p><strong>RDMA + GPUDirect</strong>: GPU &rarr; InfiniBand NIC &rarr; NIC &rarr; GPU, no CPU involved. NVIDIA/Mellanox near-monopoly limits price negotiation.</p>
        <p style="margin-top:0.5rem"><a href="economics.html#cost-stack" style="color:var(--accent-dim);font-family:var(--font-mono);font-size:0.75rem;text-decoration:none;letter-spacing:0.05em">15-25% of total cluster cost &rarr;</a></p>
        <p><strong>Ethernet alternative</strong>: 30-50% cheaper with RoCE (RDMA over Converged Ethernet), closing the gap to 2-3x latency difference. Ultra Ethernet Consortium building open standard.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">HBM Specifications</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">DDR5 vs HBM2e vs HBM3 vs HBM3e</div>
      <div class="sub-topic-detail">
        <p>Memory chips stacked 8-12 layers vertically via through-silicon vias (TSVs), placed on the same silicon package as the GPU die.</p>
        <table class="data-table">
          <thead><tr><th>Spec</th><th>DDR5</th><th>HBM2e (A100)</th><th>HBM3 (H100)</th><th>HBM3e (H200)</th></tr></thead>
          <tbody>
            <tr><td>Bandwidth</td><td>~50-100 GB/s</td><td>~2,000 GB/s</td><td>~3,350 GB/s</td><td>~4,800 GB/s</td></tr>
            <tr><td>Capacity/GPU</td><td>N/A</td><td>80 GB</td><td>80 GB</td><td>141 GB</td></tr>
          </tbody>
        </table>
        <p><strong>Bandwidth &rarr; token speed</strong>. <strong>Capacity &rarr; what fits on one GPU</strong>. H200 was a big deal: same compute as H100, but 70B model fits on <strong>one GPU</strong> instead of two &mdash; eliminating communication overhead entirely.</p>
      </div>
    </div>
  </div>
</section>

<!-- ═══════════ SERVING FRAMEWORKS ═══════════ -->
<section class="pipeline-section" id="frameworks">
  <div class="section-label reveal">Putting It All Together</div>
  <h2 class="section-title reveal">Serving Frameworks</h2>
  <p class="section-desc reveal">These frameworks implement the full pipeline above. Each makes different trade-offs.</p>

  <div class="sub-topics reveal">
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">vLLM</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Most widely adopted. PagedAttention + continuous batching.</div>
      <div class="sub-topic-detail">
        <p><strong>Key innovations</strong>: PagedAttention, continuous batching, OpenAI-compatible API. V1 architecture (2025) features separate engine core process for scheduler + KV cache management. Largest community, best model support (~15-20 new models/week).</p>
        <div class="metrics-row">
          <div class="metric"><div class="metric-value">120-160</div><div class="metric-label">req/s</div></div>
          <div class="metric"><div class="metric-value">50-80ms</div><div class="metric-label">TTFT</div></div>
        </div>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">SGLang</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">RadixAttention for automatic prefix sharing.</div>
      <div class="sub-topic-detail">
        <p><strong>RadixAttention</strong> uses a radix tree to store KV cache prefixes, enabling automatic sharing across requests with partial prefix overlap. Up to <strong>6.4x higher throughput</strong> and <strong>3.7x lower latency</strong> than vLLM on structured workloads. Best for agents, tool chains, and RAG systems.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">TensorRT-LLM</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">NVIDIA's optimised library. Fastest at low concurrency.</div>
      <div class="sub-topic-detail">
        <p>Graph-level optimisations, kernel fusion, in-flight batching. Fastest TTFT at low concurrency (<strong>35-50ms</strong>), but can degrade under high load. Tightly coupled to NVIDIA hardware. Best for ultra-low latency with well-supported models.</p>
      </div>
    </div>
    <div class="sub-topic" onclick="toggleSubTopic(this)">
      <div class="sub-topic-header">
        <span class="sub-topic-name">NVIDIA Dynamo</span>
        <span class="sub-topic-icon">+</span>
      </div>
      <div class="sub-topic-preview">Next-gen distributed framework (GTC 2025).</div>
      <div class="sub-topic-detail">
        <p>Built-in prefill-decode disaggregation, dynamic GPU scheduling, LLM-aware request routing. Up to <strong>30x</strong> more requests (DeepSeek-R1 on Blackwell), <strong>2x+</strong> throughput (Llama 70B on Hopper). Dynamo Planner: SLO-driven automation solving the rate-matching challenge between prefill and decode tiers.</p>
      </div>
    </div>
  </div>
</section>

<!-- ═══════════ FULL JOURNEY DIAGRAM ═══════════ -->
<section class="journey-section" id="journey">
  <div class="section-label reveal">End-to-End Summary</div>
  <h2 class="section-title reveal">The Complete Journey</h2>
  <div class="journey-hint reveal">Click any step label to view details</div>
  <div class="journey-container">
    <div class="journey-diagram reveal"><span class="component">User Request</span>
    <span class="arrow-line">|</span>
    <span class="arrow-line">v</span>
<span class="phase-group">── Phase A: Request Preparation ──────────────────</span>
    <span class="arrow-line">|</span>
<span class="phase-label" data-step="routing" tabindex="0" role="button">[01 Request Routing]</span>
    <span class="detail">|-- KV cache aware routing</span>
    <span class="detail">|-- Prefill/decode disaggregation</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="preprocessing" tabindex="0" role="button">[02 Preprocessing]</span>
    <span class="detail">|-- Input validation, rate limiting</span>
    <span class="detail">|-- Prompt template + RAG retrieval</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="tokenization" tabindex="0" role="button">[03 Tokenization]</span>
    <span class="detail">|-- BPE / SentencePiece -> token IDs</span>
    <span class="detail">|-- tiktoken (3-6x faster)</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="embedding" tabindex="0" role="button">[04 Embedding + Position]</span>
    <span class="detail">|-- Token ID -> vector (table lookup)</span>
    <span class="detail">|-- + RoPE positional encoding</span>
    <span class="arrow-line">|</span>
    <span class="arrow-line">v</span>
<span class="phase-group">── Phase B: GPU Computation ───────────────────────</span>
    <span class="arrow-line">|</span>
<span class="phase-label" data-step="scheduling" tabindex="0" role="button">[05 Scheduler Queue]</span>
    <span class="detail">|-- Continuous batching: join next iteration</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="prefill" tabindex="0" role="button">[06 PREFILL]</span> <span class="detail" style="color:var(--compute)">(compute-bound, parallel)</span>
    <span class="detail">|-- Check prefix cache -> skip if hit</span>
    <span class="detail">|-- All tokens through all layers</span>
    <span class="detail">|-- Store K,V in KV cache (PagedAttention)</span>
    <span class="detail">|-- FlashAttention for IO efficiency</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="kvcache" data-step-alt="attention" tabindex="0" role="button">[07-08 KV Cache + Attention]</span>
    <span class="detail">|-- GQA (standard), MLA (DeepSeek)</span>
    <span class="detail">|-- PagedAttention: &lt;4% memory waste</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="decode" tabindex="0" role="button">[09 DECODE LOOP]</span> <span class="detail" style="color:var(--memory)">(memory-bound, sequential)</span>
    <span class="detail">|-- Single token through all layers</span>
    <span class="detail">|-- Q attends to cached K,V</span>
    <span class="detail">|-- Speculative decoding: 2-3x speedup</span>
    <span class="arrow-line">|</span>
    <span class="arrow-line">v</span>
<span class="phase-group">── Phase C: Output &amp; Delivery ────────────────────</span>
    <span class="arrow-line">|</span>
<span class="phase-label" data-step="sampling" tabindex="0" role="button">[10 Sampling]</span>
    <span class="detail">|-- Penalties -> Temperature -> Min-P -> Softmax -> Sample</span>
    <span class="arrow-line">v</span>
<span class="phase-label" data-step="streaming" tabindex="0" role="button">[11 Detokenize + Stream]</span>
    <span class="detail">|-- Incremental detokenization</span>
    <span class="detail">|-- SSE streaming -> content filter -> [DONE]</span>
    <span class="arrow-line">v</span>
<span class="component">User receives streamed response</span>

<span class="detail">─────────────────────────────────────────────────────</span>
<span class="detail" style="color:var(--text-dim)">Cross-cutting: Quantization (FP8/INT4) + Parallelism (TP/PP/EP)</span>
<span class="detail" style="color:var(--text-muted)">Applied across all phases to reduce memory &amp; distribute compute</span></div>
    <div class="journey-detail-panel" id="journey-detail-panel">
      <div class="journey-detail-header">
        <span class="journey-detail-step-number"></span>
        <span class="journey-detail-step-name"></span>
        <button class="journey-detail-close" aria-label="Close">&times;</button>
      </div>
      <div class="journey-detail-content"></div>
    </div>
  </div>
</section>

<!-- ═══════════ FOOTER ═══════════ -->
<footer class="footer">
  <p>Built as an interactive learning reference &middot; Sources: vLLM, NVIDIA, Meta, DeepSeek, Hugging Face, BentoML, Apple, Google &middot; 2025-2026</p>
</footer>

<script>
// ─── Pipeline step toggle (header only) ───
document.querySelectorAll('.pipeline-step').forEach(step => {
  step.querySelector('.step-header').addEventListener('click', (e) => {
    if (e.target.closest('.term')) return;
    step.classList.toggle('active');
  });
});

// ─── Sub-topic toggle ───
function toggleSubTopic(el) {
  el.classList.toggle('expanded');
}

// ─── Scroll reveal ───
const observer = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      entry.target.classList.add('visible');
    }
  });
}, { threshold: 0.1, rootMargin: '0px 0px -40px 0px' });

document.querySelectorAll('.reveal').forEach(el => observer.observe(el));

// ─── Term tooltip system ───
const termDefs = {
  'kv-cache': { def: 'Stores precomputed Key and Value attention tensors so the model doesn\'t recompute them for every previous token at each generation step.', src: 'https://arxiv.org/abs/1706.03762' },
  'ttft': { def: 'Time to First Token \u2014 latency from sending a request to receiving the first generated token. Dominated by prefill time.', src: 'https://docs.vllm.ai/en/latest/serving/metrics.html' },
  'tpot': { def: 'Time Per Output Token \u2014 average latency between consecutive generated tokens during the decode phase.', src: 'https://docs.vllm.ai/en/latest/serving/metrics.html' },
  'bpe': { def: 'Byte Pair Encoding \u2014 a tokenization algorithm that iteratively merges the most frequent adjacent byte/character pairs into new tokens.', src: 'https://arxiv.org/abs/1508.07909' },
  'rope': { def: 'Rotary Position Embedding \u2014 encodes token position by rotating query and key vectors in 2D subspaces using sinusoidal functions. Parameter-free.', src: 'https://arxiv.org/abs/2104.09864' },
  'gqa': { def: 'Grouped-Query Attention \u2014 query heads share Key/Value heads in groups (e.g., 32 Q heads sharing 8 KV heads). The 2025 standard for efficient attention.', src: 'https://arxiv.org/abs/2305.13245' },
  'mha': { def: 'Multi-Head Attention \u2014 the original Transformer attention where each head has fully independent Query, Key, and Value projections.', src: 'https://arxiv.org/abs/1706.03762' },
  'mqa': { def: 'Multi-Query Attention \u2014 all query heads share a single Key/Value head pair. Maximum KV cache reduction at some quality cost.', src: 'https://arxiv.org/abs/1911.02150' },
  'mla': { def: 'Multi-Head Latent Attention \u2014 DeepSeek\'s method that compresses K and V into a low-rank latent before caching, then projects back at inference time.', src: 'https://arxiv.org/abs/2405.04434' },
  'flashattention': { def: 'An IO-aware attention implementation that tiles computation through fast on-chip SRAM (~19 TB/s) instead of slow HBM (~2 TB/s), never materializing the full N\u00d7N attention matrix.', src: 'https://arxiv.org/abs/2205.14135' },
  'pagedattention': { def: 'Manages KV cache like OS virtual memory \u2014 fixed-size blocks allocated on demand with a block table mapping logical to physical blocks. Reduces memory waste to under 4%.', src: 'https://arxiv.org/abs/2309.06180' },
  'prefill': { def: 'The first inference phase: all input tokens are processed through every Transformer layer in parallel, computing and storing the KV cache. Compute-bound.', src: 'https://arxiv.org/abs/2309.06180' },
  'decode': { def: 'The autoregressive generation phase: tokens produced one at a time, each requiring a read of the full KV cache. Memory-bandwidth-bound.', src: 'https://arxiv.org/abs/2309.06180' },
  'tp': { def: 'Tensor Parallelism \u2014 splits individual weight matrices across GPUs within a node. Each GPU computes a slice of every layer. Requires NVLink.', src: 'https://arxiv.org/abs/1909.08053' },
  'pp': { def: 'Pipeline Parallelism \u2014 assigns consecutive layers to different GPUs. Communication only between adjacent stages. Best across nodes.', src: 'https://arxiv.org/abs/1811.06965' },
  'ep': { def: 'Expert Parallelism \u2014 for MoE models, places different experts on different GPUs. Uses All-to-All communication to route tokens to the correct expert.', src: 'https://arxiv.org/abs/2006.16668' },
  'hbm': { def: 'High Bandwidth Memory \u2014 the main GPU memory (e.g., 80 GB on A100). ~2\u20133 TB/s bandwidth. Stores model weights and KV cache.', src: 'https://en.wikipedia.org/wiki/High_Bandwidth_Memory' },
  'sram': { def: 'Static RAM \u2014 small (~20 MB on A100), ultra-fast on-chip GPU memory at ~19 TB/s bandwidth. Used by FlashAttention for tiled computation.', src: 'https://en.wikipedia.org/wiki/Static_random-access_memory' },
  'nvlink': { def: 'NVIDIA\'s high-bandwidth GPU-to-GPU interconnect (~900 GB/s on H100). Essential for tensor parallelism within a single node.', src: 'https://www.nvidia.com/en-us/data-center/nvlink/' },
  'rdma': { def: 'Remote Direct Memory Access \u2014 lets GPUs read/write each other\'s memory directly, bypassing the CPU. Used for KV cache transfer in disaggregated serving.', src: 'https://en.wikipedia.org/wiki/Remote_direct_memory_access' },
  'sse': { def: 'Server-Sent Events \u2014 a simple HTTP protocol for unidirectional server-to-client streaming. The standard for LLM response delivery.', src: 'https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events' },
  'fp16': { def: 'Half-precision floating point (16-bit). The baseline format for LLM weights at inference. 2 bytes per parameter.', src: 'https://en.wikipedia.org/wiki/Half-precision_floating-point_format' },
  'fp8': { def: '8-bit floating point (E4M3 format). On Hopper GPUs (H100+), nearly lossless with 2\u00d7 speed and 2\u00d7 memory savings vs FP16.', src: 'https://arxiv.org/abs/2209.05433' },
  'int4': { def: '4-bit integer quantization. Reduces model size by 4\u00d7 vs FP16. Used by AWQ and GPTQ. Some quality loss (~5\u201310%).', src: 'https://arxiv.org/abs/2210.17323' },
  'lora': { def: 'Low-Rank Adaptation \u2014 parameter-efficient fine-tuning that adds small trainable rank-decomposition matrices alongside frozen base weights.', src: 'https://arxiv.org/abs/2106.09685' },
  'moe': { def: 'Mixture of Experts \u2014 an architecture with many parallel "expert" sub-networks per layer, where only a few activate per token (e.g., 2 of 64).', src: 'https://arxiv.org/abs/1701.06538' },
  'softmax': { def: 'Converts raw logits into a probability distribution where all values are positive and sum to 1: softmax(z_i) = exp(z_i) / \u03a3exp(z_j).', src: 'https://en.wikipedia.org/wiki/Softmax_function' },
  'logits': { def: 'Raw, unnormalized output scores from the model\'s final layer \u2014 one value per vocabulary token. Converted to probabilities via softmax.', src: 'https://en.wikipedia.org/wiki/Logit' },
  'autoregressive': { def: 'A generation method where each new token depends on all previously generated tokens. The model produces output one token at a time, sequentially.', src: 'https://en.wikipedia.org/wiki/Autoregressive_model' },
  'speculative-decoding': { def: 'A small draft model generates candidate tokens quickly, then the large model verifies them all in one parallel pass. Mathematically lossless, 2\u20133\u00d7 speedup.', src: 'https://arxiv.org/abs/2211.17192' },
  'continuous-batching': { def: 'Replacing finished sequences with new requests at every decode iteration, keeping GPU utilization at 80\u201395% vs 30\u201360% with static batching.', src: 'https://arxiv.org/abs/2309.06180' },
  'dynamic-batching': { def: 'A batching strategy that triggers on either max batch size or a timeout \u2014 whichever comes first. Reduces queuing latency vs static batching, but all requests in the batch still finish together.', src: 'https://www.baseten.co/blog/continuous-batching-vs-dynamic-batching-for-llm-serving/' },
  'prefix-caching': { def: 'Reusing previously computed KV cache entries for matching prompt prefixes, skipping redundant prefill computation. Can reduce TTFT by 88%+.', src: 'https://arxiv.org/abs/2312.07104' },
  'awq': { def: 'Activation-aware Weight Quantization \u2014 identifies salient weights via activation magnitudes and preserves them during 4-bit quantization.', src: 'https://arxiv.org/abs/2306.00978' },
  'gptq': { def: 'Post-training quantization using approximate second-order information (Hessian) to minimize quantization error. Widely supported in inference frameworks.', src: 'https://arxiv.org/abs/2210.17323' },
  'gguf': { def: 'A file format for quantized models optimized for CPU and edge inference via llama.cpp. Supports flexible 2\u20138 bit quantization.', src: 'https://github.com/ggerganov/ggml/blob/master/docs/gguf.md' },
  'alibi': { def: 'Attention with Linear Biases \u2014 adds a distance-based linear penalty to attention scores instead of modifying embeddings. Better length extrapolation than RoPE.', src: 'https://arxiv.org/abs/2108.12409' },
  'arithmetic-intensity': { def: 'The ratio of compute operations to bytes loaded from memory. During decode, this ratio is extremely low \u2014 the GPU loads entire weight matrices for single vector operations, leaving compute cores mostly idle.', src: 'https://en.wikipedia.org/wiki/Roofline_model' },
  'infiniband': { def: 'High-bandwidth, low-latency networking (400 Gbps, ~1\u00b5s latency) with RDMA for direct GPU-to-GPU communication. Adds 15-25% to total cluster cost.', src: 'https://en.wikipedia.org/wiki/InfiniBand' },
  'cold-start': { def: 'The delay when a model must be loaded from disk to GPU memory before serving. Takes 30\u2013120 seconds for large models \u2014 loading hundreds of GB from NVMe at ~7 GB/s.', src: 'https://docs.vllm.ai/en/latest/' },
  'disaggregated-serving': { def: 'Running prefill on compute-optimised GPUs and decode on memory-bandwidth-optimised GPUs. Each tier scales independently. Gains: 2\u20137x throughput.', src: 'https://arxiv.org/abs/2401.09670' },
  'fsm-decoding': { def: 'Using finite state machines (actually pushdown automata) to mask invalid tokens at each decode step, enforcing valid output formats like JSON. O(1) per token overhead.', src: 'https://arxiv.org/abs/2307.09702' },
  'chunked-prefill': { def: 'Splits long prompts into chunks processed across iterations, interleaved with decode steps. Prevents one large prefill from blocking ongoing generation.', src: 'https://arxiv.org/abs/2308.16369' },
  'min-p': { def: 'A sampling method that filters tokens below min_p \u00d7 max_probability. Adapts to the model\'s confidence level. Recommended over top-p for 2025.', src: 'https://arxiv.org/abs/2407.01082' },
  'top-p': { def: 'Nucleus sampling \u2014 keeps the smallest token set whose cumulative probability exceeds p. Adaptive but coupled to temperature.', src: 'https://arxiv.org/abs/1904.09751' },
  'top-k': { def: 'Keeps only the k highest-probability tokens for sampling. Simple but uses a fixed cutoff regardless of context.', src: 'https://arxiv.org/abs/1805.04833' },
  'interleaved-thinking': { def: 'Reasoning mode where the model alternates among reasoning, tool calls, and answer generation within one assistant turn.', src: 'https://docs.z.ai/guides/capabilities/thinking-mode' },
  'preserved-thinking': { def: 'Cross-turn reasoning continuity where prior-turn reasoning is retained for subsequent requests.', src: 'https://docs.fireworks.ai/guides/reasoning' },
  'reasoning-content': { def: 'A distinct reasoning channel returned by some APIs (for example as reasoning_content in streaming deltas), separate from visible output text.', src: 'https://docs.fireworks.ai/guides/reasoning' },
  'reasoning-effort': { def: 'Request control for reasoning depth. Fireworks supports minimal/low/medium/high (model-dependent).', src: 'https://docs.fireworks.ai/api-reference/post-chatcompletions' },
  'reasoning-history': { def: 'Cross-turn control for prior reasoning inclusion. Fireworks supports preserved, compact, clear, and none.', src: 'https://docs.fireworks.ai/api-reference/post-chatcompletions' },
  'thinking-budget': { def: 'Reasoning token budget control (for example Z.AI thinking.budget_tokens) used to cap thinking cost and latency.', src: 'https://docs.z.ai/guides/capabilities/thinking-mode' },
  'eos': { def: 'End of Sequence \u2014 a special token signaling the model has finished generating its response.', src: 'https://en.wikipedia.org/wiki/End-of-text_character' },
};

// Term-to-step navigation map
const termStepMap = {
  'kv-cache': { step: 'kvcache', label: '07 KV Cache' },
  'ttft': { step: 'prefill', label: '06 Prefill' },
  'tpot': { step: 'decode', label: '09 Decode Loop' },
  'bpe': { step: 'tokenization', label: '03 Tokenization' },
  'rope': { step: 'embedding', label: '04 Embedding' },
  'gqa': { step: 'attention', label: '08 Attention' },
  'mha': { step: 'attention', label: '08 Attention' },
  'mqa': { step: 'attention', label: '08 Attention' },
  'mla': { step: 'attention', label: '08 Attention' },
  'flashattention': { step: 'attention', label: '08 Attention' },
  'pagedattention': { step: 'kvcache', label: '07 KV Cache' },
  'prefill': { step: 'prefill', label: '06 Prefill' },
  'decode': { step: 'decode', label: '09 Decode Loop' },
  'tp': { id: 'cross-cutting', label: 'Parallelism' },
  'pp': { id: 'cross-cutting', label: 'Parallelism' },
  'ep': { id: 'cross-cutting', label: 'Parallelism' },
  'hbm': { step: 'kvcache', label: '07 KV Cache' },
  'sram': { step: 'attention', label: '08 Attention' },
  'nvlink': { id: 'cross-cutting', label: 'Parallelism' },
  'rdma': { id: 'cross-cutting', label: 'Networking' },
  'sse': { step: 'streaming', label: '11 Streaming' },
  'fp16': { id: 'cross-cutting', label: 'Quantization' },
  'fp8': { id: 'cross-cutting', label: 'Quantization' },
  'int4': { id: 'cross-cutting', label: 'Quantization' },
  'lora': { step: 'routing', label: '01 Routing' },
  'moe': { step: 'attention', label: '08 Attention' },
  'softmax': { step: 'sampling', label: '10 Sampling' },
  'logits': { step: 'sampling', label: '10 Sampling' },
  'autoregressive': { step: 'decode', label: '09 Decode Loop' },
  'speculative-decoding': { step: 'decode', label: '09 Decode Loop' },
  'continuous-batching': { step: 'scheduling', label: '05 Scheduling' },
  'dynamic-batching': { step: 'scheduling', label: '05 Scheduling' },
  'prefix-caching': { step: 'kvcache', label: '07 KV Cache' },
  'awq': { id: 'cross-cutting', label: 'Quantization' },
  'gptq': { id: 'cross-cutting', label: 'Quantization' },
  'gguf': { id: 'cross-cutting', label: 'Quantization' },
  'alibi': { step: 'embedding', label: '04 Embedding' },
  'arithmetic-intensity': { step: 'decode', label: '09 Decode Loop' },
  'infiniband': { id: 'cross-cutting', label: 'Networking' },
  'cold-start': { step: 'scheduling', label: '05 Scheduling' },
  'disaggregated-serving': { step: 'prefill', label: '06 Prefill' },
  'fsm-decoding': { step: 'sampling', label: '10 Sampling' },
  'chunked-prefill': { step: 'prefill', label: '06 Prefill' },
  'min-p': { step: 'sampling', label: '10 Sampling' },
  'top-p': { step: 'sampling', label: '10 Sampling' },
  'top-k': { step: 'sampling', label: '10 Sampling' },
  'interleaved-thinking': { step: 'scheduling', label: '05 Scheduling' },
  'preserved-thinking': { step: 'decode', label: '09 Decode Loop' },
  'reasoning-content': { step: 'decode', label: '09 Decode Loop' },
  'reasoning-effort': { step: 'decode', label: '09 Decode Loop' },
  'reasoning-history': { step: 'decode', label: '09 Decode Loop' },
  'thinking-budget': { step: 'decode', label: '09 Decode Loop' },
  'eos': { step: 'streaming', label: '11 Streaming' },
};

function dismissTooltip() {
  if (activeTooltip) { activeTooltip.remove(); activeTooltip = null; activeTermEl = null; }
}

function navigateToStep(mapping) {
  dismissTooltip();
  var el;
  if (mapping.step) {
    el = document.querySelector('.pipeline-step[data-step="' + mapping.step + '"]');
    if (el && !el.classList.contains('active')) el.classList.add('active');
  } else if (mapping.id) {
    el = document.getElementById(mapping.id);
  }
  if (el) {
    window.scrollTo({ top: el.getBoundingClientRect().top + window.scrollY - 64, behavior: 'smooth' });
  }
}

// Tooltip handler — direct handlers on .term to stop propagation before parent toggles
let activeTooltip = null;
let activeTermEl = null;

function showTermTooltip(term) {
  if (activeTermEl === term && activeTooltip) { dismissTooltip(); return; }
  if (activeTooltip) { activeTooltip.remove(); activeTooltip = null; }
  var key = term.dataset.term;
  var entry = termDefs[key];
  if (!entry) return;
  activeTermEl = term;
  var tooltip = document.createElement('div');
  tooltip.className = 'term-tooltip';
  var html = '<button class="term-tooltip-close" aria-label="Close">\u00d7</button>';
  html += '<div class="term-tooltip-title">' + term.textContent + '</div><div class="term-tooltip-body">' + entry.def + '</div>';
  var mapping = termStepMap[key];
  if (entry.src || mapping) {
    html += '<div class="term-tooltip-footer">';
    if (entry.src) html += '<a class="term-tooltip-source" href="' + entry.src + '" target="_blank" rel="noopener noreferrer">Source \u2197</a>';
    if (mapping) html += '<a class="term-tooltip-nav" data-nav-key="' + key + '">\u2192 ' + mapping.label + '</a>';
    html += '</div>';
  }
  tooltip.innerHTML = html;
  tooltip.querySelector('.term-tooltip-close').addEventListener('click', function(e) { e.stopPropagation(); dismissTooltip(); });
  var srcLink = tooltip.querySelector('.term-tooltip-source');
  if (srcLink) { srcLink.addEventListener('click', function(e) { e.stopPropagation(); }); }
  var navLink = tooltip.querySelector('.term-tooltip-nav');
  if (navLink) {
    navLink.addEventListener('click', function(e) {
      e.stopPropagation(); e.preventDefault();
      var m = termStepMap[navLink.getAttribute('data-nav-key')];
      if (m) navigateToStep(m);
    });
  }
  document.body.appendChild(tooltip);
  var rect = term.getBoundingClientRect();
  var ttRect = tooltip.getBoundingClientRect();
  var top = rect.bottom + 8;
  var left = rect.left + (rect.width / 2) - (ttRect.width / 2);
  if (left < 12) left = 12;
  if (left + ttRect.width > window.innerWidth - 12) left = window.innerWidth - ttRect.width - 12;
  if (top + ttRect.height > window.innerHeight - 12) top = rect.top - ttRect.height - 8;
  tooltip.style.top = top + 'px';
  tooltip.style.left = left + 'px';
  activeTooltip = tooltip;
}

// Direct click on each .term — stopPropagation prevents parent toggles (sub-topics, step-cards)
document.querySelectorAll('.term').forEach(function(t) {
  t.addEventListener('click', function(e) {
    e.stopPropagation();
    e.preventDefault();
    showTermTooltip(t);
  });
});

// Close tooltip when clicking anywhere else
document.addEventListener('click', function(e) {
  if (activeTooltip && !e.target.closest('.term-tooltip') && !e.target.closest('.term')) {
    dismissTooltip();
  }
});

// ─── Journey detail panel ───
(function() {
  var panel = document.getElementById('journey-detail-panel');
  if (!panel) return;
  var panelContent = panel.querySelector('.journey-detail-content');
  var panelNumber = panel.querySelector('.journey-detail-step-number');
  var panelName = panel.querySelector('.journey-detail-step-name');
  var closeBtn = panel.querySelector('.journey-detail-close');
  var activeLabel = null;

  function getStepData(stepId) {
    var stepEl = document.querySelector('.pipeline-step[data-step="' + stepId + '"]');
    if (!stepEl) return null;
    var number = stepEl.querySelector('.step-number');
    var name = stepEl.querySelector('.step-name');
    var detailInner = stepEl.querySelector('.detail-inner');
    return {
      number: number ? number.textContent : '',
      name: name ? name.textContent : '',
      content: detailInner ? detailInner.cloneNode(true) : null
    };
  }

  function initClonedInteractions(container) {
    // Re-attach sub-topic toggles
    container.querySelectorAll('.sub-topic').forEach(function(st) {
      st.removeAttribute('onclick');
      st.addEventListener('click', function(e) {
        if (e.target.closest('.term')) return;
        e.stopPropagation();
        toggleSubTopic(st);
      });
    });
    // Re-attach term tooltips with stopPropagation
    container.querySelectorAll('.term').forEach(function(t) {
      t.addEventListener('click', function(e) {
        e.stopPropagation();
        e.preventDefault();
        showTermTooltip(t);
      });
    });
  }

  function closePanel() {
    panel.classList.remove('visible');
    panelContent.innerHTML = '';
    if (activeLabel) {
      activeLabel.classList.remove('active-label');
      activeLabel = null;
    }
  }

  function openPanel(label) {
    var stepId = label.getAttribute('data-step');
    var altId = label.getAttribute('data-step-alt');

    // Toggle off if clicking same label
    if (activeLabel === label) {
      closePanel();
      return;
    }

    // Remove previous highlight
    if (activeLabel) activeLabel.classList.remove('active-label');
    activeLabel = label;
    label.classList.add('active-label');

    // Get primary step data
    var data = getStepData(stepId);
    if (!data) return;

    panelContent.innerHTML = '';

    // Get alt step data if combined label
    var altData = altId ? getStepData(altId) : null;

    // Set header
    if (altData) {
      panelNumber.textContent = data.number + '-' + altData.number;
      panelName.textContent = data.name + ' + ' + altData.name;
    } else {
      panelNumber.textContent = data.number;
      panelName.textContent = data.name;
    }

    // Add primary content
    if (data.content) {
      panelContent.appendChild(data.content);
      initClonedInteractions(data.content);
    }

    // Add alt step content with divider
    if (altData && altData.content) {
      var divider = document.createElement('hr');
      divider.className = 'journey-detail-divider';
      panelContent.appendChild(divider);
      panelContent.appendChild(altData.content);
      initClonedInteractions(altData.content);
    }

    panel.classList.add('visible');
  }

  // Click handlers on phase labels
  document.querySelectorAll('.journey-diagram .phase-label[data-step]').forEach(function(label) {
    label.addEventListener('click', function(e) {
      e.stopPropagation();
      openPanel(label);
    });
    label.addEventListener('keydown', function(e) {
      if (e.key === 'Enter' || e.key === ' ') {
        e.preventDefault();
        e.stopPropagation();
        openPanel(label);
      }
    });
  });

  // Close button
  closeBtn.addEventListener('click', function(e) {
    e.stopPropagation();
    closePanel();
  });

  // Escape key
  document.addEventListener('keydown', function(e) {
    if (e.key === 'Escape' && panel.classList.contains('visible')) {
      closePanel();
    }
  });
})();

// ─── Mini-map scroll-spy ───
(function() {
  var minimap = document.getElementById('minimap');
  var pipeline = document.getElementById('pipeline');
  var items = minimap.querySelectorAll('.minimap-item');
  var steps = document.querySelectorAll('.pipeline-step[data-step]');

  // Build lookup: data-target -> minimap item
  var itemMap = {};
  items.forEach(function(item) {
    itemMap[item.getAttribute('data-target')] = item;
  });

  // 1. Section observer — show/hide minimap
  var sectionObs = new IntersectionObserver(function(entries) {
    entries.forEach(function(entry) {
      minimap.classList.toggle('visible', entry.isIntersecting);
    });
  }, { threshold: 0 });
  sectionObs.observe(pipeline);

  // 2. Step observer — highlight current step
  var activeSteps = new Set();
  var scrollLock = null; // suppress observer during click-scroll
  var stepObs = new IntersectionObserver(function(entries) {
    entries.forEach(function(entry) {
      var id = entry.target.getAttribute('data-step');
      if (entry.isIntersecting) {
        activeSteps.add(id);
      } else {
        activeSteps.delete(id);
      }
    });
    if (scrollLock) return; // click-scroll in progress, don't override
    // Highlight the step closest to viewport center
    items.forEach(function(item) { item.classList.remove('active'); });
    var vpCenter = window.innerHeight / 2;
    var bestId = null;
    var bestDist = Infinity;
    steps.forEach(function(step) {
      var id = step.getAttribute('data-step');
      if (!activeSteps.has(id)) return;
      var rect = step.getBoundingClientRect();
      var stepCenter = rect.top + rect.height / 2;
      var dist = Math.abs(stepCenter - vpCenter);
      if (dist < bestDist) { bestDist = dist; bestId = id; }
    });
    if (bestId && itemMap[bestId]) {
      itemMap[bestId].classList.add('active');
    }
  }, { rootMargin: '-30% 0px -30% 0px', threshold: 0 });

  steps.forEach(function(step) { stepObs.observe(step); });

  // 3. Click handler — smooth scroll + toggle expand/collapse + immediate highlight
  items.forEach(function(item) {
    item.addEventListener('click', function(e) {
      e.preventDefault();
      var target = item.getAttribute('data-target');
      var stepEl = document.querySelector('.pipeline-step[data-step="' + target + '"]') || document.getElementById(target);
      if (stepEl) {
        // Toggle section expand/collapse (only for pipeline steps)
        if (stepEl.classList.contains('pipeline-step')) stepEl.classList.toggle('active');
        // Immediately highlight clicked item
        items.forEach(function(i) { i.classList.remove('active'); });
        item.classList.add('active');
        // Suppress observer during smooth scroll
        clearTimeout(scrollLock);
        scrollLock = setTimeout(function() { scrollLock = null; }, 800);
        var top = stepEl.getBoundingClientRect().top + window.scrollY - 64;
        window.scrollTo({ top: top, behavior: 'smooth' });
      }
    });
  });
})();

// ─── Theme toggle ───
(function() {
  var toggle = document.getElementById('theme-toggle');
  var html = document.documentElement;
  function getTheme() { return html.getAttribute('data-theme') || 'dark'; }
  toggle.addEventListener('click', function() {
    var next = getTheme() === 'dark' ? 'light' : 'dark';
    html.setAttribute('data-theme', next);
    localStorage.setItem('theme', next);
  });
  if (window.matchMedia) {
    window.matchMedia('(prefers-color-scheme: light)').addEventListener('change', function(e) {
      if (!localStorage.getItem('theme')) {
        html.setAttribute('data-theme', e.matches ? 'light' : 'dark');
      }
    });
  }
})();

// ─── Step Visual System (lazy init via MutationObserver) ───
(function() {
  var visualInits = {};
  var initialized = {};

  function initVisual(step) {
    var id = step.getAttribute('data-step');
    if (initialized[id] || !visualInits[id]) return;
    var container = step.querySelector('.step-visual');
    if (!container || container.getAttribute('data-init')) return;
    container.setAttribute('data-init', '1');
    visualInits[id](container);
    initialized[id] = true;
  }

  // Observe class changes on pipeline steps
  var obs = new MutationObserver(function(muts) {
    muts.forEach(function(m) {
      if (m.type === 'attributes' && m.attributeName === 'class') {
        var t = m.target;
        if (t.classList.contains('pipeline-step') && t.classList.contains('active')) {
          initVisual(t);
        }
      }
    });
  });
  document.querySelectorAll('.pipeline-step').forEach(function(s) {
    obs.observe(s, { attributes: true });
  });

  // Journey panel: observe for cloned visual containers and initialize them
  var journeyPanel = document.getElementById('journey-detail-panel');
  if (journeyPanel) {
    var jpContent = journeyPanel.querySelector('.journey-detail-content');
    if (jpContent) {
      var jpObs = new MutationObserver(function() {
        var visuals = jpContent.querySelectorAll('.step-visual:not([data-init])');
        visuals.forEach(function(vis) {
          var id = vis.getAttribute('data-visual');
          if (id && visualInits[id]) {
            vis.setAttribute('data-init', '1');
            setTimeout(function() { visualInits[id](vis); }, 150);
          }
        });
      });
      jpObs.observe(jpContent, { childList: true, subtree: true });
    }
  }

  // Helper: create element with classes
  function el(tag, cls, text) {
    var e = document.createElement(tag);
    if (cls) e.className = cls;
    if (text) e.textContent = text;
    return e;
  }

  function stopProp(e) { e.stopPropagation(); }

  // ═══ 01 ROUTING ═══
  visualInits.routing = function(c) {
    var w = c.offsetWidth, h = c.offsetHeight;
    // Nodes
    var nodes = [
      { label: 'API Gateway', x: 0.08, y: 0.18 },
      { label: 'Load Balancer', x: 0.38, y: 0.18 },
      { label: 'GPU Worker 1', x: 0.68, y: 0.08 },
      { label: 'GPU Worker 2', x: 0.68, y: 0.38 },
      { label: 'GPU Worker 3', x: 0.68, y: 0.68 }
    ];
    nodes.forEach(function(n) {
      var node = el('div', 'sv-node', n.label);
      node.style.left = (n.x * w) + 'px';
      node.style.top = (n.y * h) + 'px';
      n.el = node;
      c.appendChild(node);
    });

    // KV fill bars for workers
    [2,3,4].forEach(function(i, idx) {
      var bar = el('div', 'sv-bar');
      bar.style.position = 'absolute';
      bar.style.left = (nodes[i].x * w) + 'px';
      bar.style.top = (nodes[i].y * h + 28) + 'px';
      bar.style.width = '90px';
      var fill = el('div', 'sv-bar-fill');
      fill.style.background = 'var(--accent)';
      fill.style.width = [65, 30, 80][idx] + '%';
      bar.appendChild(fill);
      c.appendChild(bar);
      var lbl = el('div', 'sv-label');
      lbl.style.position = 'absolute';
      lbl.style.left = (nodes[i].x * w + 94) + 'px';
      lbl.style.top = (nodes[i].y * h + 28) + 'px';
      lbl.textContent = ['65%', '30%', '80%'][idx] + ' KV';
      c.appendChild(lbl);
    });

    // Lines
    var lines = [[0,1],[1,2],[1,3],[1,4]];
    lines.forEach(function(l) {
      var a = nodes[l[0]], b = nodes[l[1]];
      var line = el('div', 'sv-line h');
      var x1 = a.x * w + 80, y1 = a.y * h + 10;
      var x2 = b.x * w, y2 = b.y * h + 10;
      var dx = x2 - x1, dy = y2 - y1;
      var len = Math.sqrt(dx*dx + dy*dy);
      var angle = Math.atan2(dy, dx) * 180 / Math.PI;
      line.style.width = len + 'px';
      line.style.left = x1 + 'px';
      line.style.top = y1 + 'px';
      line.style.transform = 'rotate(' + angle + 'deg)';
      line.style.transformOrigin = '0 0';
      c.appendChild(line);
    });

    // Animated packet
    var packet = el('div', 'sv-packet');
    c.appendChild(packet);
    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;left:8%;bottom:22%;display:flex;align-items:center;gap:10px;font-size:0.45rem;color:var(--text-muted)';
    var kvSwatch = el('div', '');
    kvSwatch.style.cssText = 'width:24px;height:6px;background:var(--accent);border-radius:2px';
    legend.appendChild(kvSwatch);
    legend.appendChild(el('span', '', 'KV Cache Usage'));
    c.appendChild(legend);

    var stat = el('div', 'sv-stat', 'Route to lowest KV usage');
    c.appendChild(stat);

    var animating = false;
    function animatePacket() {
      if (animating) return;
      animating = true;
      var target = 3; // Worker 2 (lowest KV)
      var path = [nodes[0], nodes[1], nodes[target]];
      var step = 0;
      nodes.forEach(function(n) { n.el.classList.remove('highlight'); });

      function moveNext() {
        if (step >= path.length) {
          animating = false;
          return;
        }
        var n = path[step];
        n.el.classList.add('highlight');
        packet.style.left = (n.x * w + 30) + 'px';
        packet.style.top = (n.y * h + 6) + 'px';
        packet.style.transition = 'left 0.4s ease, top 0.4s ease';
        step++;
        setTimeout(moveNext, 500);
      }
      packet.style.transition = 'none';
      packet.style.left = (nodes[0].x * w - 15) + 'px';
      packet.style.top = (nodes[0].y * h + 6) + 'px';
      setTimeout(moveNext, 100);
    }

    var ctrl = el('div', 'sv-controls');
    var btn = el('button', 'sv-btn', 'Replay');
    btn.addEventListener('click', function(e) { stopProp(e); animatePacket(); });
    ctrl.appendChild(btn);
    c.appendChild(ctrl);
    setTimeout(animatePacket, 300);
  };

  // ═══ 02 PREPROCESSING ═══
  visualInits.preprocessing = function(c) {
    var w = c.offsetWidth, h = c.offsetHeight;
    var layers = [
      { label: 'System Prompt', tokens: '~500 tokens', color: 'var(--accent)', y: 0.55 },
      { label: 'Few-Shot Examples', tokens: '~200 tokens', color: 'var(--logic)', y: 0.35 },
      { label: 'User Input', tokens: '~50 tokens', color: 'var(--io)', y: 0.15 }
    ];
    var assembled = false;

    var stat = el('div', 'sv-stat', 'Hover layers to see token counts');

    layers.forEach(function(l, i) {
      var box = el('div', 'sv-node');
      box.innerHTML = '<strong>' + l.label + '</strong><br><span style="font-size:0.55rem;color:var(--text-muted)">' + l.tokens + '</span>';
      box.style.left = '8%';
      box.style.top = (l.y * h) + 'px';
      box.style.width = '140px';
      box.style.textAlign = 'center';
      box.style.borderColor = l.color;
      box.style.transition = 'top 0.6s ease, left 0.6s ease';
      box.style.cursor = 'pointer';
      l.el = box;
      box.addEventListener('mouseenter', function() {
        box.classList.add('highlight');
        stat.textContent = l.label + ': ' + l.tokens;
      });
      box.addEventListener('mouseleave', function() {
        box.classList.remove('highlight');
        stat.textContent = 'Hover layers to see token counts';
      });
      c.appendChild(box);
    });

    // Bracket for prefix cacheable
    var bracket = el('div', '');
    bracket.style.cssText = 'position:absolute;right:15%;top:20%;height:50%;border:1px dashed var(--accent-dim);border-left:none;width:20px;border-radius:0 6px 6px 0;opacity:0;transition:opacity 0.5s';
    c.appendChild(bracket);
    var bracketLabel = el('div', 'sv-label');
    bracketLabel.textContent = 'PREFIX CACHEABLE';
    bracketLabel.style.cssText = 'right:3%;top:42%;transform:rotate(0deg);font-size:0.45rem;opacity:0;transition:opacity 0.5s';
    c.appendChild(bracketLabel);

    // Arrow result
    var arrow = el('div', '');
    arrow.style.cssText = 'position:absolute;left:52%;top:35%;font-size:1.5rem;color:var(--text-muted);opacity:0;transition:opacity 0.4s';
    arrow.textContent = '\u2192';
    c.appendChild(arrow);

    var result = el('div', 'sv-node');
    result.innerHTML = '<strong>Final Prompt</strong><br><span style="font-size:0.55rem;color:var(--text-muted)">~750 tokens</span>';
    result.style.cssText = 'left:62%;top:30%;width:120px;text-align:center;opacity:0;transition:opacity 0.5s;border-color:var(--accent)';
    c.appendChild(result);

    c.appendChild(stat);

    var ctrl = el('div', 'sv-controls');
    var btn = el('button', 'sv-btn', 'Assemble');
    btn.addEventListener('click', function(e) {
      stopProp(e);
      if (!assembled) {
        layers.forEach(function(l, i) {
          l.el.style.left = '30%';
          l.el.style.top = (0.2 * h + i * 45) + 'px';
        });
        arrow.style.opacity = '1';
        result.style.opacity = '1';
        bracket.style.opacity = '1';
        bracketLabel.style.opacity = '1';
        btn.textContent = 'Reset';
        assembled = true;
      } else {
        layers.forEach(function(l) {
          l.el.style.left = '8%';
          l.el.style.top = (l.y * h) + 'px';
        });
        arrow.style.opacity = '0';
        result.style.opacity = '0';
        bracket.style.opacity = '0';
        bracketLabel.style.opacity = '0';
        btn.textContent = 'Assemble';
        assembled = false;
      }
    });
    ctrl.appendChild(btn);
    c.appendChild(ctrl);
  };

  // ═══ 03 TOKENIZATION ═══
  visualInits.tokenization = function(c) {
    var samples = {
      'English': { text: 'How does inference work?', tokens: ['How', ' does', ' inference', ' work', '?'], ids: [2347, 1721, 45892, 1736, 30], ratio: '~4.6 chars/token' },
      'Code': { text: 'def forward(self, x):', tokens: ['def', ' forward', '(', 'self', ',', ' x', '):'], ids: [755, 4054, 7, 944, 11, 865, 1125], ratio: '~3.0 chars/token' },
      'Japanese': { text: '\u304A\u5143\u6C17\u3067\u3059\u304B', tokens: ['\u304A', '\u5143', '\u6C17', '\u3067\u3059', '\u304B'], ids: [2571, 3901, 4827, 9823, 1548], ratio: '~1.2 chars/token' }
    };
    var current = 'English';

    var inputDiv = el('div', '');
    inputDiv.style.cssText = 'position:absolute;top:12%;left:5%;right:5%;text-align:center';
    var inputLabel = el('div', 'sv-label');
    inputLabel.style.cssText = 'position:static;margin-bottom:6px';
    inputLabel.textContent = 'INPUT TEXT';
    inputDiv.appendChild(inputLabel);
    var inputText = el('div', '');
    inputText.style.cssText = 'font-size:1rem;color:var(--text);font-family:var(--font-body)';
    inputDiv.appendChild(inputText);
    c.appendChild(inputDiv);

    var arrowDiv = el('div', '');
    arrowDiv.style.cssText = 'position:absolute;top:38%;left:50%;transform:translateX(-50%);color:var(--text-muted);font-size:1.2rem';
    arrowDiv.textContent = '\u2193 BPE \u2193';
    c.appendChild(arrowDiv);

    var tokensDiv = el('div', '');
    tokensDiv.style.cssText = 'position:absolute;top:48%;left:5%;right:5%;text-align:center';
    var tokLabel = el('div', 'sv-label');
    tokLabel.style.cssText = 'position:static;margin-bottom:6px';
    tokLabel.textContent = 'TOKENS';
    tokensDiv.appendChild(tokLabel);
    var tokRow = el('div', '');
    tokRow.style.cssText = 'display:flex;flex-wrap:wrap;justify-content:center;gap:4px';
    tokensDiv.appendChild(tokRow);
    c.appendChild(tokensDiv);

    var ratioDiv = el('div', 'sv-stat');
    c.appendChild(ratioDiv);

    var colors = ['var(--accent)', 'var(--logic)', 'var(--io)', 'var(--compute)', 'var(--memory)', 'var(--network)', 'var(--accent-dim)'];

    function render() {
      var s = samples[current];
      inputText.textContent = s.text;
      tokRow.innerHTML = '';
      s.tokens.forEach(function(t, i) {
        var tok = el('span', 'sv-token');
        tok.textContent = t;
        tok.style.borderColor = colors[i % colors.length];
        tok.style.color = colors[i % colors.length];
        tok.style.animation = 'svAppear 0.3s ease ' + (i * 0.08) + 's both';
        var idSpan = document.createElement('span');
        idSpan.style.cssText = 'display:block;font-size:0.5rem;color:var(--text-muted);margin-top:2px';
        idSpan.textContent = s.ids[i];
        tok.appendChild(idSpan);
        tokRow.appendChild(tok);
      });
      ratioDiv.textContent = s.tokens.length + ' tokens \u2022 ' + s.ratio;
    }

    var ctrl = el('div', 'sv-controls');
    ['English', 'Code', 'Japanese'].forEach(function(lang) {
      var btn = el('button', 'sv-btn' + (lang === current ? ' active' : ''), lang);
      btn.addEventListener('click', function(e) {
        stopProp(e);
        current = lang;
        ctrl.querySelectorAll('.sv-btn').forEach(function(b) { b.classList.remove('active'); });
        btn.classList.add('active');
        render();
      });
      ctrl.appendChild(btn);
    });
    c.appendChild(ctrl);
    render();
  };

  // ═══ 04 EMBEDDING ═══
  visualInits.embedding = function(c) {
    var tokens = ['How', 'does', 'inference', 'work', '?'];
    var dims = 8;
    var vectors = tokens.map(function() {
      var v = [];
      for (var i = 0; i < dims; i++) v.push(Math.random() * 0.8 + 0.1);
      return v;
    });

    var leftCol = el('div', '');
    leftCol.style.cssText = 'position:absolute;left:5%;top:15%;width:20%';
    var lbl1 = el('div', 'sv-label');
    lbl1.style.cssText = 'position:static;margin-bottom:8px';
    lbl1.textContent = 'TOKEN IDS';
    leftCol.appendChild(lbl1);
    tokens.forEach(function(t, i) {
      var row = el('div', 'sv-node');
      row.style.cssText = 'position:static;margin-bottom:4px;cursor:pointer;text-align:center;font-size:0.6rem';
      row.textContent = t;
      row.setAttribute('data-idx', i);
      row.addEventListener('click', function(e) { stopProp(e); highlightToken(i); });
      row.addEventListener('mouseenter', function() { highlightToken(i); });
      leftCol.appendChild(row);
    });
    c.appendChild(leftCol);

    var arrowDiv = el('div', '');
    arrowDiv.style.cssText = 'position:absolute;left:28%;top:45%;color:var(--text-muted);font-size:1.5rem';
    arrowDiv.textContent = '\u2192';
    c.appendChild(arrowDiv);

    var lookupLabel = el('div', 'sv-label');
    lookupLabel.textContent = 'LOOKUP TABLE';
    lookupLabel.style.cssText = 'position:absolute;left:35%;top:10%';
    c.appendChild(lookupLabel);

    var rightCol = el('div', '');
    rightCol.style.cssText = 'position:absolute;left:35%;top:15%;width:60%';
    var lbl2 = el('div', 'sv-label');
    lbl2.style.cssText = 'position:static;margin-bottom:8px';
    lbl2.textContent = 'EMBEDDING VECTORS (8 of 4096 DIMS)';
    rightCol.appendChild(lbl2);

    var barRows = [];
    tokens.forEach(function(t, i) {
      var row = el('div', '');
      row.style.cssText = 'display:flex;align-items:center;gap:3px;margin-bottom:4px;opacity:0.4;transition:opacity 0.3s';
      var name = el('span', '');
      name.style.cssText = 'width:60px;font-size:0.55rem;color:var(--text-muted)';
      name.textContent = t;
      row.appendChild(name);
      vectors[i].forEach(function(v) {
        var bar = el('div', '');
        bar.style.cssText = 'width:' + (v * 40) + 'px;height:10px;background:var(--accent);border-radius:2px;transition:width 0.3s';
        row.appendChild(bar);
      });
      rightCol.appendChild(row);
      barRows.push(row);
    });
    c.appendChild(rightCol);

    var stat = el('div', 'sv-stat', 'Hover or click a token');
    c.appendChild(stat);

    function highlightToken(idx) {
      barRows.forEach(function(r, i) { r.style.opacity = i === idx ? '1' : '0.25'; });
      leftCol.querySelectorAll('.sv-node').forEach(function(n, i) {
        n.classList.toggle('highlight', i === idx);
      });
      stat.textContent = tokens[idx] + ' \u2192 [' + vectors[idx].map(function(v) { return v.toFixed(2); }).join(', ') + '...]';
    }
  };

  // ═══ 05 SCHEDULING ═══
  visualInits.scheduling = function(c) {
    var cols = 12, rows = 4;
    var mode = 'static';

    var staticData = [
      [1,1,1,1,1,1,0,0,0,0,0,0],
      [1,1,1,1,1,1,1,1,1,0,0,0],
      [1,1,1,0,0,0,0,0,0,0,0,0],
      [1,1,1,1,1,1,1,1,1,1,1,1]
    ];
    var dynamicData = [
      [1,1,1,1,1,0, 3,3,3,3,3,3],
      [1,1,1,1,1,1, 3,3,3,3,0,0],
      [1,1,0,0,0,0, 3,3,3,3,3,0],
      [0,0,0,0,0,0, 3,3,3,3,3,3]
    ];
    var contData = [
      [1,1,1,1,1,3,3,3,3,3,3,3],
      [1,1,1,1,1,1,1,1,1,3,3,3],
      [1,1,1,3,3,3,3,3,5,5,5,5],
      [1,1,1,1,1,1,1,1,1,1,1,1]
    ];

    var colorMap = {
      1: { bg: 'var(--accent)', border: 'var(--accent-dim)', label: 'Request A' },
      3: { bg: 'var(--io)', border: 'var(--io)', label: 'Request B' },
      5: { bg: 'var(--logic)', border: 'var(--logic)', label: 'Request C' }
    };

    function calcUtil(data) {
      var total = 0, used = 0;
      data.forEach(function(r) { r.forEach(function(v) { total++; if (v > 0) used++; }); });
      return Math.round(used / total * 100);
    }

    // Title
    var titleL = el('div', 'sv-label');
    titleL.style.cssText = 'position:absolute;left:5%;top:6%';
    c.appendChild(titleL);

    var stat = el('div', 'sv-stat');
    c.appendChild(stat);

    // Grid area: slot labels (left) + grid (right) using CSS grid
    var gridArea = el('div', '');
    gridArea.style.cssText = 'position:absolute;left:5%;top:18%;right:5%;bottom:28%';
    c.appendChild(gridArea);

    // Inner table: slot label col + grid cols
    var tableDiv = el('div', '');
    tableDiv.style.cssText = 'display:grid;grid-template-columns:auto 1fr;gap:0 6px;width:100%;height:100%';
    gridArea.appendChild(tableDiv);

    // Slot labels container (left column)
    var slotLabels = el('div', '');
    slotLabels.style.cssText = 'display:grid;grid-template-rows:repeat(' + rows + ',1fr);gap:2px;align-items:stretch';
    tableDiv.appendChild(slotLabels);
    for (var si = 0; si < rows; si++) {
      var slbl = el('div', '');
      slbl.style.cssText = 'display:flex;align-items:center;font-family:var(--font-mono);font-size:0.45rem;color:var(--text-muted);white-space:nowrap;padding-right:2px';
      slbl.textContent = 'Slot ' + si;
      slotLabels.appendChild(slbl);
    }

    // Grid container (right column)
    var gridDiv = el('div', '');
    gridDiv.style.cssText = 'display:grid;grid-template-columns:repeat(' + cols + ',1fr);grid-template-rows:repeat(' + rows + ',1fr);gap:2px';
    tableDiv.appendChild(gridDiv);

    // Time axis below grid
    var timeLabel = el('div', '');
    timeLabel.style.cssText = 'text-align:center;font-family:var(--font-mono);font-size:0.45rem;letter-spacing:0.15em;color:var(--text-muted);margin-top:4px';
    timeLabel.textContent = 'TIME \u2192';
    gridArea.appendChild(timeLabel);

    // Legend container (below time axis)
    var legendDiv = el('div', '');
    legendDiv.style.cssText = 'display:none;justify-content:center;gap:0.75rem;margin-top:4px';
    gridArea.appendChild(legendDiv);

    var titles = { static: 'STATIC BATCHING', dynamic: 'DYNAMIC BATCHING (TIMEOUT + SIZE TRIGGERS)', continuous: 'CONTINUOUS BATCHING' };

    function render() {
      var data = mode === 'static' ? staticData : mode === 'dynamic' ? dynamicData : contData;
      titleL.textContent = titles[mode];
      stat.textContent = 'GPU Util: ' + calcUtil(data) + '%';
      gridDiv.innerHTML = '';
      data.forEach(function(row) {
        row.forEach(function(val) {
          var cell = el('div', '');
          cell.style.cssText = 'border-radius:2px;border:1px solid var(--border);transition:background 0.15s';
          if (val === 0) { cell.style.background = 'rgba(213,94,0,0.25)'; cell.style.borderColor = 'rgba(213,94,0,0.4)'; }
          else { var cm = colorMap[val]; cell.style.background = cm.bg; cell.style.borderColor = cm.border; }
          gridDiv.appendChild(cell);
        });
      });

      // Build legend
      legendDiv.innerHTML = '';
      legendDiv.style.display = 'flex';
      if (mode === 'static') {
        // Static: Active + Idle
        addLegendItem(legendDiv, 'var(--accent)', '', 'Active');
        addLegendItem(legendDiv, 'rgba(213,94,0,0.25)', 'rgba(213,94,0,0.4)', 'Idle (wasted)');
      } else {
        // Dynamic and Continuous: show used request colors + Idle
        var usedVals = {};
        data.forEach(function(r) { r.forEach(function(v) { if (v > 0) usedVals[v] = true; }); });
        Object.keys(usedVals).sort().forEach(function(v) {
          var cm = colorMap[parseInt(v)];
          if (cm) addLegendItem(legendDiv, cm.bg, '', cm.label);
        });
        addLegendItem(legendDiv, 'rgba(213,94,0,0.25)', 'rgba(213,94,0,0.4)', 'Idle');
      }
    }

    function addLegendItem(parent, bg, border, text) {
      var item = el('div', '');
      item.style.cssText = 'display:flex;align-items:center;gap:4px;font-family:var(--font-mono);font-size:0.45rem;color:var(--text-muted)';
      var swatch = el('div', '');
      swatch.style.cssText = 'width:10px;height:10px;border-radius:2px;background:' + bg + (border ? ';border:1px solid ' + border : '');
      item.appendChild(swatch);
      item.appendChild(document.createTextNode(text));
      parent.appendChild(item);
    }

    var ctrl = el('div', 'sv-controls');
    ['Static', 'Dynamic', 'Continuous'].forEach(function(m) {
      var btn = el('button', 'sv-btn' + (m.toLowerCase() === mode ? ' active' : ''), m);
      btn.addEventListener('click', function(e) {
        stopProp(e);
        mode = m.toLowerCase();
        ctrl.querySelectorAll('.sv-btn').forEach(function(b) { b.classList.remove('active'); });
        btn.classList.add('active');
        render();
      });
      ctrl.appendChild(btn);
    });
    c.appendChild(ctrl);
    render();
  };

  // ═══ 06 PREFILL ═══
  visualInits.prefill = function(c) {
    var tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
    var n = tokens.length;
    var mode = 'prefill';

    var titleDiv = el('div', 'sv-label');
    titleDiv.style.cssText = 'position:absolute;left:5%;top:6%';
    c.appendChild(titleDiv);

    var gridContainer = el('div', '');
    gridContainer.style.cssText = 'position:absolute;left:12%;top:16%;right:12%';
    c.appendChild(gridContainer);

    // Token labels (top)
    var topLabels = el('div', '');
    topLabels.style.cssText = 'display:grid;gap:2px;margin-bottom:4px;grid-template-columns:40px repeat(' + n + ',1fr)';
    var corner = el('div', '');
    topLabels.appendChild(corner);
    tokens.forEach(function(t) {
      var lbl = el('div', '');
      lbl.style.cssText = 'text-align:center;font-size:0.5rem;color:var(--text-muted)';
      lbl.textContent = t;
      topLabels.appendChild(lbl);
    });
    gridContainer.appendChild(topLabels);

    var grid = el('div', 'sv-grid');
    grid.style.gridTemplateColumns = '40px repeat(' + n + ', 1fr)';
    grid.style.width = '100%';
    var cells = [];
    for (var i = 0; i < n; i++) {
      var rowLabel = el('div', '');
      rowLabel.style.cssText = 'display:flex;align-items:center;font-size:0.5rem;color:var(--text-muted)';
      rowLabel.textContent = tokens[i];
      grid.appendChild(rowLabel);
      var row = [];
      for (var j = 0; j < n; j++) {
        var cell = el('div', 'sv-cell');
        cell.style.width = '100%';
        cell.style.paddingBottom = '100%';
        cell.style.height = '0';
        grid.appendChild(cell);
        row.push(cell);
      }
      cells.push(row);
    }
    gridContainer.appendChild(grid);

    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;right:12%;top:6%;display:flex;align-items:center;gap:12px;font-size:0.45rem;color:var(--text-muted)';
    var litSwatch = el('div', '');
    litSwatch.style.cssText = 'width:10px;height:10px;background:var(--accent);border:1px solid var(--accent-dim);border-radius:2px';
    legend.appendChild(litSwatch);
    legend.appendChild(el('span', '', 'Computed'));
    var emSwatch = el('div', '');
    emSwatch.style.cssText = 'width:10px;height:10px;background:transparent;border:1px solid var(--border);border-radius:2px';
    legend.appendChild(emSwatch);
    legend.appendChild(el('span', '', 'Masked (causal)'));
    c.appendChild(legend);

    var stat = el('div', 'sv-stat');
    c.appendChild(stat);

    function render() {
      titleDiv.textContent = mode === 'prefill' ? 'PREFILL: ALL CELLS SIMULTANEOUSLY' : 'DECODE: ROW BY ROW (SEQUENTIAL)';
      stat.textContent = mode === 'prefill' ? 'Compute-bound \u2022 Parallel' : 'Memory-bound \u2022 Sequential';
      // Clear all
      cells.forEach(function(row) { row.forEach(function(cell) {
        cell.className = 'sv-cell';
        cell.style.transition = 'background 0.15s, border-color 0.15s';
      }); });

      if (mode === 'prefill') {
        // Light up causal mask (lower triangle) all at once
        setTimeout(function() {
          for (var i = 0; i < n; i++) {
            for (var j = 0; j <= i; j++) {
              cells[i][j].classList.add('lit');
            }
          }
        }, 200);
      } else {
        // Light up row by row with delay
        var row = 0;
        function lightRow() {
          if (row >= n) return;
          for (var j = 0; j <= row; j++) {
            cells[row][j].classList.add('lit');
          }
          row++;
          setTimeout(lightRow, 400);
        }
        setTimeout(lightRow, 200);
      }
    }

    var ctrl = el('div', 'sv-controls');
    ['Prefill', 'Decode'].forEach(function(m) {
      var btn = el('button', 'sv-btn' + (m.toLowerCase() === mode ? ' active' : ''), m);
      btn.addEventListener('click', function(e) {
        stopProp(e);
        mode = m.toLowerCase();
        ctrl.querySelectorAll('.sv-btn').forEach(function(b) { b.classList.remove('active'); });
        btn.classList.add('active');
        render();
      });
      ctrl.appendChild(btn);
    });
    c.appendChild(ctrl);
    render();
  };

  // ═══ 07 KV CACHE ═══
  visualInits.kvcache = function(c) {
    var w = c.offsetWidth, h = c.offsetHeight;
    var numTokens = 4;

    var logLabel = el('div', 'sv-label');
    logLabel.textContent = 'LOGICAL (SEQUENTIAL)';
    logLabel.style.cssText = 'position:absolute;left:8%;top:8%';
    c.appendChild(logLabel);

    var physLabel = el('div', 'sv-label');
    physLabel.textContent = 'PHYSICAL (PAGED)';
    physLabel.style.cssText = 'position:absolute;left:55%;top:8%';
    c.appendChild(physLabel);

    var logSlots = [];
    var physSlots = [];
    var physPositions = [0.15, 0.6, 0.35, 0.75, 0.5, 0.2, 0.85, 0.45];
    var physYPositions = [0.2, 0.25, 0.5, 0.45, 0.7, 0.75, 0.6, 0.35];
    var connections = [];

    function addToken() {
      if (numTokens > 7) return;
      var i = numTokens;
      // Logical slot
      var lslot = el('div', 'sv-node');
      lslot.textContent = 'Token ' + i;
      lslot.style.cssText = 'position:absolute;left:8%;top:' + (18 + i * 28) + 'px;font-size:0.55rem;width:70px;text-align:center';
      lslot.style.animation = 'svAppear 0.3s ease both';
      c.appendChild(lslot);
      logSlots.push(lslot);

      // Physical block
      var px = 55 + physPositions[i % physPositions.length] * 35;
      var py = 15 + physYPositions[i % physYPositions.length] * 70;
      var pslot = el('div', 'sv-node highlight');
      pslot.textContent = 'Block ' + i;
      pslot.style.cssText = 'position:absolute;left:' + px + '%;top:' + py + '%;font-size:0.55rem;width:65px;text-align:center';
      pslot.style.animation = 'svAppear 0.3s ease both';
      c.appendChild(pslot);
      physSlots.push(pslot);

      // Connection line (SVG would be better but keep it simple)
      var conn = el('div', 'sv-line h active');
      conn.style.cssText = 'position:absolute;height:1px;background:var(--accent-dim);opacity:0.5';
      var lx = 25;
      var ly = 18 + i * 28 + 8;
      var rx = px;
      var ry = py;
      // Approximate with % → px
      setTimeout(function() {
        var lslotRect = lslot.getBoundingClientRect();
        var pslotRect = pslot.getBoundingClientRect();
        var cRect = c.getBoundingClientRect();
        var x1 = lslotRect.right - cRect.left;
        var y1 = lslotRect.top + lslotRect.height/2 - cRect.top;
        var x2 = pslotRect.left - cRect.left;
        var y2 = pslotRect.top + pslotRect.height/2 - cRect.top;
        var dx = x2 - x1, dy = y2 - y1;
        var len = Math.sqrt(dx*dx + dy*dy);
        var angle = Math.atan2(dy, dx) * 180 / Math.PI;
        conn.style.width = len + 'px';
        conn.style.left = x1 + 'px';
        conn.style.top = y1 + 'px';
        conn.style.transform = 'rotate(' + angle + 'deg)';
        conn.style.transformOrigin = '0 0';
      }, 100);
      c.appendChild(conn);
      connections.push(conn);
      numTokens++;
      stat.textContent = numTokens + ' tokens \u2022 ' + numTokens + ' blocks allocated';
    }

    // Initial tokens
    for (var t = 0; t < numTokens; t++) {
      (function(i) {
        var lslot = el('div', 'sv-node');
        lslot.textContent = 'Token ' + i;
        lslot.style.cssText = 'position:absolute;left:8%;top:' + (18 + i * 28) + 'px;font-size:0.55rem;width:70px;text-align:center';
        c.appendChild(lslot);
        logSlots.push(lslot);

        var px = 55 + physPositions[i] * 35;
        var py = 15 + physYPositions[i] * 70;
        var pslot = el('div', 'sv-node highlight');
        pslot.textContent = 'Block ' + i;
        pslot.style.cssText = 'position:absolute;left:' + px + '%;top:' + py + '%;font-size:0.55rem;width:65px;text-align:center';
        c.appendChild(pslot);
        physSlots.push(pslot);

        var conn = el('div', 'sv-line h active');
        conn.style.cssText = 'position:absolute;height:1px;background:var(--accent-dim);opacity:0.5';
        c.appendChild(conn);
        connections.push(conn);

        setTimeout(function() {
          var lslotRect = lslot.getBoundingClientRect();
          var pslotRect = pslot.getBoundingClientRect();
          var cRect = c.getBoundingClientRect();
          var x1 = lslotRect.right - cRect.left;
          var y1 = lslotRect.top + lslotRect.height/2 - cRect.top;
          var x2 = pslotRect.left - cRect.left;
          var y2 = pslotRect.top + pslotRect.height/2 - cRect.top;
          var dx = x2 - x1, dy = y2 - y1;
          var len = Math.sqrt(dx*dx + dy*dy);
          var angle = Math.atan2(dy, dx) * 180 / Math.PI;
          conn.style.width = len + 'px';
          conn.style.left = x1 + 'px';
          conn.style.top = y1 + 'px';
          conn.style.transform = 'rotate(' + angle + 'deg)';
          conn.style.transformOrigin = '0 0';
        }, 300);
      })(t);
    }

    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;left:8%;bottom:20%;display:flex;align-items:center;gap:12px;font-size:0.45rem;color:var(--text-muted)';
    var logSw = el('div', '');
    logSw.style.cssText = 'width:10px;height:10px;border:1px solid var(--border-active);border-radius:3px';
    legend.appendChild(logSw);
    legend.appendChild(el('span', '', 'Logical (sequential)'));
    var physSw = el('div', '');
    physSw.style.cssText = 'width:10px;height:10px;border:1px solid var(--accent);background:var(--accent-glow);border-radius:3px';
    legend.appendChild(physSw);
    legend.appendChild(el('span', '', 'Physical (scattered)'));
    c.appendChild(legend);

    var stat = el('div', 'sv-stat', numTokens + ' tokens \u2022 ' + numTokens + ' blocks allocated');
    c.appendChild(stat);

    var ctrl = el('div', 'sv-controls');
    var btn = el('button', 'sv-btn', 'Add Token');
    btn.addEventListener('click', function(e) { stopProp(e); addToken(); });
    ctrl.appendChild(btn);
    c.appendChild(ctrl);
  };

  // ═══ 08 ATTENTION ═══
  visualInits.attention = function(c) {
    var configs = {
      'MHA': { q: 8, kv: 8, label: '8 Q \u2192 8 KV (1:1)', cache: '1x' },
      'GQA': { q: 8, kv: 2, label: '8 Q \u2192 2 KV (4:1)', cache: '4x reduction' },
      'MQA': { q: 8, kv: 1, label: '8 Q \u2192 1 KV (8:1)', cache: '8x reduction' }
    };
    var mode = 'GQA';

    var titleDiv = el('div', 'sv-label');
    titleDiv.style.cssText = 'position:absolute;left:50%;top:5%;transform:translateX(-50%)';
    c.appendChild(titleDiv);

    var stat = el('div', 'sv-stat');
    c.appendChild(stat);

    var vizDiv = el('div', '');
    vizDiv.style.cssText = 'position:absolute;left:5%;right:5%;top:15%;bottom:18%';
    c.appendChild(vizDiv);

    function render() {
      var cfg = configs[mode];
      titleDiv.textContent = mode + ': ' + cfg.label;
      stat.textContent = 'KV Cache: ' + cfg.cache;
      vizDiv.innerHTML = '';

      var qLabel = el('div', 'sv-label');
      qLabel.style.cssText = 'position:static;text-align:center;margin-bottom:6px';
      qLabel.textContent = 'QUERY HEADS';
      vizDiv.appendChild(qLabel);

      var qRow = el('div', '');
      qRow.style.cssText = 'display:flex;justify-content:center;gap:6px;margin-bottom:20px';
      for (var i = 0; i < cfg.q; i++) {
        var qHead = el('div', '');
        var kvGroup = Math.floor(i / (cfg.q / cfg.kv));
        var colors = ['var(--accent)', 'var(--logic)', 'var(--io)', 'var(--compute)', 'var(--memory)', 'var(--network)', 'var(--accent-dim)', '#9a98a0'];
        qHead.style.cssText = 'width:24px;height:24px;border-radius:50%;border:2px solid ' + colors[kvGroup % colors.length] + ';display:flex;align-items:center;justify-content:center;font-size:0.45rem;color:var(--text-muted)';
        qHead.textContent = 'Q' + i;
        qRow.appendChild(qHead);
      }
      vizDiv.appendChild(qRow);

      // Connection indicators
      var connDiv = el('div', '');
      connDiv.style.cssText = 'text-align:center;margin-bottom:20px;font-size:0.8rem;color:var(--text-muted)';
      connDiv.textContent = '\u2193'.repeat(cfg.kv);
      vizDiv.appendChild(connDiv);

      var kvLabel = el('div', 'sv-label');
      kvLabel.style.cssText = 'position:static;text-align:center;margin-bottom:6px';
      kvLabel.textContent = 'KEY/VALUE HEADS';
      vizDiv.appendChild(kvLabel);

      var kvRow = el('div', '');
      kvRow.style.cssText = 'display:flex;justify-content:center;gap:6px';
      for (var j = 0; j < cfg.kv; j++) {
        var kvHead = el('div', '');
        kvHead.style.cssText = 'width:28px;height:28px;border-radius:50%;background:' + colors[j % colors.length] + ';display:flex;align-items:center;justify-content:center;font-size:0.45rem;color:var(--bg);font-weight:700';
        kvHead.textContent = 'KV' + j;
        kvRow.appendChild(kvHead);
      }
      vizDiv.appendChild(kvRow);
    }

    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;left:5%;bottom:18%;font-size:0.45rem;color:var(--text-muted)';
    legend.textContent = 'Matching colors = shared KV head group';
    c.appendChild(legend);

    var ctrl = el('div', 'sv-controls');
    ['MHA', 'GQA', 'MQA'].forEach(function(m) {
      var btn = el('button', 'sv-btn' + (m === mode ? ' active' : ''), m);
      btn.addEventListener('click', function(e) {
        stopProp(e);
        mode = m;
        ctrl.querySelectorAll('.sv-btn').forEach(function(b) { b.classList.remove('active'); });
        btn.classList.add('active');
        render();
      });
      ctrl.appendChild(btn);
    });
    c.appendChild(ctrl);
    render();
  };

  // ═══ 09 DECODE ═══
  visualInits.decode = function(c) {
    var steps = [
      { label: 'Load Weights', angle: 0, color: 'var(--compute)', thick: true },
      { label: 'Compute', angle: 90, color: 'var(--accent)', thick: false },
      { label: 'Sample', angle: 180, color: 'var(--logic)', thick: false },
      { label: 'Append KV', angle: 270, color: 'var(--memory)', thick: false }
    ];
    var currentStep = 0;
    var tokenCount = 0;
    var playing = false;
    var timer = null;
    var cx = c.offsetWidth / 2, cy = c.offsetHeight / 2 - 10;
    var r = Math.min(cx, cy) * 0.55;

    // Draw step nodes
    steps.forEach(function(s, i) {
      var rad = (s.angle - 90) * Math.PI / 180;
      var x = cx + r * Math.cos(rad);
      var y = cy + r * Math.sin(rad);
      var node = el('div', 'sv-node');
      node.textContent = s.label;
      node.style.left = (x - 40) + 'px';
      node.style.top = (y - 12) + 'px';
      node.style.width = '80px';
      node.style.textAlign = 'center';
      if (s.thick) {
        node.style.borderWidth = '2px';
        node.style.borderColor = s.color;
      }
      s.el = node;
      c.appendChild(node);
    });

    // Weight loading dominance note
    var note = el('div', '');
    note.style.cssText = 'position:absolute;left:50%;bottom:36%;transform:translateX(-50%);font-size:0.5rem;color:var(--text-muted);text-align:center;width:120px';
    note.textContent = 'Weight loading dominates (~80% of time)';
    c.appendChild(note);

    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;left:5%;bottom:18%;display:flex;flex-wrap:wrap;gap:8px;font-size:0.45rem;color:var(--text-muted)';
    steps.forEach(function(s) {
      var item = el('span', '');
      item.style.cssText = 'display:inline-flex;align-items:center;gap:3px';
      var dot = el('span', '');
      dot.style.cssText = 'width:8px;height:8px;border-radius:50%;border:' + (s.thick ? '2px' : '1px') + ' solid ' + s.color;
      item.appendChild(dot);
      item.appendChild(el('span', '', s.label));
      legend.appendChild(item);
    });
    c.appendChild(legend);

    // Packet
    var packet = el('div', 'sv-packet');
    c.appendChild(packet);

    var stat = el('div', 'sv-stat', 'Tokens: 0');
    c.appendChild(stat);

    function positionPacket(i) {
      var rad = (steps[i].angle - 90) * Math.PI / 180;
      var x = cx + r * Math.cos(rad);
      var y = cy + r * Math.sin(rad);
      packet.style.transition = 'left 0.3s ease, top 0.3s ease';
      packet.style.left = (x - 4) + 'px';
      packet.style.top = (y - 4) + 'px';
    }

    function highlightStep(i) {
      steps.forEach(function(s, j) {
        s.el.classList.toggle('highlight', j === i);
      });
      positionPacket(i);
    }

    function advance() {
      highlightStep(currentStep);
      currentStep = (currentStep + 1) % steps.length;
      if (currentStep === 0) {
        tokenCount++;
        stat.textContent = 'Tokens: ' + tokenCount;
      }
    }

    var ctrl = el('div', 'sv-controls');
    var playBtn = el('button', 'sv-btn active', 'Play');
    playBtn.addEventListener('click', function(e) {
      stopProp(e);
      if (playing) {
        clearInterval(timer);
        playing = false;
        playBtn.textContent = 'Play';
        playBtn.classList.remove('active');
      } else {
        timer = setInterval(advance, 600);
        playing = true;
        playBtn.textContent = 'Pause';
        playBtn.classList.add('active');
      }
    });
    ctrl.appendChild(playBtn);

    var stepBtn = el('button', 'sv-btn', 'Step');
    stepBtn.addEventListener('click', function(e) { stopProp(e); advance(); });
    ctrl.appendChild(stepBtn);
    c.appendChild(ctrl);

    // Auto-play
    highlightStep(0);
    timer = setInterval(advance, 600);
    playing = true;

    // Pause when step closes
    var parentStep = c.closest('.pipeline-step');
    if (parentStep) {
      var closeObs = new MutationObserver(function() {
        if (!parentStep.classList.contains('active') && playing) {
          clearInterval(timer);
          playing = false;
          playBtn.textContent = 'Play';
          playBtn.classList.remove('active');
        }
      });
      closeObs.observe(parentStep, { attributes: true });
    }
  };

  // ═══ 10 SAMPLING ═══
  visualInits.sampling = function(c) {
    var rawProbs = [0.32, 0.18, 0.12, 0.08, 0.06, 0.05, 0.04, 0.03, 0.025, 0.02, 0.015, 0.01, 0.008, 0.006, 0.004];
    var labels = ['the', 'a', 'one', 'my', 'his', 'her', 'its', 'an', 'our', 'this', 'that', 'some', 'your', 'each', 'no'];
    var stage = 0; // 0=raw, 1=temperature, 2=filtered
    var stageNames = ['Raw Logits', 'After Temperature (T=0.7)', 'After Top-K (K=5)'];

    var barContainer = el('div', '');
    barContainer.style.cssText = 'position:absolute;left:5%;right:5%;top:12%;bottom:18%';
    c.appendChild(barContainer);

    var titleDiv = el('div', 'sv-label');
    titleDiv.style.cssText = 'position:absolute;left:5%;top:5%';
    c.appendChild(titleDiv);

    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;right:5%;top:5%;display:flex;align-items:center;gap:10px;font-size:0.45rem;color:var(--text-muted)';
    var actSw = el('div', '');
    actSw.style.cssText = 'width:16px;height:6px;background:var(--accent);border-radius:2px';
    legend.appendChild(actSw);
    legend.appendChild(el('span', '', 'Candidate'));
    var filtSw = el('div', '');
    filtSw.style.cssText = 'width:16px;height:6px;background:var(--border);opacity:0.5;border-radius:2px';
    legend.appendChild(filtSw);
    legend.appendChild(el('span', '', 'Filtered out'));
    c.appendChild(legend);

    var stat = el('div', 'sv-stat');
    c.appendChild(stat);

    function sharpen(probs, temp) {
      var logs = probs.map(function(p) { return Math.log(p) / temp; });
      var maxLog = Math.max.apply(null, logs);
      var exps = logs.map(function(l) { return Math.exp(l - maxLog); });
      var sum = exps.reduce(function(a, b) { return a + b; }, 0);
      return exps.map(function(e) { return e / sum; });
    }

    function render() {
      titleDiv.textContent = stageNames[stage];
      barContainer.innerHTML = '';
      var probs;
      if (stage === 0) probs = rawProbs.slice();
      else if (stage === 1) probs = sharpen(rawProbs, 0.7);
      else {
        probs = sharpen(rawProbs, 0.7);
        // Top-5: zero out rest, renormalize
        var sorted = probs.map(function(p, i) { return { p: p, i: i }; }).sort(function(a, b) { return b.p - a.p; });
        var kept = {};
        for (var k = 0; k < 5; k++) kept[sorted[k].i] = true;
        var sum = 0;
        probs = probs.map(function(p, i) {
          if (kept[i]) { sum += p; return p; }
          return 0;
        });
        probs = probs.map(function(p) { return p / sum; });
      }

      var maxP = Math.max.apply(null, probs);
      probs.forEach(function(p, i) {
        var row = el('div', '');
        row.style.cssText = 'display:flex;align-items:center;gap:6px;margin-bottom:2px';
        var lbl = el('span', '');
        lbl.style.cssText = 'width:30px;text-align:right;font-size:0.55rem;color:var(--text-muted)';
        lbl.textContent = labels[i];
        row.appendChild(lbl);
        var bar = el('div', 'sv-bar');
        bar.style.flex = '1';
        bar.style.height = '11px';
        var fill = el('div', 'sv-bar-fill');
        fill.style.background = p === 0 ? 'var(--border)' : 'var(--accent)';
        fill.style.width = (p / maxP * 100) + '%';
        fill.style.opacity = p === 0 ? '0.3' : '1';
        bar.appendChild(fill);
        row.appendChild(bar);
        var pct = el('span', '');
        pct.style.cssText = 'width:35px;font-size:0.5rem;color:' + (p === 0 ? 'var(--text-muted)' : 'var(--accent)');
        pct.textContent = p === 0 ? '-' : (p * 100).toFixed(1) + '%';
        row.appendChild(pct);
        barContainer.appendChild(row);
      });
      stat.textContent = stage === 2 ? '5 tokens kept, renormalized' : probs.filter(function(p) { return p > 0; }).length + ' candidate tokens';
    }

    var ctrl = el('div', 'sv-controls');
    stageNames.forEach(function(name, i) {
      var short = ['Raw', 'Temp', 'Top-K'][i];
      var btn = el('button', 'sv-btn' + (i === stage ? ' active' : ''), short);
      btn.addEventListener('click', function(e) {
        stopProp(e);
        stage = i;
        ctrl.querySelectorAll('.sv-btn').forEach(function(b) { b.classList.remove('active'); });
        btn.classList.add('active');
        render();
      });
      ctrl.appendChild(btn);
    });

    var sampleBtn = el('button', 'sv-btn', 'Sample');
    sampleBtn.addEventListener('click', function(e) {
      stopProp(e);
      stage = 2;
      ctrl.querySelectorAll('.sv-btn').forEach(function(b) { b.classList.remove('active'); });
      ctrl.children[2].classList.add('active');
      render();
      // Pick random from top 5
      var probs2 = sharpen(rawProbs, 0.7);
      var sorted = probs2.map(function(p, i) { return { p: p, i: i }; }).sort(function(a, b) { return b.p - a.p; });
      var sum = 0;
      for (var k = 0; k < 5; k++) sum += sorted[k].p;
      var r = Math.random() * sum, acc = 0, picked = 0;
      for (var k = 0; k < 5; k++) { acc += sorted[k].p; if (r <= acc) { picked = sorted[k].i; break; } }
      stat.textContent = 'Sampled: "' + labels[picked] + '" (' + (probs2[picked] / sum * 100).toFixed(1) + '%)';
      // Highlight picked bar
      var rows = barContainer.children;
      if (rows[picked]) rows[picked].style.background = 'var(--accent-glow)';
    });
    ctrl.appendChild(sampleBtn);
    c.appendChild(ctrl);
    render();
  };

  // ═══ 11 STREAMING ═══
  visualInits.streaming = function(c) {
    var w = c.offsetWidth, h = c.offsetHeight;
    var tokens = ['Hello', ' world', ',', ' how', ' are', ' you', ' today', '?'];
    var currentToken = 0;
    var playing = false;
    var timer = null;

    // Server and client nodes
    var server = el('div', 'sv-node');
    server.textContent = 'GPU Server';
    server.style.cssText = 'position:absolute;left:8%;top:30%;width:80px;text-align:center;border-color:var(--compute)';
    c.appendChild(server);

    var client = el('div', 'sv-node');
    client.textContent = 'Client';
    client.style.cssText = 'position:absolute;right:8%;top:30%;width:70px;text-align:center;border-color:var(--accent)';
    c.appendChild(client);

    // Connection line
    var connLine = el('div', 'sv-line h');
    connLine.style.cssText = 'position:absolute;left:22%;top:' + (h * 0.3 + 10) + 'px;width:56%;height:1px;background:var(--border)';
    c.appendChild(connLine);

    var sseLabel = el('div', 'sv-label');
    sseLabel.textContent = 'SSE CONNECTION';
    sseLabel.style.cssText = 'position:absolute;left:50%;top:25%;transform:translateX(-50%)';
    c.appendChild(sseLabel);

    // Legend
    var legend = el('div', '');
    legend.style.cssText = 'position:absolute;left:50%;top:8%;transform:translateX(-50%);display:flex;align-items:center;gap:12px;font-size:0.45rem;color:var(--text-muted)';
    var srvSw = el('div', '');
    srvSw.style.cssText = 'width:10px;height:10px;border:1px solid var(--compute);border-radius:2px';
    legend.appendChild(srvSw);
    legend.appendChild(el('span', '', 'GPU Server'));
    var cliSw = el('div', '');
    cliSw.style.cssText = 'width:10px;height:10px;border:1px solid var(--accent);border-radius:2px';
    legend.appendChild(cliSw);
    legend.appendChild(el('span', '', 'Client'));
    c.appendChild(legend);

    // TTFT marker
    var ttftMarker = el('div', '');
    ttftMarker.style.cssText = 'position:absolute;left:50%;top:42%;transform:translateX(-50%);font-size:0.5rem;color:var(--compute);opacity:0;transition:opacity 0.3s';
    ttftMarker.textContent = '\u2190 TTFT \u2192';
    c.appendChild(ttftMarker);

    // Accumulated text
    var textDiv = el('div', '');
    textDiv.style.cssText = 'position:absolute;left:10%;right:10%;bottom:18%;background:rgba(255,255,255,0.02);border:1px solid var(--border);border-radius:6px;padding:8px 12px;font-family:var(--font-body);font-size:0.85rem;color:var(--text);min-height:30px';
    c.appendChild(textDiv);

    var textLabel = el('div', 'sv-label');
    textLabel.textContent = 'ACCUMULATED RESPONSE';
    textLabel.style.cssText = 'position:absolute;left:10%;bottom:' + (h * 0.18 + 38) + 'px';
    c.appendChild(textLabel);

    // Packet
    var packet = el('div', 'sv-packet');
    packet.style.opacity = '0';
    c.appendChild(packet);

    var stat = el('div', 'sv-stat', 'Ready');
    c.appendChild(stat);

    function sendToken() {
      if (currentToken >= tokens.length) {
        stat.textContent = 'data: [DONE]';
        playing = false;
        clearInterval(timer);
        var playBtn = c.querySelector('.sv-btn');
        if (playBtn) { playBtn.textContent = 'Replay'; playBtn.classList.remove('active'); }
        return;
      }

      if (currentToken === 0) ttftMarker.style.opacity = '1';
      else ttftMarker.style.opacity = '0';

      // Animate packet from server to client
      packet.style.transition = 'none';
      packet.style.left = '22%';
      packet.style.top = (h * 0.3 + 6) + 'px';
      packet.style.opacity = '1';
      setTimeout(function() {
        packet.style.transition = 'left 0.25s ease';
        packet.style.left = '75%';
      }, 50);

      setTimeout(function() {
        packet.style.opacity = '0';
        textDiv.textContent += tokens[currentToken];
        stat.textContent = 'data: {"delta":"' + tokens[currentToken].trim() + '"}';
        currentToken++;
      }, 300);
    }

    function startStream() {
      currentToken = 0;
      textDiv.textContent = '';
      stat.textContent = 'Streaming...';
      playing = true;
      timer = setInterval(sendToken, 500);
      var playBtn = c.querySelector('.sv-btn');
      if (playBtn) { playBtn.textContent = 'Pause'; playBtn.classList.add('active'); }
    }

    var ctrl = el('div', 'sv-controls');
    var playBtn = el('button', 'sv-btn', 'Play');
    playBtn.addEventListener('click', function(e) {
      stopProp(e);
      if (playing) {
        clearInterval(timer);
        playing = false;
        playBtn.textContent = 'Play';
        playBtn.classList.remove('active');
      } else if (currentToken >= tokens.length) {
        startStream();
      } else {
        timer = setInterval(sendToken, 500);
        playing = true;
        playBtn.textContent = 'Pause';
        playBtn.classList.add('active');
      }
    });
    ctrl.appendChild(playBtn);
    c.appendChild(ctrl);

    // Auto-play
    setTimeout(startStream, 400);

    // Pause when step closes
    var parentStep = c.closest('.pipeline-step');
    if (parentStep) {
      var closeObs = new MutationObserver(function() {
        if (!parentStep.classList.contains('active') && playing) {
          clearInterval(timer);
          playing = false;
          playBtn.textContent = 'Play';
          playBtn.classList.remove('active');
        }
      });
      closeObs.observe(parentStep, { attributes: true });
    }
  };
})();
</script>


<script>
// ─── SEARCH ───
var CURRENT_PAGE = 'index.html';
var SEARCH_INDEX = [
  {page:'index.html',pageLabel:'Inference',anchor:'step-routing',title:'Request Routing',desc:'Load balancing, KV cache-aware routing, geo-aware, gateway frameworks'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-preprocessing',title:'Preprocessing',desc:'Prompt templates, RAG retrieval, rate limiting, input validation'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-tokenization',title:'Tokenization',desc:'BPE, SentencePiece, vocabulary sizes, tiktoken'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-embedding',title:'Embedding & Position',desc:'Token embeddings, RoPE, ALiBi, positional encoding'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-scheduling',title:'Scheduling & Batching',desc:'Continuous batching, chunked prefill, thinking model scheduling'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-prefill',title:'Prefill Phase',desc:'Parallel forward pass, TTFT, prefix caching, KV-Runahead'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-kvcache',title:'KV Cache & Memory',desc:'PagedAttention, KV compression, MLA, memory management'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-attention',title:'Attention Mechanisms',desc:'MHA, GQA, MQA, FlashAttention, MLA'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-decode',title:'Decode Phase',desc:'Autoregressive loop, speculative decoding, thinking model decode'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-sampling',title:'Sampling & Selection',desc:'Temperature, top-k, top-p, min-p, repetition penalties'},
  {page:'index.html',pageLabel:'Inference',anchor:'step-streaming',title:'Detokenization & Streaming',desc:'UTF-8 buffering, SSE protocol, streaming pipeline'},
  {page:'index.html',pageLabel:'Inference',anchor:'inference-metrics',title:'Inference Metrics',desc:'TTFT, TPOT, throughput, GPU utilization, SLOs, cost per token'},
  {page:'index.html',pageLabel:'Inference',anchor:'cross-cutting',title:'Quantization & Parallelism',desc:'FP8, AWQ, GPTQ, tensor/pipeline/expert parallelism, LoRA serving'},
  {page:'index.html',pageLabel:'Inference',anchor:'frameworks',title:'Serving Frameworks',desc:'vLLM, SGLang, TensorRT-LLM, NVIDIA Dynamo'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'cost-stack',title:'The Cost Stack',desc:'GPU CapEx, power, data center, networking, operations'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'throughput',title:'Throughput as Margin',desc:'3-5x throughput variance; thinking model throughput impact'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'pricing',title:'Pricing Structures',desc:'Per-token, per-GPU-hour, reserved, batch, thinking token pricing'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'managed-vs-rental',title:'Managed vs GPU Rental',desc:'Statistical multiplexing, outcomes vs infrastructure'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'buy-vs-rent',title:'Buy vs Rent GPUs',desc:'Ownership cost model, breakeven calculator'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'data-centers',title:'Data Centers',desc:'Power costs, PUE, facility economics'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'equity-vs-debt',title:'Equity vs Debt',desc:'Risk, information asymmetry, tax shields'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'contracted-revenue',title:'Contracted Revenue',desc:'How pricing decisions unlock debt capacity'},
  {page:'economics.html',pageLabel:'Inference Economics',anchor:'stage-by-stage',title:'Stage-by-Stage Capital',desc:'Series A to public capital structure evolution'},
  {page:'training.html',pageLabel:'Training',anchor:'data-pipeline',title:'Data Pipeline',desc:'Collection, filtering, dedup, mixing; DCLM, FineWeb'},
  {page:'training.html',pageLabel:'Training',anchor:'tokenizer',title:'Tokenizer Training',desc:'BPE, SentencePiece, vocabulary sizes, 128K tokens'},
  {page:'training.html',pageLabel:'Training',anchor:'architecture',title:'Model Architecture',desc:'Attention variants, SwiGLU, MoE, RoPE, RMSNorm'},
  {page:'training.html',pageLabel:'Training',anchor:'optimization',title:'Optimization',desc:'AdamW, Muon, WSD schedule, mixed precision, FP8'},
  {page:'training.html',pageLabel:'Training',anchor:'distributed',title:'Distributed Training',desc:'ZeRO, tensor/pipeline/4D parallelism, Megatron-LM'},
  {page:'training.html',pageLabel:'Training',anchor:'monitoring',title:'Monitoring & Recovery',desc:'MFU, loss spikes, checkpointing, failure recovery'},
  {page:'training.html',pageLabel:'Training',anchor:'sft',title:'Supervised Fine-Tuning',desc:'LoRA, supervised text/chat/vision, loss masking, instruction tuning'},
  {page:'training.html',pageLabel:'Training',anchor:'alignment',title:'Alignment',desc:'RLHF, DPO triplets, GRPO, Constitutional AI, KTO, thinking models'},
  {page:'training.html',pageLabel:'Training',anchor:'rft',title:'Reinforcement Fine-Tuning',desc:'Evaluator-driven RFT, environment hooks, parameter tuning, o1/o3/R1'},
  {page:'training.html',pageLabel:'Training',anchor:'evaluation',title:'Evaluation',desc:'MMLU, benchmarks, contamination detection, LiveBench'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'hardware-costs',title:'Hardware & Compute Costs',desc:'GPU pricing, cloud rental decline, cost benchmarks'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'scaling-laws',title:'Scaling Laws & Efficiency',desc:'Chinchilla, over-training, test-time compute scaling'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'failure-costs',title:'Failure & Wasted Compute',desc:'Training failures, wasted GPU-hours, checkpoint overhead'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'build-vs-buy',title:'Build vs Fine-Tune vs API',desc:'When to train, fine-tune, or use API — cost comparison'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'training-providers',title:'Training Providers',desc:'Foundation model companies, training-as-a-service'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'cloud-vs-onprem',title:'Cloud vs On-Premise',desc:'Cloud advantages, on-prem breakeven, hidden costs'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'gpu-financing',title:'GPU Financing',desc:'CoreWeave debt, sale-leaseback, GPU depreciation'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'model-funding',title:'Foundation Model Funding',desc:'Mega-rounds, Big Tech infrastructure spend'},
  {page:'training-economics.html',pageLabel:'Training Economics',anchor:'training-vs-inference',title:'Training vs Inference Spend',desc:'Spend ratio shift 2023–2026, revenue gap'},
];

var searchSelectedIdx = -1;
var searchSelectedEl = null;
var searchFlatResults = [];
var searchDebounceTimer = null;
var searchCrossPageRequested = false;
var SEARCH_PAGE_ORDER = ['Inference', 'Inference Economics', 'Training', 'Training Economics'];

function normalizeSearchText(text) {
  return (text || '')
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, ' ')
    .replace(/\s+/g, ' ')
    .trim();
}

function escapeHtml(text) {
  return (text || '')
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&#39;');
}

function tokenizeQuery(queryNorm) {
  if (!queryNorm) return [];
  return queryNorm.split(' ').filter(function(token) { return token.length > 1; });
}

function resolveAnchorTarget(root, anchor) {
  if (!root || !anchor) return null;
  var target = root.getElementById(anchor);
  if (target) return target;
  var stepKey = anchor.replace(/^step-/, '');
  target = root.querySelector('.pipeline-step[data-step="' + stepKey + '"]');
  if (target) return target;
  return root.querySelector('[data-step="' + stepKey + '"]');
}

function extractSectionText(root, anchor) {
  var target = resolveAnchorTarget(root, anchor);
  if (!target) return '';
  var text = (target.textContent || '').replace(/\s+/g, ' ').trim();
  if (text.length > 5000) text = text.slice(0, 5000);
  return text;
}

function rebuildSearchBlob(item) {
  var base = [item.title, item.desc, item.pageLabel, item.anchor.replace(/[-_]/g, ' ')].join(' ');
  item._titleNorm = normalizeSearchText(item.title);
  item._descNorm = normalizeSearchText(item.desc);
  item._contentNorm = normalizeSearchText(item._content || '');
  item._blobNorm = normalizeSearchText(base + ' ' + (item._content || ''));
}

function prepareSearchIndex() {
  SEARCH_INDEX.forEach(function(item) {
    if (item.page === CURRENT_PAGE) {
      item._content = extractSectionText(document, item.anchor);
    } else {
      item._content = item._content || '';
    }
    rebuildSearchBlob(item);
  });
}

function fuzzySubsequenceScore(query, text) {
  if (!query || !text) return 0;
  var qi = 0;
  var ti = 0;
  var start = -1;
  var last = -1;
  var gapPenalty = 0;

  while (qi < query.length && ti < text.length) {
    if (query.charAt(qi) === text.charAt(ti)) {
      if (start === -1) start = ti;
      if (last !== -1 && ti > last + 1) gapPenalty += (ti - last - 1);
      last = ti;
      qi++;
    }
    ti++;
  }

  if (qi !== query.length || start === -1 || last === -1) return 0;

  var span = last - start + 1;
  var compactness = query.length / Math.max(span, 1);
  var startBonus = start === 0 ? 0.2 : Math.max(0, 0.12 - start * 0.003);
  var penalty = Math.min(0.45, gapPenalty * 0.01);
  var score = compactness + startBonus - penalty;
  return Math.max(0, Math.min(1, score));
}

function scoreSearchItem(item, queryNorm, tokens) {
  if (!queryNorm) return 1;

  var titleNorm = item._titleNorm || '';
  var descNorm = item._descNorm || '';
  var blobNorm = item._blobNorm || '';

  var score = 0;
  var matched = false;

  if (titleNorm.indexOf(queryNorm) === 0) {
    score += 140;
    matched = true;
  } else if (titleNorm.indexOf(queryNorm) !== -1) {
    score += 110;
    matched = true;
  }

  if (descNorm.indexOf(queryNorm) !== -1) {
    score += 48;
    matched = true;
  }

  if (blobNorm.indexOf(queryNorm) !== -1) {
    score += 78;
    matched = true;
  }

  var tokenHits = 0;
  tokens.forEach(function(token) {
    if (titleNorm.indexOf(token) !== -1) {
      score += 34;
      tokenHits += 1;
      matched = true;
      return;
    }
    if (blobNorm.indexOf(token) !== -1) {
      score += 18;
      tokenHits += 1;
      matched = true;
      return;
    }

    var fuzzyTitle = fuzzySubsequenceScore(token, titleNorm);
    var fuzzyBlob = fuzzySubsequenceScore(token, blobNorm);
    var fuzzyToken = Math.max(fuzzyTitle, fuzzyBlob * 0.9);
    if (fuzzyToken >= 0.74) {
      score += fuzzyToken * 16;
      tokenHits += 0.6;
      matched = true;
    }
  });

  if (tokens.length) score += (tokenHits / tokens.length) * 20;

  var fuzzyWhole = Math.max(
    fuzzySubsequenceScore(queryNorm, titleNorm),
    fuzzySubsequenceScore(queryNorm, blobNorm) * 0.88
  );
  if (fuzzyWhole >= 0.76) {
    score += fuzzyWhole * 34;
    matched = true;
  }

  if (!matched || score < 22) return 0;
  if (item.page === CURRENT_PAGE) score += 6;
  return score;
}

function buildSearchSnippet(item, queryNorm, tokens) {
  var fallback = item.desc || '';
  var content = (item._content || '').replace(/\s+/g, ' ').trim();
  if (!content) return fallback;

  var lower = content.toLowerCase();
  var hit = -1;

  for (var i = 0; i < tokens.length; i++) {
    if (tokens[i].length < 3) continue;
    hit = lower.indexOf(tokens[i]);
    if (hit !== -1) break;
  }

  if (hit === -1 && queryNorm.length >= 3) {
    hit = lower.indexOf(queryNorm);
  }

  if (hit === -1) {
    return content.length > 160 ? content.slice(0, 160).trim() + '…' : content;
  }

  var start = Math.max(0, hit - 64);
  var end = Math.min(content.length, hit + 120);
  var prefix = start > 0 ? '…' : '';
  var suffix = end < content.length ? '…' : '';
  return prefix + content.slice(start, end).trim() + suffix;
}

function getSearchResults(query) {
  var queryNorm = normalizeSearchText(query);
  var tokens = tokenizeQuery(queryNorm);

  if (!queryNorm) {
    return SEARCH_INDEX.map(function(item) {
      return {
        item: item,
        score: item.page === CURRENT_PAGE ? 1 : 0,
        snippet: item.desc
      };
    }).sort(function(a, b) {
      if (b.score !== a.score) return b.score - a.score;
      var pageDelta = SEARCH_PAGE_ORDER.indexOf(a.item.pageLabel) - SEARCH_PAGE_ORDER.indexOf(b.item.pageLabel);
      if (pageDelta !== 0) return pageDelta;
      return a.item.title.localeCompare(b.item.title);
    });
  }

  var matches = [];
  SEARCH_INDEX.forEach(function(item) {
    var score = scoreSearchItem(item, queryNorm, tokens);
    if (score <= 0) return;
    matches.push({
      item: item,
      score: score,
      snippet: buildSearchSnippet(item, queryNorm, tokens)
    });
  });

  matches.sort(function(a, b) {
    if (b.score !== a.score) return b.score - a.score;
    var pageDelta = SEARCH_PAGE_ORDER.indexOf(a.item.pageLabel) - SEARCH_PAGE_ORDER.indexOf(b.item.pageLabel);
    if (pageDelta !== 0) return pageDelta;
    return a.item.title.localeCompare(b.item.title);
  });

  return matches.slice(0, 120);
}

function openSearch() {
  var overlay = document.getElementById('search-overlay');
  var input = document.getElementById('search-input');
  if (!overlay || !input) return;
  overlay.classList.add('open');
  input.value = '';
  input.focus();
  searchSelectedIdx = -1;
  searchSelectedEl = null;
  renderSearchResults('');
  warmCrossPageContent();
}

function closeSearch() {
  var overlay = document.getElementById('search-overlay');
  if (overlay) overlay.classList.remove('open');
}

function setSearchSelected(idx) {
  if (idx < 0 || idx >= searchFlatResults.length) return;
  if (searchSelectedIdx === idx) return;

  if (searchSelectedEl) searchSelectedEl.classList.remove('selected');

  searchSelectedIdx = idx;
  var container = document.getElementById('search-results');
  searchSelectedEl = container ? container.querySelector('.search-result[data-idx="' + idx + '"]') : null;
  if (searchSelectedEl) searchSelectedEl.classList.add('selected');
}

function renderSearchResults(query) {
  var container = document.getElementById('search-results');
  if (!container) return;

  var results = getSearchResults(query || '');
  searchFlatResults = results;
  searchSelectedIdx = -1;
  searchSelectedEl = null;

  if (!results.length) {
    container.innerHTML = '<div class="search-no-results">No results for &ldquo;' + escapeHtml(query || '') + '&rdquo;</div>';
    return;
  }

  var grouped = {};
  results.forEach(function(entry, idx) {
    var label = entry.item.pageLabel;
    if (!grouped[label]) grouped[label] = [];
    grouped[label].push({ entry: entry, idx: idx });
  });

  var labels = SEARCH_PAGE_ORDER.slice();
  Object.keys(grouped).forEach(function(label) {
    if (labels.indexOf(label) === -1) labels.push(label);
  });

  var html = '';
  labels.forEach(function(label) {
    if (!grouped[label] || !grouped[label].length) return;
    html += '<div class="search-group-header">' + escapeHtml(label) + '</div>';

    grouped[label].forEach(function(row) {
      var item = row.entry.item;
      var snippet = row.entry.snippet || item.desc || '';
      html += '<div class="search-result" data-idx="' + row.idx + '" data-page="' + item.page + '" data-anchor="' + item.anchor + '">' +
        '<div class="search-result-dot"></div>' +
        '<div class="search-result-body"><div class="search-result-title">' + escapeHtml(item.title) + '</div>' +
        '<div class="search-result-desc">' + escapeHtml(snippet) + '</div></div>' +
        '<span class="search-result-badge">' + (item.page === CURRENT_PAGE ? 'this page' : item.pageLabel.toLowerCase()) + '</span>' +
      '</div>';
    });
  });

  container.innerHTML = html;
}

function scheduleSearchRender(query) {
  if (searchDebounceTimer) clearTimeout(searchDebounceTimer);
  searchDebounceTimer = setTimeout(function() {
    renderSearchResults(query);
  }, 70);
}

function navigateToResult(el) {
  var page = el.getAttribute('data-page');
  var anchor = el.getAttribute('data-anchor');
  if (!page || !anchor) return;

  closeSearch();

  if (page === CURRENT_PAGE) {
    var target = resolveAnchorTarget(document, anchor);
    if (target) {
      if (target.classList.contains('pipeline-step') && !target.classList.contains('active')) {
        target.classList.add('active');
      }
      window.scrollTo({ top: target.getBoundingClientRect().top + window.scrollY - 64, behavior: 'smooth' });
    }
  } else {
    window.location.href = page + '#' + anchor;
  }
}

function warmCrossPageContent() {
  if (searchCrossPageRequested || typeof fetch !== 'function' || typeof DOMParser === 'undefined') return;
  searchCrossPageRequested = true;

  var pages = [];
  var seen = {};
  SEARCH_INDEX.forEach(function(item) {
    if (item.page === CURRENT_PAGE || seen[item.page]) return;
    seen[item.page] = true;
    pages.push(item.page);
  });

  if (!pages.length) {
    return;
  }

  Promise.allSettled(pages.map(function(page) {
    return fetch(page, { cache: 'force-cache', credentials: 'same-origin' })
      .then(function(response) {
        if (!response.ok) return '';
        return response.text();
      })
      .then(function(html) {
        if (!html) return;
        var parser = new DOMParser();
        var doc = parser.parseFromString(html, 'text/html');
        SEARCH_INDEX.forEach(function(item) {
          if (item.page !== page) return;
          var extracted = extractSectionText(doc, item.anchor);
          if (extracted) {
            item._content = extracted;
            rebuildSearchBlob(item);
          }
        });
      })
      .catch(function() {});
  })).then(function() {
    var overlay = document.getElementById('search-overlay');
    var input = document.getElementById('search-input');
    if (overlay && overlay.classList.contains('open') && input && input.value.trim()) {
      renderSearchResults(input.value);
    }
  });
}

prepareSearchIndex();

document.addEventListener('keydown', function(e) {
  if ((e.metaKey || e.ctrlKey) && e.key.toLowerCase() === 'k') {
    e.preventDefault();
    var overlay = document.getElementById('search-overlay');
    if (!overlay) return;
    if (overlay.classList.contains('open')) { closeSearch(); } else { openSearch(); }
    return;
  }

  var overlay = document.getElementById('search-overlay');
  if (!overlay || !overlay.classList.contains('open')) return;

  if (e.key === 'Escape') {
    closeSearch();
    return;
  }

  if (!searchFlatResults.length) return;

  if (e.key === 'ArrowDown') {
    e.preventDefault();
    var next = searchSelectedIdx < 0 ? 0 : Math.min(searchSelectedIdx + 1, searchFlatResults.length - 1);
    setSearchSelected(next);
    if (searchSelectedEl) searchSelectedEl.scrollIntoView({ block: 'nearest' });
  } else if (e.key === 'ArrowUp') {
    e.preventDefault();
    var prev = searchSelectedIdx <= 0 ? 0 : searchSelectedIdx - 1;
    setSearchSelected(prev);
    if (searchSelectedEl) searchSelectedEl.scrollIntoView({ block: 'nearest' });
  } else if (e.key === 'Enter') {
    e.preventDefault();
    var idx = searchSelectedIdx >= 0 ? searchSelectedIdx : 0;
    var container = document.getElementById('search-results');
    var target = container ? container.querySelector('.search-result[data-idx="' + idx + '"]') : null;
    if (target) navigateToResult(target);
  }
});

var searchInputEl = document.getElementById('search-input');
if (searchInputEl) {
  searchInputEl.addEventListener('input', function(e) {
    scheduleSearchRender(e.target.value);
  });
}

var searchResultsEl = document.getElementById('search-results');
if (searchResultsEl) {
  searchResultsEl.addEventListener('click', function(e) {
    var resultEl = e.target.closest('.search-result');
    if (!resultEl || !searchResultsEl.contains(resultEl)) return;
    navigateToResult(resultEl);
  });

  searchResultsEl.addEventListener('mouseover', function(e) {
    var resultEl = e.target.closest('.search-result');
    if (!resultEl || !searchResultsEl.contains(resultEl)) return;
    var idx = parseInt(resultEl.getAttribute('data-idx'), 10);
    if (isNaN(idx)) return;
    setSearchSelected(idx);
  });
}

var searchOverlayEl = document.getElementById('search-overlay');
if (searchOverlayEl) {
  searchOverlayEl.addEventListener('click', function(e) {
    if (e.target === searchOverlayEl) closeSearch();
  });
}

var searchTriggerEl = document.getElementById('search-trigger');
if (searchTriggerEl) {
  searchTriggerEl.addEventListener('click', function() { openSearch(); });
}

// Update shortcut label for non-Mac
(function() {
  var isMac = /Mac|iPhone|iPad|iPod/.test(navigator.platform || navigator.userAgent);
  if (!isMac) {
    var hint = document.getElementById('search-key-hint');
    if (hint) hint.textContent = 'Ctrl+K';
  }
})();

// Handle hash anchor from cross-page search navigation
(function() {
  if (!window.location.hash) return;
  var anchor = window.location.hash.slice(1);
  setTimeout(function() {
    var target = resolveAnchorTarget(document, anchor);
    if (!target) return;
    if (target.classList.contains('pipeline-step') && !target.classList.contains('active')) target.classList.add('active');
    window.scrollTo({ top: target.getBoundingClientRect().top + window.scrollY - 64, behavior: 'smooth' });
  }, 180);
})();

</script>

<script>
// ─── KNOWLEDGE MARKDOWN DOWNLOAD ───
(function() {
  var downloadBtn = document.getElementById('download-knowledge');
  if (!downloadBtn) return;

  var KNOWLEDGE_PAGES = [
    { path: 'index.html', label: 'Inference Technical Pipeline' },
    { path: 'economics.html', label: 'Inference Economics' },
    { path: 'training.html', label: 'Training Technical Pipeline' },
    { path: 'training-economics.html', label: 'Training Economics' }
  ];

  function normalizeWhitespace(text) {
    return (text || '').replace(/\s+/g, ' ').trim();
  }

  function resolveHref(href, baseUrl) {
    if (!href) return '';
    if (href.charAt(0) === '#') return baseUrl + href;
    try {
      return new URL(href, baseUrl).href;
    } catch (err) {
      return href;
    }
  }

  function inlineFromNode(node, baseUrl) {
    if (!node) return '';
    if (node.nodeType === Node.TEXT_NODE) return node.nodeValue || '';
    if (node.nodeType !== Node.ELEMENT_NODE) return '';

    var tag = node.tagName.toLowerCase();
    if (tag === 'br') return '  \n';

    var childText = '';
    node.childNodes.forEach(function(child) {
      childText += inlineFromNode(child, baseUrl);
    });

    childText = normalizeWhitespace(childText);

    if (!childText && tag !== 'a') return '';
    if (tag === 'strong' || tag === 'b') return '**' + childText + '**';
    if (tag === 'em' || tag === 'i') return '*' + childText + '*';
    if (tag === 'code') return '`' + childText.replace(/`/g, '\\`') + '`';
    if (tag === 'a') {
      var href = resolveHref(node.getAttribute('href') || '', baseUrl);
      var label = childText || href;
      return href ? '[' + label + '](' + href + ')' : label;
    }

    return childText;
  }

  function inlineText(element, baseUrl) {
    if (!element) return '';
    var text = '';
    element.childNodes.forEach(function(child) {
      text += inlineFromNode(child, baseUrl) + ' ';
    });
    return normalizeWhitespace(text);
  }

  function markdownFromList(listEl, baseUrl, depth) {
    var level = depth || 0;
    var isOrdered = listEl.tagName.toLowerCase() === 'ol';
    var lines = [];

    Array.from(listEl.children).forEach(function(li, idx) {
      if (!li || li.tagName.toLowerCase() !== 'li') return;

      var itemTextParts = [];
      li.childNodes.forEach(function(node) {
        if (node.nodeType === Node.ELEMENT_NODE) {
          var tag = node.tagName.toLowerCase();
          if (tag === 'ul' || tag === 'ol') return;
        }
        itemTextParts.push(inlineFromNode(node, baseUrl));
      });

      var itemText = normalizeWhitespace(itemTextParts.join(' '));
      if (!itemText) itemText = '(item)';
      var prefix = isOrdered ? ((idx + 1) + '.') : '-';
      lines.push(Array(level + 1).join('  ') + prefix + ' ' + itemText);

      Array.from(li.children).forEach(function(child) {
        var childTag = child.tagName.toLowerCase();
        if (childTag === 'ul' || childTag === 'ol') {
          var nested = markdownFromList(child, baseUrl, level + 1);
          if (nested) lines.push(nested);
        }
      });
    });

    return lines.join('\n');
  }

  function escapeTableCell(text) {
    return (text || '').replace(/\|/g, '\\|');
  }

  function markdownFromTable(tableEl, baseUrl) {
    var rows = Array.from(tableEl.querySelectorAll('tr')).map(function(tr) {
      return Array.from(tr.children)
        .filter(function(cell) {
          var tag = cell.tagName.toLowerCase();
          return tag === 'th' || tag === 'td';
        })
        .map(function(cell) {
          return escapeTableCell(inlineText(cell, baseUrl));
        });
    }).filter(function(row) {
      return row.length > 0;
    });

    if (!rows.length) return '';

    var colCount = rows.reduce(function(max, row) {
      return Math.max(max, row.length);
    }, 0);

    rows = rows.map(function(row) {
      var copy = row.slice(0);
      while (copy.length < colCount) copy.push('');
      return copy;
    });

    var header = rows[0];
    var body = rows.slice(1);
    var sep = [];
    for (var i = 0; i < colCount; i++) sep.push('---');

    var md = [];
    md.push('| ' + header.join(' | ') + ' |');
    md.push('| ' + sep.join(' | ') + ' |');
    body.forEach(function(row) {
      md.push('| ' + row.join(' | ') + ' |');
    });

    return md.join('\n');
  }

  function shouldSkipBlock(el) {
    if (!el) return true;
    if (el.closest('nav, .minimap, .search-overlay, .footer, .journey-detail-panel, .term-tooltip')) return true;
    return false;
  }

  function sectionToMarkdown(section, baseUrl, headingToSkip) {
    var blocks = section.querySelectorAll('h1,h2,h3,h4,h5,p,ul,ol,pre,table');
    var out = [];
    var previous = '';

    blocks.forEach(function(el) {
      if (el === headingToSkip) return;
      if (shouldSkipBlock(el)) return;

      var tag = el.tagName.toLowerCase();
      var chunk = '';

      if ((tag === 'p' && el.closest('li')) || (tag === 'p' && el.closest('table'))) return;
      if ((tag === 'ul' || tag === 'ol') && el.closest('li')) return;
      if ((tag === 'pre' || tag === 'code') && el.closest('table')) return;

      if (tag === 'h1' || tag === 'h2' || tag === 'h3' || tag === 'h4' || tag === 'h5') {
        var headingText = inlineText(el, baseUrl);
        if (!headingText) return;
        var level = parseInt(tag.charAt(1), 10) + 2;
        if (level > 6) level = 6;
        chunk = Array(level + 1).join('#') + ' ' + headingText;
      } else if (tag === 'p') {
        chunk = inlineText(el, baseUrl);
      } else if (tag === 'ul' || tag === 'ol') {
        chunk = markdownFromList(el, baseUrl, 0);
      } else if (tag === 'pre') {
        var codeText = (el.textContent || '').trim();
        if (codeText) chunk = '```\n' + codeText + '\n```';
      } else if (tag === 'table') {
        chunk = markdownFromTable(el, baseUrl);
      }

      chunk = (chunk || '').replace(/\n\s+\n/g, '\n\n').replace(/\n{3,}/g, '\n\n').trim();
      if (!chunk || chunk === previous) return;
      out.push(chunk);
      previous = chunk;
    });

    return out.join('\n\n').trim();
  }

  function parsePageToMarkdown(html, pageMeta, pageUrl) {
    var parser = new DOMParser();
    var doc = parser.parseFromString(html, 'text/html');
    var lines = [];

    lines.push('## ' + pageMeta.label);
    lines.push('Source: [' + pageMeta.path + '](' + pageUrl + ')');

    var heroTitleEl = doc.querySelector('.hero h1');
    var heroTitle = heroTitleEl ? inlineText(heroTitleEl, pageUrl) : normalizeWhitespace(doc.title || pageMeta.label);
    if (heroTitle) lines.push('### ' + heroTitle);

    var heroSubEl = doc.querySelector('.hero-sub') || doc.querySelector('.hero p');
    var heroSub = heroSubEl ? inlineText(heroSubEl, pageUrl) : '';
    if (heroSub) lines.push(heroSub);

    var sections = Array.from(doc.querySelectorAll('section'));
    sections.forEach(function(section) {
      if (section.classList.contains('hero')) return;
      if (shouldSkipBlock(section)) return;

      var sectionHeadingEl = section.querySelector('h2,h3');
      var sectionHeading = sectionHeadingEl ? inlineText(sectionHeadingEl, pageUrl) : '';
      if (!sectionHeading && section.id) sectionHeading = section.id.replace(/[-_]/g, ' ');
      if (sectionHeading) lines.push('### ' + sectionHeading);

      var sectionBody = sectionToMarkdown(section, pageUrl, sectionHeadingEl);
      if (sectionBody) lines.push(sectionBody);
    });

    return lines.join('\n\n').replace(/\n{3,}/g, '\n\n').trim();
  }

  async function buildKnowledgeMarkdown() {
    var generatedAt = new Date();
    var parts = [];

    parts.push('# LLM Anatomy Knowledge Base');
    parts.push('');
    parts.push('Generated: ' + generatedAt.toISOString());
    parts.push('Origin: ' + window.location.origin);
    parts.push('');
    parts.push('This markdown file is generated from the live website at download time.');
    parts.push('');

    for (var i = 0; i < KNOWLEDGE_PAGES.length; i++) {
      var page = KNOWLEDGE_PAGES[i];
      var pageUrl = new URL(page.path, window.location.href).href;

      try {
        var response = await fetch(pageUrl, { cache: 'no-store', credentials: 'same-origin' });
        if (!response.ok) throw new Error('HTTP ' + response.status);

        var html = await response.text();
        var pageMd = parsePageToMarkdown(html, page, pageUrl);
        parts.push(pageMd || ('## ' + page.label + '\n\nSource: [' + page.path + '](' + pageUrl + ')\n\n_No extractable content found._'));
      } catch (err) {
        parts.push('## ' + page.label);
        parts.push('Source: [' + page.path + '](' + pageUrl + ')');
        parts.push('');
        parts.push('_Failed to fetch this page while generating knowledge markdown: ' + (err && err.message ? err.message : 'Unknown error') + '._');
      }

      if (i < KNOWLEDGE_PAGES.length - 1) {
        parts.push('');
        parts.push('---');
        parts.push('');
      }
    }

    return parts.join('\n').replace(/\n{3,}/g, '\n\n').trim() + '\n';
  }

  function downloadMarkdown(content) {
    var stamp = new Date().toISOString().replace(/:/g, '-').replace(/\.\d+Z$/, 'Z');
    var fileName = 'llm-anatomy-knowledge-' + stamp + '.md';

    var blob = new Blob([content], { type: 'text/markdown;charset=utf-8' });
    var url = URL.createObjectURL(blob);
    var a = document.createElement('a');
    a.href = url;
    a.download = fileName;
    document.body.appendChild(a);
    a.click();
    a.remove();
    URL.revokeObjectURL(url);
  }

  downloadBtn.addEventListener('click', async function() {
    if (downloadBtn.disabled) return;

    var labelEl = downloadBtn.querySelector('.search-trigger-label');
    var originalLabel = labelEl ? labelEl.textContent : '';

    downloadBtn.disabled = true;
    if (labelEl) labelEl.textContent = 'Preparing...';

    try {
      var markdown = await buildKnowledgeMarkdown();
      downloadMarkdown(markdown);
    } catch (err) {
      console.error('Knowledge markdown generation failed:', err);
      window.alert('Failed to generate knowledge markdown. Please try again.');
    } finally {
      downloadBtn.disabled = false;
      if (labelEl) labelEl.textContent = originalLabel;
    }
  });
})();
</script>

<!-- ─── SEARCH MODAL ─── -->
<div class="search-overlay" id="search-overlay" role="dialog" aria-modal="true" aria-label="Site search">
  <div class="search-modal">
    <div class="search-input-wrap">
      <svg class="search-icon-svg" viewBox="0 0 16 16" stroke="currentColor" stroke-width="1.5"><circle cx="6.5" cy="6.5" r="4.5"/><path stroke-linecap="round" d="m10 10 3.5 3.5"/></svg>
      <input type="text" class="search-input" id="search-input" placeholder="Search sections, topics, terms..." autocomplete="off" spellcheck="false">
      <kbd class="search-esc-hint">ESC</kbd>
    </div>
    <div class="search-results" id="search-results"></div>
    <div class="search-footer">
      <span class="search-footer-hint"><span class="search-footer-key">&#x2191;&#x2193;</span>&nbsp;navigate</span>
      <span class="search-footer-hint"><span class="search-footer-key">&#x23CE;</span>&nbsp;select</span>
      <span class="search-footer-hint"><span class="search-footer-key">ESC</span>&nbsp;close</span>
    </div>
  </div>
</div>

</body>
</html>
